get headers list|public java.util.List<com.google.privacy.dlp.v2.FieldId> () { return headers_; }
more than|public static Validation<String> (int size) { return notEmpty().and(moreThan(size, I18N_MAP.get(i18nPrefix + "MORE_THAN"))); }
validate citizen id number|public static <T extends CharSequence> T (T value, String errorMsg) throws ValidateException { if (false == isCitizenId(value)) { throw new ValidateException(errorMsg); } return value; }
djb hash|public static int (String str) { int hash = 5381; for (int i = 0; i < str.length(); i++) { hash = ((hash << 5) + hash) + str.charAt(i); } return hash & 0x7FFFFFFF; }
read object|private void (ObjectInputStream in) throws IOException, ClassNotFoundException { in.defaultReadObject(); // kryoRegistrations may be null if this value serializer is deserialized from an old version if (kryoRegistrations == null) { this.kryoRegistrations = asKryoRegistrations(type); } }
set skip url string|public void (String skipURL) { this.skipURL = skipURL; getConfig().setProperty(SPIDER_SKIP_URL, this.skipURL); parseSkipURL(this.skipURL); }
resume animation|@MainThread public void () { if (compositionLayer == null) { lazyCompositionTasks.add(new LazyCompositionTask() { @Override public void run(LottieComposition composition) { resumeAnimation(); } }); return; } animator.resumeAnimation(); }
close|@Override public void () throws IOException { if (open) { // stop accepting work, interrupt worker thread. exec.shutdownNow(); try { // give worker thread a bit of time to process the interruption. exec.awaitTermination(1, TimeUnit.SECONDS); } catch (InterruptedException e) { Thread.currentThread().interrupt(); } chan.close(); open = false; } }
run query permanent table|public void (String destinationDataset, String destinationTable) throws InterruptedException { // [START bigquery_query_destination_table] // BigQuery bigquery = BigQueryOptions.getDefaultInstance().getService(); // String destinationDataset = 'my_destination_dataset'; // String destinationTable = 'my_destination_table'; String query = "SELECT corpus FROM `bigquery-public-data.samples.shakespeare` GROUP BY corpus;"; QueryJobConfiguration queryConfig = // Note that setUseLegacySql is set to false by default QueryJobConfiguration.newBuilder(query).setDestinationTable(TableId.of(destinationDataset, destinationTable)).build(); // Print the results. for (FieldValueList row : bigquery.query(queryConfig).iterateAll()) { for (FieldValue val : row) { System.out.printf("%s,", val.toString()); } System.out.printf("\n"); } // [END bigquery_query_destination_table] }
last modified time|public static Date (File file) { if (!exist(file)) { return null; } return new Date(file.lastModified()); }
parse time today|public static DateTime (String timeString) { timeString = StrUtil.format("{} {}", today(), timeString); return parse(timeString, DatePattern.NORM_DATETIME_FORMAT); }
validate cluster specification|private void (ClusterSpecification clusterSpecification) throws FlinkException { try { final long taskManagerMemorySize = clusterSpecification.getTaskManagerMemoryMB(); // We do the validation by calling the calculation methods here // Internally these methods will check whether the cluster can be started with the provided // ClusterSpecification and the configured memory requirements final long cutoff = ContaineredTaskManagerParameters.calculateCutoffMB(flinkConfiguration, taskManagerMemorySize); TaskManagerServices.calculateHeapSizeMB(taskManagerMemorySize - cutoff, flinkConfiguration); } catch (IllegalArgumentException iae) { throw new FlinkException("Cannot fulfill the minimum memory requirements with the provided " + "cluster specification. Please increase the memory of the cluster.", iae); } }
add all|public Environment (@NonNull Map<String, String> map) { map.forEach(( key, value) -> this.props.setProperty(key, value)); return this; }
get granular stats|@java.lang.Deprecated public java.util.Map<java.lang.String, com.google.cloud.automl.v1beta1.TimestampStats.GranularStats> () { return getGranularStatsMap(); }
require positive|@Nonnull public static Number (String name, Number value) { requireNonNull(name, value); requirePositive(name, value.intValue()); return value; }
add strength|private void (String collectName, String queueName, TypeName type) { context.cache.addMethod(MethodSpec.methodBuilder(queueName).addModifiers(context.protectedFinalModifiers()).returns(type).addStatement("return $N", queueName).build()); context.cache.addField(FieldSpec.builder(type, queueName, Modifier.FINAL).initializer("new $T()", type).build()); context.cache.addMethod(MethodSpec.methodBuilder(collectName).addModifiers(context.protectedFinalModifiers()).addStatement("return true").returns(boolean.class).build()); }
handle server header decode size error|protected void (ChannelHandlerContext ctx, Http2Stream stream) { encoder().writeHeaders(ctx, stream.id(), HEADERS_TOO_LARGE_HEADERS, 0, true, ctx.newPromise()); }
is history enabled|@Override public boolean () { if (log.isDebugEnabled()) { log.debug("Current history level: {}", historyLevel); } return !historyLevel.equals(HistoryLevel.NONE); }
create warn decision state|protected void (final Flow flow) { createDecisionState(flow, CasWebflowConstants.STATE_ID_WARN, "flowScope.warnCookieValue", CasWebflowConstants.STATE_ID_SHOW_WARNING_VIEW, CasWebflowConstants.STATE_ID_REDIRECT); }
rename to|@Override public void (String newName) throws IOException { File oldBuildDir = getBuildDir(); super.renameTo(newName); File newBuildDir = getBuildDir(); if (oldBuildDir.isDirectory() && !newBuildDir.isDirectory()) { if (!newBuildDir.getParentFile().isDirectory()) { newBuildDir.getParentFile().mkdirs(); } if (!oldBuildDir.renameTo(newBuildDir)) { throw new IOException("failed to rename " + oldBuildDir + " to " + newBuildDir); } } }
get response body as stream|@Override public InputStream () throws IOException { if (responseStream != null) { return responseStream; } if (responseBody != null) { InputStream byteResponseStream = new ByteArrayInputStream(responseBody); LOG.debug("re-creating response stream from byte array"); return byteResponseStream; } return null; }
put stats|public void (String route, String cause) { if (route == null) route = "UNKNOWN_ROUTE"; route = route.replace("/", "_"); ConcurrentHashMap<String, ErrorStatsData> statsMap = routeMap.get(route); if (statsMap == null) { statsMap = new ConcurrentHashMap<String, ErrorStatsData>(); routeMap.putIfAbsent(route, statsMap); } ErrorStatsData sd = statsMap.get(cause); if (sd == null) { sd = new ErrorStatsData(route, cause); ErrorStatsData sd1 = statsMap.putIfAbsent(cause, sd); if (sd1 != null) { sd = sd1; } else { MonitorRegistry.getInstance().registerObject(sd); } } sd.update(); }
get helper workspace|public <T extends Pointer> T (String key) { return helperWorkspacePointers == null ? null : (T) helperWorkspacePointers.get(key); }
find element|@Override public WebElement () { SlowLoadingElement loadingElement = new SlowLoadingElement(clock, timeOutInSeconds); try { return loadingElement.get().getElement(); } catch (NoSuchElementError e) { throw new NoSuchElementException(String.format("Timed out after %d seconds. %s", timeOutInSeconds, e.getMessage()), e.getCause()); } }
matches|@Override public boolean (final CharSequence rawPassword, final String encodedPassword) { if (StringUtils.isBlank(encodedPassword)) { LOGGER.warn("The encoded password provided for matching is null. Returning false"); return false; } var providedSalt = StringUtils.EMPTY; val lastDollarIndex = encodedPassword.lastIndexOf('$'); if (lastDollarIndex == -1) { providedSalt = encodedPassword.substring(0, 2); LOGGER.debug("Assuming DES UnixCrypt as no delimiter could be found in the encoded password provided"); } else { providedSalt = encodedPassword.substring(0, lastDollarIndex); LOGGER.debug("Encoded password uses algorithm [{}]", providedSalt.charAt(1)); } var encodedRawPassword = Crypt.crypt(rawPassword.toString(), providedSalt); var matched = StringUtils.equals(encodedRawPassword, encodedPassword); LOGGER.debug("Provided password does {}match the encoded password", BooleanUtils.toString(matched, StringUtils.EMPTY, "not ")); return matched; }
get all entries|private List<MergedIndexEntry> (long offsetAdjustment) { Exceptions.checkArgument(offsetAdjustment >= 0, "offsetAdjustment", "offsetAdjustment must be a non-negative number."); synchronized (this.lock) { List<MergedIndexEntry> result = new ArrayList<>(this.indexEntries.size()); this.indexEntries.forEach( entry -> { if (entry.isDataEntry()) { result.add(new MergedIndexEntry(entry.getStreamSegmentOffset() + offsetAdjustment, this.metadata.getId(), (CacheIndexEntry) entry)); } }); return result; } }
calculate current uri|protected String (HttpServletRequest request) throws UnsupportedEncodingException { ServletUriComponentsBuilder builder = ServletUriComponentsBuilder.fromRequest(request); // Now work around SPR-10172... String queryString = request.getQueryString(); boolean legalSpaces = queryString != null && queryString.contains("+"); if (legalSpaces) { builder.replaceQuery(queryString.replace("+", "%20")); } UriComponents uri = null; try { uri = builder.replaceQueryParam("code").build(true); } catch (IllegalArgumentException ex) { return null; } String query = uri.getQuery(); if (legalSpaces) { query = query.replace("%20", "+"); } return ServletUriComponentsBuilder.fromUri(uri.toUri()).replaceQuery(query).build().toString(); }
listen|public static void (ClipboardListener listener, boolean sync) { listen(ClipboardMonitor.DEFAULT_TRY_COUNT, ClipboardMonitor.DEFAULT_DELAY, listener, sync); }
write nested serializer snapshots|public final void (DataOutputView out) throws IOException { out.writeInt(MAGIC_NUMBER); out.writeInt(VERSION); out.writeInt(nestedSnapshots.length); for (TypeSerializerSnapshot<?> snap : nestedSnapshots) { TypeSerializerSnapshot.writeVersionedSnapshot(out, snap); } }
multiply|public Rational (final int val) { BigInteger tmp = BigInteger.valueOf(val); return multiply(tmp); }
set query timeout|private void (final Statement statement, final int timeoutSec) { if (isQueryTimeoutSupported != FALSE) { try { statement.setQueryTimeout(timeoutSec); isQueryTimeoutSupported = TRUE; } catch (Exception e) { if (isQueryTimeoutSupported == UNINITIALIZED) { isQueryTimeoutSupported = FALSE; logger.info("{} - Failed to set query timeout for statement. ({})", poolName, e.getMessage()); } } } }
get|@Override public DataSet (int i) { if (i >= numExamples() || i < 0) throw new IllegalArgumentException("invalid example number: must be 0 to " + (numExamples() - 1) + ", got " + i); if (i == 0 && numExamples() == 1) return this; return new DataSet(getHelper(features, i), getHelper(labels, i), getHelper(featuresMask, i), getHelper(labelsMask, i)); }
words|public List<VocabularyWord> () { List<VocabularyWord> vocab = new ArrayList<>(vocabulary.values()); Collections.sort(vocab, new Comparator<VocabularyWord>() { @Override public int compare(VocabularyWord o1, VocabularyWord o2) { return Integer.compare(o2.getCount(), o1.getCount()); } }); return vocab; }
build new field serializers index|private static <T> Map<Field, TypeSerializer<?>> (PojoSerializer<T> newPojoSerializer) { final Field[] newFields = newPojoSerializer.getFields(); final TypeSerializer<?>[] newFieldSerializers = newPojoSerializer.getFieldSerializers(); checkState(newFields.length == newFieldSerializers.length); int numFields = newFields.length; final Map<Field, TypeSerializer<?>> index = new HashMap<>(numFields); for (int i = 0; i < numFields; i++) { index.put(newFields[i], newFieldSerializers[i]); } return index; }
is valid|public boolean (K key, UUID ticket) { if (timeouts.containsKey(key)) { Timeout<K> timeout = timeouts.get(key); return timeout.getTicket().equals(ticket); } else { return false; } }
transform|@Override public Schema (Schema schema) { int nCols = schema.numColumns(); List<ColumnMetaData> meta = schema.getColumnMetaData(); List<ColumnMetaData> newMeta = new ArrayList<>(nCols); newMeta.addAll(meta); newMeta.add(new StringMetaData(outputColumnName)); return schema.newSchema(newMeta); }
quoted av|private String () { pos++; beg = pos; end = beg; while (true) { if (pos == length) { throw new IllegalStateException("Unexpected end of DN: " + dn); } if (chars[pos] == '"') { // enclosing quotation was found pos++; break; } else if (chars[pos] == '\\') { chars[end] = getEscaped(); } else { // shift char: required for string with escaped chars chars[end] = chars[pos]; } pos++; end++; } // (compatibility with RFC 1779) for (; pos < length && chars[pos] == ' '; pos++) { } return new String(chars, beg, end - beg); }
convert to page sql|public String (String sql, Integer offset, Integer limit) { //解析SQL Statement stmt; try { stmt = CCJSqlParserUtil.parse(sql); } catch (Throwable e) { throw new PageException("不支持该SQL转换为分页查询!", e); } if (!(stmt instanceof Select)) { throw new PageException("分页语句必须是Select查询!"); } //获取分页查询的select Select pageSelect = getPageSelect((Select) stmt); String pageSql = pageSelect.toString(); //缓存移到外面了，所以不替换参数 if (offset != null) { pageSql = pageSql.replace(START_ROW, String.valueOf(offset)); } if (limit != null) { pageSql = pageSql.replace(PAGE_SIZE, String.valueOf(limit)); } return pageSql; }
write|@Override public void (ChannelHandlerContext ctx, Object msg, ChannelPromise promise) { if (msg instanceof Http2DataFrame) { Http2DataFrame dataFrame = (Http2DataFrame) msg; encoder().writeData(ctx, dataFrame.stream().id(), dataFrame.content(), dataFrame.padding(), dataFrame.isEndStream(), promise); } else if (msg instanceof Http2HeadersFrame) { writeHeadersFrame(ctx, (Http2HeadersFrame) msg, promise); } else if (msg instanceof Http2WindowUpdateFrame) { Http2WindowUpdateFrame frame = (Http2WindowUpdateFrame) msg; Http2FrameStream frameStream = frame.stream(); // to set the Http2FrameStream so we assume if it is null the WINDOW_UPDATE is for the connection stream. try { if (frameStream == null) { increaseInitialConnectionWindow(frame.windowSizeIncrement()); } else { consumeBytes(frameStream.id(), frame.windowSizeIncrement()); } promise.setSuccess(); } catch (Throwable t) { promise.setFailure(t); } } else if (msg instanceof Http2ResetFrame) { Http2ResetFrame rstFrame = (Http2ResetFrame) msg; encoder().writeRstStream(ctx, rstFrame.stream().id(), rstFrame.errorCode(), promise); } else if (msg instanceof Http2PingFrame) { Http2PingFrame frame = (Http2PingFrame) msg; encoder().writePing(ctx, frame.ack(), frame.content(), promise); } else if (msg instanceof Http2SettingsFrame) { encoder().writeSettings(ctx, ((Http2SettingsFrame) msg).settings(), promise); } else if (msg instanceof Http2SettingsAckFrame) { // In the event of manual SETTINGS ACK is is assumed the encoder will apply the earliest received but not // yet ACKed settings. encoder().writeSettingsAck(ctx, promise); } else if (msg instanceof Http2GoAwayFrame) { writeGoAwayFrame(ctx, (Http2GoAwayFrame) msg, promise); } else if (msg instanceof Http2UnknownFrame) { Http2UnknownFrame unknownFrame = (Http2UnknownFrame) msg; encoder().writeFrame(ctx, unknownFrame.frameType(), unknownFrame.stream().id(), unknownFrame.flags(), unknownFrame.content(), promise); } else if (!(msg instanceof Http2Frame)) { ctx.write(msg, promise); } else { ReferenceCountUtil.release(msg); throw new UnsupportedMessageTypeException(msg); } }
submit to server in uri|private Observable<HttpClientResponse<O>> (HttpClientRequest<I> request, IClientConfig requestConfig, ClientConfig config, RetryHandler errorHandler, ExecutionContext<HttpClientRequest<I>> context) { // First, determine server from the URI URI uri; try { uri = new URI(request.getUri()); } catch (URISyntaxException e) { return Observable.error(e); } String host = uri.getHost(); if (host == null) { return null; } int port = uri.getPort(); if (port < 0) { if (Optional.ofNullable(clientConfig.get(IClientConfigKey.Keys.IsSecure)).orElse(false)) { port = 443; } else { port = 80; } } return LoadBalancerCommand.<HttpClientResponse<O>>builder().withRetryHandler(errorHandler).withLoadBalancerContext(lbContext).withListeners(listeners).withExecutionContext(context).withServer(new Server(host, port)).build().submit(this.requestToOperation(request, getRxClientConfig(requestConfig, config))); }
get kv state lookup info|private CompletableFuture<KvStateLocation> (final JobID jobId, final String queryableStateName, final boolean forceUpdate) { final Tuple2<JobID, String> cacheKey = new Tuple2<>(jobId, queryableStateName); final CompletableFuture<KvStateLocation> cachedFuture = lookupCache.get(cacheKey); if (!forceUpdate && cachedFuture != null && !cachedFuture.isCompletedExceptionally()) { LOG.debug("Retrieving location for state={} of job={} from the cache.", queryableStateName, jobId); return cachedFuture; } final KvStateLocationOracle kvStateLocationOracle = proxy.getKvStateLocationOracle(jobId); if (kvStateLocationOracle != null) { LOG.debug("Retrieving location for state={} of job={} from the key-value state location oracle.", queryableStateName, jobId); final CompletableFuture<KvStateLocation> location = new CompletableFuture<>(); lookupCache.put(cacheKey, location); kvStateLocationOracle.requestKvStateLocation(jobId, queryableStateName).whenComplete((KvStateLocation kvStateLocation, Throwable throwable) -> { if (throwable != null) { if (ExceptionUtils.stripCompletionException(throwable) instanceof FlinkJobNotFoundException) { // if the jobId was wrong, remove the entry from the cache. lookupCache.remove(cacheKey); } location.completeExceptionally(throwable); } else { location.complete(kvStateLocation); } }); return location; } else { return FutureUtils.completedExceptionally(new UnknownLocationException("Could not retrieve location of state=" + queryableStateName + " of job=" + jobId + ". Potential reasons are: i) the state is not ready, or ii) the job does not exist.")); } }
zeromean unit variance|public static JavaRDD<List<Writable>> (Schema schema, JavaRDD<List<Writable>> data) { return zeromeanUnitVariance(schema, data, Collections.<String>emptyList()); }
add component|private int (boolean increaseWriterIndex, int cIndex, ByteBuf buffer) { assert buffer != null; boolean wasAdded = false; try { checkComponentIndex(cIndex); // No need to consolidate - just add a component to the list. Component c = newComponent(buffer, 0); int readableBytes = c.length(); addComp(cIndex, c); wasAdded = true; if (readableBytes > 0 && cIndex < componentCount - 1) { updateComponentOffsets(cIndex); } else if (cIndex > 0) { c.reposition(components[cIndex - 1].endOffset); } if (increaseWriterIndex) { writerIndex += readableBytes; } return cIndex; } finally { if (!wasAdded) { buffer.release(); } } }
numeric to string vec|public static Vec (Vec src) { if (src.isCategorical() || src.isUUID()) throw new H2OIllegalValueException("Cannot convert a non-numeric column" + " using numericToStringVec() ", src); Vec res = new MRTask() { @Override public void map(Chunk chk, NewChunk newChk) { if (chk instanceof C0DChunk) { // all NAs for (int i = 0; i < chk._len; i++) newChk.addNA(); } else { for (int i = 0; i < chk._len; i++) { if (!chk.isNA(i)) newChk.addStr(PrettyPrint.number(chk, chk.atd(i), 4)); else newChk.addNA(); } } } }.doAll(Vec.T_STR, src).outputFrame().anyVec(); assert res != null; return res; }
get strings|public String[] (String name) { String valueString = get(name); return StringUtils.split(valueString, ","); }
handle async if not done|public static <IN, OUT> CompletableFuture<OUT> (CompletableFuture<IN> completableFuture, Executor executor, BiFunction<? super IN, Throwable, ? extends OUT> handler) { return completableFuture.isDone() ? completableFuture.handle(handler) : completableFuture.handleAsync(handler, executor); }
get slider max depth|private JSlider () { if (sliderMaxDepth == null) { sliderMaxDepth = new JSlider(); sliderMaxDepth.setMaximum(19); sliderMaxDepth.setMinimum(0); sliderMaxDepth.setMinorTickSpacing(1); sliderMaxDepth.setPaintTicks(true); sliderMaxDepth.setPaintLabels(true); sliderMaxDepth.setName(""); sliderMaxDepth.setMajorTickSpacing(1); sliderMaxDepth.setSnapToTicks(true); sliderMaxDepth.setPaintTrack(true); } return sliderMaxDepth; }
write span|void (Span span, int sizeOfSpan, Buffer result) { result.writeByte(SPAN.key); // length prefix result.writeVarint(sizeOfSpan); SPAN.writeValue(result, span); }
from json|public static Map<String, ? extends CacheConfig> (File file) throws IOException { return new CacheConfigSupport().fromJSON(file); }
create sort|public TableOperation (List<Expression> orders, TableOperation child) { failIfStreaming(); List<Expression> convertedOrders = orders.stream().map( f -> f.accept(orderWrapper)).collect(Collectors.toList()); return new SortTableOperation(convertedOrders, child); }
key with default|public static <T> Key<T> (String name, T defaultValue) { return new Key<>(name, defaultValue); }
wrap set|public static <T> Set<T> (final T source) { val list = new LinkedHashSet<T>(); if (source != null) { list.add(source); } return list; }
put service user interface metadata|public static void (final RequestContext requestContext, final Serializable mdui) { if (mdui != null) { requestContext.getFlowScope().put(PARAMETER_SERVICE_UI_METADATA, mdui); } }
get node template|@BetaApi public final NodeTemplate (ProjectRegionNodeTemplateName nodeTemplate) { GetNodeTemplateHttpRequest request = GetNodeTemplateHttpRequest.newBuilder().setNodeTemplate(nodeTemplate == null ? null : nodeTemplate.toString()).build(); return getNodeTemplate(request); }
compute offset|protected long (final long t, final DateTimeZone tz) { // start is the beginning of the last period ending within dataInterval long start = dataInterval.getEndMillis() - periodMillis; long startOffset = start % periodMillis - originMillis % periodMillis; if (startOffset < 0) { startOffset += periodMillis; } start -= startOffset; // tOffset is the offset time t within the last period long tOffset = t % periodMillis - originMillis % periodMillis; if (tOffset < 0) { tOffset += periodMillis; } tOffset += start; return tOffset - t - (tz.getOffset(tOffset) - tz.getOffset(t)); }
add configurations|public void (Configuration... configurations) { Assert.notNull(configurations, "Configurations must not be null"); this.configurations.addAll(Arrays.asList(configurations)); }
proxy user kill all spawned hadoop jobs|public static void (final String logFilePath, Props jobProps, File tokenFile, final Logger log) { Properties properties = new Properties(); properties.putAll(jobProps.getFlattened()); try { if (HadoopSecureWrapperUtils.shouldProxy(properties)) { UserGroupInformation proxyUser = HadoopSecureWrapperUtils.setupProxyUser(properties, tokenFile.getAbsolutePath(), log); proxyUser.doAs(new PrivilegedExceptionAction<Void>() { @Override public Void run() throws Exception { HadoopJobUtils.killAllSpawnedHadoopJobs(logFilePath, log); return null; } }); } else { HadoopJobUtils.killAllSpawnedHadoopJobs(logFilePath, log); } } catch (Throwable t) { log.warn("something happened while trying to kill all spawned jobs", t); } }
random ele|public static <T> T (List<T> list) { return randomEle(list, list.size()); }
append|private static ByteBuf (ByteBufAllocator allocator, boolean useDirect, X509Certificate cert, int count, ByteBuf pem) throws CertificateEncodingException { ByteBuf encoded = Unpooled.wrappedBuffer(cert.getEncoded()); try { ByteBuf base64 = SslUtils.toBase64(allocator, encoded); try { if (pem == null) { // We try to approximate the buffer's initial size. The sizes of // certificates can vary a lot so it'll be off a bit depending // on the number of elements in the array (count argument). pem = newBuffer(allocator, useDirect, (BEGIN_CERT.length + base64.readableBytes() + END_CERT.length) * count); } pem.writeBytes(BEGIN_CERT); pem.writeBytes(base64); pem.writeBytes(END_CERT); } finally { base64.release(); } } finally { encoded.release(); } return pem; }
get build trigger upstream projects|public final List<AbstractProject> () { ArrayList<AbstractProject> result = new ArrayList<AbstractProject>(); for (AbstractProject<?, ?> ap : getUpstreamProjects()) { BuildTrigger buildTrigger = ap.getPublishersList().get(BuildTrigger.class); if (buildTrigger != null) if (buildTrigger.getChildJobs(ap).contains(this)) result.add(ap); } return result; }
read module|private boolean (int row, int column, int numRows, int numColumns) { // Adjust the row and column indices based on boundary wrapping if (row < 0) { row += numRows; column += 4 - ((numRows + 4) & 0x07); } if (column < 0) { column += numColumns; row += 4 - ((numColumns + 4) & 0x07); } readMappingMatrix.set(column, row); return mappingBitMatrix.get(column, row); }
read table|public CompletableFuture<List<TableEntry<byte[], byte[]>>> (final String tableName, final List<TableKey<byte[]>> keys, String delegationToken, final long clientRequestId) { final CompletableFuture<List<TableEntry<byte[], byte[]>>> result = new CompletableFuture<>(); final Controller.NodeUri uri = getTableUri(tableName); final WireCommandType type = WireCommandType.READ_TABLE; final long requestId = (clientRequestId == RequestTag.NON_EXISTENT_ID) ? idGenerator.get() : clientRequestId; final FailingReplyProcessor replyProcessor = new FailingReplyProcessor() { @Override public void connectionDropped() { log.warn(requestId, "readTable {} Connection dropped", tableName); result.completeExceptionally(new WireCommandFailedException(type, WireCommandFailedException.Reason.ConnectionDropped)); } @Override public void wrongHost(WireCommands.WrongHost wrongHost) { log.warn(requestId, "readTable {} wrong host", tableName); result.completeExceptionally(new WireCommandFailedException(type, WireCommandFailedException.Reason.UnknownHost)); } @Override public void noSuchSegment(WireCommands.NoSuchSegment noSuchSegment) { log.warn(requestId, "readTable {} NoSuchSegment", tableName); result.completeExceptionally(new WireCommandFailedException(type, WireCommandFailedException.Reason.SegmentDoesNotExist)); } @Override public void tableRead(WireCommands.TableRead tableRead) { log.debug(requestId, "readTable {} successful.", tableName); List<TableEntry<byte[], byte[]>> tableEntries = tableRead.getEntries().getEntries().stream().map( e -> new TableEntryImpl<>(convertFromWireCommand(e.getKey()), getArray(e.getValue().getData()))).collect(Collectors.toList()); result.complete(tableEntries); } @Override public void processingFailure(Exception error) { log.error(requestId, "readTable {} failed", tableName, error); handleError(error, result, type); } @Override public void authTokenCheckFailed(WireCommands.AuthTokenCheckFailed authTokenCheckFailed) { result.completeExceptionally(new WireCommandFailedException(new AuthenticationException(authTokenCheckFailed.toString()), type, WireCommandFailedException.Reason.AuthFailed)); } }; List<ByteBuf> buffersToRelease = new ArrayList<>(); // the version is always NO_VERSION as read returns the latest version of value. List<WireCommands.TableKey> keyList = keys.stream().map( k -> { ByteBuf buffer = wrappedBuffer(k.getKey()); buffersToRelease.add(buffer); return new WireCommands.TableKey(buffer, WireCommands.TableKey.NO_VERSION); }).collect(Collectors.toList()); WireCommands.ReadTable request = new WireCommands.ReadTable(requestId, tableName, delegationToken, keyList); sendRequestAsync(request, replyProcessor, result, ModelHelper.encode(uri)); return result.whenComplete(( r, e) -> release(buffersToRelease)); }
set variable|protected void (String variableName, Object value, ExecutionEntity sourceExecution, boolean fetchAllVariables) { if (fetchAllVariables == true) { // If it's in the cache, it's more recent if (usedVariablesCache.containsKey(variableName)) { updateVariableInstance(usedVariablesCache.get(variableName), value, sourceExecution); } // If the variable exists on this scope, replace it if (hasVariableLocal(variableName)) { setVariableLocal(variableName, value, sourceExecution, true); return; } // Otherwise, go up the hierarchy (we're trying to put it as high as possible) VariableScopeImpl parentVariableScope = getParentVariableScope(); if (parentVariableScope != null) { if (sourceExecution == null) { parentVariableScope.setVariable(variableName, value); } else { parentVariableScope.setVariable(variableName, value, sourceExecution, true); } return; } // we're creating it if (sourceExecution != null) { createVariableLocal(variableName, value, sourceExecution); } else { createVariableLocal(variableName, value); } } else { // Check local cache first if (usedVariablesCache.containsKey(variableName)) { updateVariableInstance(usedVariablesCache.get(variableName), value, sourceExecution); } else if (variableInstances != null && variableInstances.containsKey(variableName)) { updateVariableInstance(variableInstances.get(variableName), value, sourceExecution); } else { // Not in local cache, check if defined on this scope // Create it if it doesn't exist yet VariableInstanceEntity variable = getSpecificVariable(variableName); if (variable != null) { updateVariableInstance(variable, value, sourceExecution); usedVariablesCache.put(variableName, variable); } else { VariableScopeImpl parent = getParentVariableScope(); if (parent != null) { if (sourceExecution == null) { parent.setVariable(variableName, value, fetchAllVariables); } else { parent.setVariable(variableName, value, sourceExecution, fetchAllVariables); } return; } variable = createVariableInstance(variableName, value, sourceExecution); usedVariablesCache.put(variableName, variable); } } } }
get valid directory|public final File () { File file = this.directory; file = (file != null) ? file : getWarFileDocumentRoot(); file = (file != null) ? file : getExplodedWarFileDocumentRoot(); file = (file != null) ? file : getCommonDocumentRoot(); if (file == null && this.logger.isDebugEnabled()) { logNoDocumentRoots(); } else if (this.logger.isDebugEnabled()) { this.logger.debug("Document root: " + file); } return file; }
bootstrap sinks from disk|private Object () { Preconditions.checkState(sinks.isEmpty(), "Already bootstrapped?!"); final File baseDir = tuningConfig.getBasePersistDirectory(); if (!baseDir.exists()) { return null; } final File[] files = baseDir.listFiles(); if (files == null) { return null; } final Committed committed; File commitFile = null; try { commitLock.lock(); commitFile = computeCommitFile(); if (commitFile.exists()) { committed = objectMapper.readValue(commitFile, Committed.class); } else { committed = Committed.nil(); } } catch (Exception e) { throw new ISE(e, "Failed to read commitFile: %s", commitFile); } finally { commitLock.unlock(); } int rowsSoFar = 0; log.info("Loading sinks from[%s]: %s", baseDir, committed.getHydrants().keySet()); for (File sinkDir : files) { final File identifierFile = new File(sinkDir, IDENTIFIER_FILE_NAME); if (!identifierFile.isFile()) { // No identifier in this sinkDir; it must not actually be a sink directory. Skip it. continue; } try { final SegmentIdWithShardSpec identifier = objectMapper.readValue(new File(sinkDir, "identifier.json"), SegmentIdWithShardSpec.class); final int committedHydrants = committed.getCommittedHydrants(identifier.toString()); if (committedHydrants <= 0) { log.info("Removing uncommitted sink at [%s]", sinkDir); FileUtils.deleteDirectory(sinkDir); continue; } // To avoid reading and listing of "merged" dir and other special files final File[] sinkFiles = sinkDir.listFiles(( dir, fileName) -> !(Ints.tryParse(fileName) == null)); Arrays.sort(sinkFiles, ( o1, o2) -> Ints.compare(Integer.parseInt(o1.getName()), Integer.parseInt(o2.getName()))); List<FireHydrant> hydrants = new ArrayList<>(); for (File hydrantDir : sinkFiles) { final int hydrantNumber = Integer.parseInt(hydrantDir.getName()); if (hydrantNumber >= committedHydrants) { log.info("Removing uncommitted segment at [%s]", hydrantDir); FileUtils.deleteDirectory(hydrantDir); } else { log.info("Loading previously persisted segment at [%s]", hydrantDir); if (hydrantNumber != hydrants.size()) { throw new ISE("Missing hydrant [%,d] in sinkDir [%s].", hydrants.size(), sinkDir); } hydrants.add(new FireHydrant(new QueryableIndexSegment(indexIO.loadIndex(hydrantDir), identifier.asSegmentId()), hydrantNumber)); } } // Make sure we loaded enough hydrants. if (committedHydrants != hydrants.size()) { throw new ISE("Missing hydrant [%,d] in sinkDir [%s].", hydrants.size(), sinkDir); } Sink currSink = new Sink(identifier.getInterval(), schema, identifier.getShardSpec(), identifier.getVersion(), tuningConfig.getMaxRowsInMemory(), maxBytesTuningConfig, tuningConfig.isReportParseExceptions(), null, hydrants); rowsSoFar += currSink.getNumRows(); sinks.put(identifier, currSink); sinkTimeline.add(currSink.getInterval(), currSink.getVersion(), identifier.getShardSpec().createChunk(currSink)); segmentAnnouncer.announceSegment(currSink.getSegment()); } catch (IOException e) { log.makeAlert(e, "Problem loading sink[%s] from disk.", schema.getDataSource()).addData("sinkDir", sinkDir).emit(); } } // Make sure we loaded all committed sinks. final Set<String> loadedSinks = Sets.newHashSet(Iterables.transform(sinks.keySet(), SegmentIdWithShardSpec::toString)); final Set<String> missingSinks = Sets.difference(committed.getHydrants().keySet(), loadedSinks); if (!missingSinks.isEmpty()) { throw new ISE("Missing committed sinks [%s]", Joiner.on(", ").join(missingSinks)); } totalRows.set(rowsSoFar); return committed.getMetadata(); }
order|public static char (DataBuffer buffer) { int length = Shape.shapeInfoLength(Shape.rank(buffer)); return (char) buffer.getInt(length - 1); }
output|public synchronized INDArray[] (boolean train, boolean clearInputs, INDArray... input) { //If !clearInputs, then inputs should be detached (otherwise: will be out of scope) boolean detachedInputs = !clearInputs; try { return outputOfLayersDetached(train, FwdPassType.STANDARD, getOutputLayerIndices(), input, null, null, clearInputs, detachedInputs, null); } catch (OutOfMemoryError e) { CrashReportingUtil.writeMemoryCrashDump(this, e); throw e; } }
retrieve attributes from attribute repository|public static Map<String, List<Object>> (final IPersonAttributeDao attributeRepository, final String principalId, final Set<String> activeAttributeRepositoryIdentifiers) { var filter = IPersonAttributeDaoFilter.alwaysChoose(); if (activeAttributeRepositoryIdentifiers != null && !activeAttributeRepositoryIdentifiers.isEmpty()) { val repoIdsArray = activeAttributeRepositoryIdentifiers.toArray(ArrayUtils.EMPTY_STRING_ARRAY); filter = dao -> Arrays.stream(dao.getId()).anyMatch( daoId -> daoId.equalsIgnoreCase(IPersonAttributeDao.WILDCARD) || StringUtils.equalsAnyIgnoreCase(daoId, repoIdsArray) || StringUtils.equalsAnyIgnoreCase(IPersonAttributeDao.WILDCARD, repoIdsArray)); } val attrs = attributeRepository.getPerson(principalId, filter); if (attrs == null) { return new HashMap<>(0); } return attrs.getAttributes(); }
update page|private void () { // retrieve page final int retrievalPage = page == LAST_PAGE ? pageCount : page; final List<Row> rows; try { rows = client.getExecutor().retrieveResultPage(resultDescriptor.getResultId(), retrievalPage); } catch (SqlExecutionException e) { close(e); return; } // convert page final List<String[]> stringRows = rows.stream().map(CliUtils::rowToString).collect(Collectors.toList()); // update results if (previousResultsPage == retrievalPage) { // only use the previous results if the current page number has not changed // this allows for updated results when the key space remains constant previousResults = results; } else { previousResults = null; previousResultsPage = retrievalPage; } results = stringRows; // check if selected row is still valid if (selectedRow != NO_ROW_SELECTED) { if (selectedRow >= results.size()) { selectedRow = NO_ROW_SELECTED; } } // reset view resetAllParts(); }
get ticket granting ticket from request|public static TicketGrantingTicket (final CasCookieBuilder ticketGrantingTicketCookieGenerator, final TicketRegistry ticketRegistry, final HttpServletRequest request) { val cookieValue = ticketGrantingTicketCookieGenerator.retrieveCookieValue(request); if (StringUtils.isNotBlank(cookieValue)) { val tgt = ticketRegistry.getTicket(cookieValue, TicketGrantingTicket.class); if (tgt != null && !tgt.isExpired()) { return tgt; } } return null; }
sign url with signer|public URL (String bucketName, String blobName, String keyPath) throws IOException { // [START signUrlWithSigner] URL signedUrl = storage.signUrl(BlobInfo.newBuilder(bucketName, blobName).build(), 14, TimeUnit.DAYS, SignUrlOption.signWith(ServiceAccountCredentials.fromStream(new FileInputStream(keyPath)))); // [END signUrlWithSigner] return signedUrl; }
append|int (ByteArraySegment data) { ensureAppendConditions(); int actualLength = Math.min(data.getLength(), getAvailableLength()); if (actualLength > 0) { this.contents.copyFrom(data, writePosition, actualLength); writePosition += actualLength; } return actualLength; }
show spider dialog|public void (Target target) { if (spiderDialog == null) { spiderDialog = new SpiderDialog(this, View.getSingleton().getMainFrame(), new Dimension(700, 430)); } if (spiderDialog.isVisible()) { // Its behind you! Actually not needed no the window is alwaysOnTop, but keeping in case we change that ;) spiderDialog.toFront(); return; } if (target != null) { spiderDialog.init(target); } else { // Keep the previous target spiderDialog.init(null); } spiderDialog.setVisible(true); }
compare|@Override public int (VocabularyWord o1, VocabularyWord o2) { return Integer.compare(o2.getCount(), o1.getCount()); }
on open|@Override public synchronized void (WebSocket webSocket, Response response) { System.out.println("onOpen: " + response); }
run|@Override public void (LottieComposition composition) { resumeAnimation(); }
decode|public static byte[] (String key) { return Validator.isHex(key) ? HexUtil.decodeHex(key) : Base64.decode(key); }
create pooled byte buf allocator|public static PooledByteBufAllocator (boolean allowDirectBufs, boolean allowCache, int numCores) { if (numCores == 0) { numCores = Runtime.getRuntime().availableProcessors(); } return new PooledByteBufAllocator(allowDirectBufs && PlatformDependent.directBufferPreferred(), Math.min(PooledByteBufAllocator.defaultNumHeapArena(), numCores), Math.min(PooledByteBufAllocator.defaultNumDirectArena(), allowDirectBufs ? numCores : 0), PooledByteBufAllocator.defaultPageSize(), PooledByteBufAllocator.defaultMaxOrder(), allowCache ? PooledByteBufAllocator.defaultTinyCacheSize() : 0, allowCache ? PooledByteBufAllocator.defaultSmallCacheSize() : 0, allowCache ? PooledByteBufAllocator.defaultNormalCacheSize() : 0, allowCache ? PooledByteBufAllocator.defaultUseCacheForAllThreads() : false); }
load jar|public static void (URLClassLoader loader, File jarFile) throws UtilException { try { final Method method = ClassUtil.getDeclaredMethod(URLClassLoader.class, "addURL", URL.class); if (null != method) { method.setAccessible(true); final List<File> jars = loopJar(jarFile); for (File jar : jars) { ReflectUtil.invoke(loader, method, new Object[] { jar.toURI().toURL() }); } } } catch (IOException e) { throw new UtilException(e); } }
change method|public static HttpRequestHeader (String method, String header, String body) throws URIException, HttpMalformedHeaderException { HttpRequestHeader hrh = new HttpRequestHeader(header); URI uri = hrh.getURI(); String prevMethod = hrh.getMethod(); if (prevMethod.equalsIgnoreCase(method)) { return hrh; } if (prevMethod.equals(HttpRequestHeader.POST)) { // Was POST, move all params onto the URL if (body != null && body.length() > 0) { StringBuilder sb = new StringBuilder(); if (uri.getQuery() != null) { sb.append(uri.getQuery()); } String[] params = body.split("&"); for (String param : params) { if (sb.length() > 0) { sb.append('&'); } String[] nv = param.split("="); if (nv.length == 1) { // This effectively strips out the equals if theres no value sb.append(nv[0]); } else { sb.append(param); } } uri.setQuery(sb.toString()); } hrh.setURI(uri); // Clear the body body = ""; } else if (method.equals(HttpRequestHeader.POST)) { // To be a port, move all URL query params into the body String query = uri.getQuery(); if (query != null) { StringBuilder sb = new StringBuilder(); String[] params = query.split("&"); for (String param : params) { if (sb.length() > 0) { sb.append('&'); } sb.append(param); String[] nv = param.split("="); if (nv.length == 1) { // Cope with URL params with no values e.g. http://www.example.com/test?key sb.append('='); } } // fixed: dead store to variable body by commenting the following line // body = sb.toString(); uri.setQuery(null); hrh.setURI(uri); } } hrh.setMethod(method); return hrh; }
compare to|public int (Object o) { int thisValue = this.value; int thatValue = ((IntWritable) o).value; return (thisValue < thatValue ? -1 : (thisValue == thatValue ? 0 : 1)); }
set custom endpoint initializer|public void (EndpointInitializer initializer) { Objects.requireNonNull(initializer, "Initializer has to be set"); ClosureCleaner.ensureSerializable(initializer); this.initializer = initializer; }
translate selector function reducer|private static <T, K> org.apache.flink.api.common.operators.SingleInputOperator<?, T, ?> (SelectorFunctionKeys<T, ?> rawKeys, ReduceFunction<T> function, TypeInformation<T> inputType, String name, Operator<T> input, int parallelism, CombineHint hint) { @SuppressWarnings("unchecked") final SelectorFunctionKeys<T, K> keys = (SelectorFunctionKeys<T, K>) rawKeys; TypeInformation<Tuple2<K, T>> typeInfoWithKey = KeyFunctions.createTypeWithKey(keys); Operator<Tuple2<K, T>> keyedInput = KeyFunctions.appendKeyExtractor(input, keys); PlanUnwrappingReduceOperator<T, K> reducer = new PlanUnwrappingReduceOperator<>(function, keys, name, inputType, typeInfoWithKey); reducer.setInput(keyedInput); reducer.setParallelism(parallelism); reducer.setCombineHint(hint); return KeyFunctions.appendKeyRemover(reducer, keys); }
connect job vertices|private DistributionPattern (Channel channel, int inputNumber, final JobVertex sourceVertex, final TaskConfig sourceConfig, final JobVertex targetVertex, final TaskConfig targetConfig, boolean isBroadcast) throws CompilerException { // ------------ connect the vertices to the job graph -------------- final DistributionPattern distributionPattern; switch(channel.getShipStrategy()) { case FORWARD: distributionPattern = DistributionPattern.POINTWISE; break; case PARTITION_RANDOM: case BROADCAST: case PARTITION_HASH: case PARTITION_CUSTOM: case PARTITION_RANGE: case PARTITION_FORCED_REBALANCE: distributionPattern = DistributionPattern.ALL_TO_ALL; break; default: throw new RuntimeException("Unknown runtime ship strategy: " + channel.getShipStrategy()); } final ResultPartitionType resultType; switch(channel.getDataExchangeMode()) { case PIPELINED: resultType = ResultPartitionType.PIPELINED; break; case BATCH: // BLOCKING results are currently not supported in closed loop iterations // // See https://issues.apache.org/jira/browse/FLINK-1713 for details resultType = channel.getSource().isOnDynamicPath() ? ResultPartitionType.PIPELINED : ResultPartitionType.BLOCKING; break; case PIPELINE_WITH_BATCH_FALLBACK: throw new UnsupportedOperationException("Data exchange mode " + channel.getDataExchangeMode() + " currently not supported."); default: throw new UnsupportedOperationException("Unknown data exchange mode."); } JobEdge edge = targetVertex.connectNewDataSetAsInput(sourceVertex, distributionPattern, resultType); // -------------- configure the source task's ship strategy strategies in task config -------------- final int outputIndex = sourceConfig.getNumOutputs(); sourceConfig.addOutputShipStrategy(channel.getShipStrategy()); if (outputIndex == 0) { sourceConfig.setOutputSerializer(channel.getSerializer()); } if (channel.getShipStrategyComparator() != null) { sourceConfig.setOutputComparator(channel.getShipStrategyComparator(), outputIndex); } if (channel.getShipStrategy() == ShipStrategyType.PARTITION_RANGE) { final DataDistribution dataDistribution = channel.getDataDistribution(); if (dataDistribution != null) { sourceConfig.setOutputDataDistribution(dataDistribution, outputIndex); } else { throw new RuntimeException("Range partitioning requires data distribution."); } } if (channel.getShipStrategy() == ShipStrategyType.PARTITION_CUSTOM) { if (channel.getPartitioner() != null) { sourceConfig.setOutputPartitioner(channel.getPartitioner(), outputIndex); } else { throw new CompilerException("The ship strategy was set to custom partitioning, but no partitioner was set."); } } // ---------------- configure the receiver ------------------- if (isBroadcast) { targetConfig.addBroadcastInputToGroup(inputNumber); } else { targetConfig.addInputToGroup(inputNumber); } // ---------------- attach the additional infos to the job edge ------------------- String shipStrategy = JsonMapper.getShipStrategyString(channel.getShipStrategy()); if (channel.getShipStrategyKeys() != null && channel.getShipStrategyKeys().size() > 0) { shipStrategy += " on " + (channel.getShipStrategySortOrder() == null ? channel.getShipStrategyKeys().toString() : Utils.createOrdering(channel.getShipStrategyKeys(), channel.getShipStrategySortOrder()).toString()); } String localStrategy; if (channel.getLocalStrategy() == null || channel.getLocalStrategy() == LocalStrategy.NONE) { localStrategy = null; } else { localStrategy = JsonMapper.getLocalStrategyString(channel.getLocalStrategy()); if (localStrategy != null && channel.getLocalStrategyKeys() != null && channel.getLocalStrategyKeys().size() > 0) { localStrategy += " on " + (channel.getLocalStrategySortOrder() == null ? channel.getLocalStrategyKeys().toString() : Utils.createOrdering(channel.getLocalStrategyKeys(), channel.getLocalStrategySortOrder()).toString()); } } String caching = channel.getTempMode() == TempMode.NONE ? null : channel.getTempMode().toString(); edge.setShipStrategyName(shipStrategy); edge.setPreProcessingOperationName(localStrategy); edge.setOperatorLevelCachingDescription(caching); return distributionPattern; }
create frame protector|TsiFrameProtector (int maxFrameSize, ByteBufAllocator alloc) { unwrapper = null; return internalHandshaker.createFrameProtector(maxFrameSize, alloc); }
check not null|public static void (Object o, String message, Object... args) { if (o == null) { throwStateEx(message, args); } }
create select distinct|private RelBuilder (RelBuilder relBuilder, Aggregate aggregate, List<Integer> argList, int filterArg, Map<Integer, Integer> sourceOf) { relBuilder.push(aggregate.getInput()); final List<Pair<RexNode, String>> projects = new ArrayList<>(); final List<RelDataTypeField> childFields = relBuilder.peek().getRowType().getFieldList(); for (int i : aggregate.getGroupSet()) { sourceOf.put(i, projects.size()); projects.add(RexInputRef.of2(i, childFields)); } if (filterArg >= 0) { sourceOf.put(filterArg, projects.size()); projects.add(RexInputRef.of2(filterArg, childFields)); } for (Integer arg : argList) { if (filterArg >= 0) { // Implement // agg(DISTINCT arg) FILTER $f // by generating // SELECT DISTINCT ... CASE WHEN $f THEN arg ELSE NULL END AS arg // and then applying // agg(arg) // as usual. // // It works except for (rare) agg functions that need to see null // values. final RexBuilder rexBuilder = aggregate.getCluster().getRexBuilder(); final RexInputRef filterRef = RexInputRef.of(filterArg, childFields); final Pair<RexNode, String> argRef = RexInputRef.of2(arg, childFields); RexNode condition = rexBuilder.makeCall(SqlStdOperatorTable.CASE, filterRef, argRef.left, rexBuilder.ensureType(argRef.left.getType(), rexBuilder.makeCast(argRef.left.getType(), rexBuilder.constantNull()), true)); sourceOf.put(arg, projects.size()); projects.add(Pair.of(condition, "i$" + argRef.right)); continue; } if (sourceOf.get(arg) != null) { continue; } sourceOf.put(arg, projects.size()); projects.add(RexInputRef.of2(arg, childFields)); } relBuilder.project(Pair.left(projects), Pair.right(projects)); // Get the distinct values of the GROUP BY fields and the arguments // to the agg functions. relBuilder.push(aggregate.copy(aggregate.getTraitSet(), relBuilder.build(), false, ImmutableBitSet.range(projects.size()), null, com.google.common.collect.ImmutableList.<AggregateCall>of())); return relBuilder; }
put|public static void (String key, Object value) { contextMap.get().put(key, value); }
read array|public static byte[] (ReadableBuffer buffer) { Preconditions.checkNotNull(buffer, "buffer"); int length = buffer.readableBytes(); byte[] bytes = new byte[length]; buffer.readBytes(bytes, 0, length); return bytes; }
kill|@Deprecated public void (Map<String, String> modelEnvVars) throws InterruptedException { kill(null, modelEnvVars); }
get active scan|private ActiveScan (JSONObject params) throws ApiException { int id = getParam(params, PARAM_SCAN_ID, -1); GenericScanner2 activeScan = null; if (id == -1) { activeScan = controller.getLastScan(); } else { activeScan = controller.getScan(id); } if (activeScan == null) { throw new ApiException(ApiException.Type.DOES_NOT_EXIST, PARAM_SCAN_ID); } return (ActiveScan) activeScan; }
get json web key set|public static Optional<JsonWebKeySet> (final OidcRegisteredService service) { try { LOGGER.trace("Loading JSON web key from [{}]", service.getJwks()); val resource = getJsonWebKeySetResource(service); val jsonWebKeySet = buildJsonWebKeySet(resource); if (jsonWebKeySet == null || jsonWebKeySet.getJsonWebKeys().isEmpty()) { LOGGER.warn("No JSON web keys could be found for [{}]", service); return Optional.empty(); } val badKeysCount = jsonWebKeySet.getJsonWebKeys().stream().filter( k -> StringUtils.isBlank(k.getAlgorithm()) && StringUtils.isBlank(k.getKeyId()) && StringUtils.isBlank(k.getKeyType())).count(); if (badKeysCount == jsonWebKeySet.getJsonWebKeys().size()) { LOGGER.warn("No valid JSON web keys could be found for [{}]", service); return Optional.empty(); } val webKey = getJsonWebKeyFromJsonWebKeySet(jsonWebKeySet); if (Objects.requireNonNull(webKey).getPublicKey() == null) { LOGGER.warn("JSON web key retrieved [{}] has no associated public key", webKey.getKeyId()); return Optional.empty(); } return Optional.of(jsonWebKeySet); } catch (final Exception e) { LOGGER.error(e.getMessage(), e); } return Optional.empty(); }
generate image|public InputStream () { if (closed) { throw new ActivitiImageException("ProcessDiagramGenerator already closed"); } try { ByteArrayOutputStream stream = new ByteArrayOutputStream(); Writer out; out = new OutputStreamWriter(stream, "UTF-8"); g.stream(out, true); return new ByteArrayInputStream(stream.toByteArray()); } catch (UnsupportedEncodingException | SVGGraphics2DIOException e) { throw new ActivitiImageException("Error while generating process image", e); } }
ack|public Boolean (String taskId, String workerId) { Preconditions.checkArgument(StringUtils.isNotBlank(taskId), "Task id cannot be blank"); String response = postForEntity("tasks/{taskId}/ack", null, new Object[] { "workerid", workerId }, String.class, taskId); return Boolean.valueOf(response); }
highlight all|private void () { HighlighterManager highlighter = HighlighterManager.getInstance(); LinkedList<HighlightSearchEntry> highlights = highlighter.getHighlights(); for (HighlightSearchEntry entry : highlights) { highlightEntryParser(entry); } }
update table spec|public final TableSpec (TableSpec tableSpec) { UpdateTableSpecRequest request = UpdateTableSpecRequest.newBuilder().setTableSpec(tableSpec).build(); return updateTableSpec(request); }
verify saml profile request if needed|public void (final RequestAbstractType profileRequest, final SamlRegisteredServiceServiceProviderMetadataFacade adaptor, final HttpServletRequest request, final MessageContext context) throws Exception { verifySamlProfileRequestIfNeeded(profileRequest, adaptor.getMetadataResolver(), request, context); }
show param panel|public void (String name) { if (name == null || name.equals("")) { return; } // exit if panel name not found. AbstractParamPanel panel = tablePanel.get(name); if (panel == null) { return; } showParamPanel(panel, name); }
entries|@Override public Iterable<Map.Entry<K, V>> () { return Collections.unmodifiableSet(state.entrySet()); }
check task manager timeouts|@VisibleForTesting void () { if (!taskManagerRegistrations.isEmpty()) { long currentTime = System.currentTimeMillis(); ArrayList<TaskManagerRegistration> timedOutTaskManagers = new ArrayList<>(taskManagerRegistrations.size()); // first retrieve the timed out TaskManagers for (TaskManagerRegistration taskManagerRegistration : taskManagerRegistrations.values()) { if (currentTime - taskManagerRegistration.getIdleSince() >= taskManagerTimeout.toMilliseconds()) { // we collect the instance ids first in order to avoid concurrent modifications by the // ResourceActions.releaseResource call timedOutTaskManagers.add(taskManagerRegistration); } } // second we trigger the release resource callback which can decide upon the resource release for (TaskManagerRegistration taskManagerRegistration : timedOutTaskManagers) { InstanceID timedOutTaskManagerId = taskManagerRegistration.getInstanceId(); if (waitResultConsumedBeforeRelease) { // checking whether TaskManagers can be safely removed taskManagerRegistration.getTaskManagerConnection().getTaskExecutorGateway().canBeReleased().thenAcceptAsync( canBeReleased -> { if (canBeReleased) { releaseTaskExecutor(timedOutTaskManagerId); } }, mainThreadExecutor); } else { releaseTaskExecutor(timedOutTaskManagerId); } } } }
load|public void (ObjectInputStream in, V[] value) throws IOException, ClassNotFoundException { base = (int[]) in.readObject(); check = (int[]) in.readObject(); fail = (int[]) in.readObject(); output = (int[][]) in.readObject(); l = (int[]) in.readObject(); v = value; }
set current|static CLICommand (CLICommand cmd) { CLICommand old = getCurrent(); CURRENT_COMMAND.set(cmd); return old; }
append null for build|public void (JoinProbe probe) { // probe side appendProbeIndex(probe); // build side buildPageBuilder.declarePosition(); for (int i = 0; i < buildOutputChannelCount; i++) { buildPageBuilder.getBlockBuilder(i).appendNull(); } }
to bean|public <T> T (T bean, boolean isToCamelCase) { BeanUtil.fillBeanWithMap(this, bean, isToCamelCase, false); return bean; }
enable|public void () throws IOException { if (!disableFile.exists()) { LOGGER.log(Level.FINEST, "Plugin {0} has been already enabled. Skipping the enable() operation", getShortName()); return; } if (!disableFile.delete()) throw new IOException("Failed to delete " + disableFile); }
as|public OverWindow (Expression alias) { return new OverWindow(alias, partitionBy, orderBy, new CallExpression(BuiltInFunctionDefinitions.UNBOUNDED_RANGE, Collections.emptyList()), Optional.empty()); }
close|@Override public void (boolean compact, boolean cleanup) { logger.debug("close"); super.close(compact, cleanup); if (this.getDatabaseServer() == null) { return; } try { // shutdown ((HsqldbDatabaseServer) this.getDatabaseServer()).shutdown(compact); } catch (Exception e) { logger.error(e.getMessage(), e); } }
iterate sample|public double (T w1, T w2, double score) { INDArray w1Vector = syn0.slice(w1.getIndex()); INDArray w2Vector = syn0.slice(w2.getIndex()); //prediction: input + bias if (w1.getIndex() < 0 || w1.getIndex() >= syn0.rows()) throw new IllegalArgumentException("Illegal index for word " + w1.getLabel()); if (w2.getIndex() < 0 || w2.getIndex() >= syn0.rows()) throw new IllegalArgumentException("Illegal index for word " + w2.getLabel()); //w1 * w2 + bias double prediction = Nd4j.getBlasWrapper().dot(w1Vector, w2Vector); prediction += bias.getDouble(w1.getIndex()) + bias.getDouble(w2.getIndex()); double weight = Math.pow(Math.min(1.0, (score / maxCount)), xMax); double fDiff = score > xMax ? prediction : weight * (prediction - Math.log(score)); if (Double.isNaN(fDiff)) fDiff = Nd4j.EPS_THRESHOLD; //amount of change double gradient = fDiff; //note the update step here: the gradient is //the gradient of the OPPOSITE word //for adagrad we will use the index of the word passed in //for the gradient calculation we will use the context vector update(w1, w1Vector, w2Vector, gradient); update(w2, w2Vector, w1Vector, gradient); return fDiff; }
start book keeper client|private BookKeeper () throws Exception { // These two are in Seconds, not Millis. int writeTimeout = (int) Math.ceil(this.config.getBkWriteTimeoutMillis() / 1000.0); int readTimeout = (int) Math.ceil(this.config.getBkReadTimeoutMillis() / 1000.0); ClientConfiguration config = new ClientConfiguration().setClientTcpNoDelay(true).setAddEntryTimeout(writeTimeout).setReadEntryTimeout(readTimeout).setGetBookieInfoTimeout(readTimeout).setClientConnectTimeoutMillis((int) this.config.getZkConnectionTimeout().toMillis()).setZkTimeout((int) this.config.getZkConnectionTimeout().toMillis()); if (this.config.isTLSEnabled()) { config = (ClientConfiguration) config.setTLSProvider("OpenSSL"); config = config.setTLSTrustStore(this.config.getTlsTrustStore()); config.setTLSTrustStorePasswordPath(this.config.getTlsTrustStorePasswordPath()); } String metadataServiceUri = "zk://" + this.config.getZkAddress(); if (this.config.getBkLedgerPath().isEmpty()) { metadataServiceUri += "/" + this.namespace + "/bookkeeper/ledgers"; } else { metadataServiceUri += this.config.getBkLedgerPath(); } config.setMetadataServiceUri(metadataServiceUri); return new BookKeeper(config); }
create private key|public static PrivateKey (byte[] privateKey) { if (privateKey == null) { return null; } try { KeyFactory fac = KeyFactory.getInstance("RSA"); EncodedKeySpec spec = new PKCS8EncodedKeySpec(privateKey); return fac.generatePrivate(spec); } catch (NoSuchAlgorithmException e) { throw new IllegalStateException(e); } catch (InvalidKeySpecException e) { throw new IllegalStateException(e); } }
extract summary|public static List<String> (String document, int size, String sentence_separator) { return TextRankSentence.getTopSentenceList(document, size, sentence_separator); }
batch annotate images|public final BatchAnnotateImagesResponse (List<AnnotateImageRequest> requests) { BatchAnnotateImagesRequest request = BatchAnnotateImagesRequest.newBuilder().addAllRequests(requests).build(); return batchAnnotateImages(request); }
read vocab cache|public static VocabCache<VocabWord> (@NonNull File file) throws IOException { try (FileInputStream fis = new FileInputStream(file)) { return readVocabCache(fis); } }
viterbi decode|public double (Instance instance, int[] guessLabel) { final int[] allLabel = featureMap.allLabels(); final int bos = featureMap.bosTag(); final int sentenceLength = instance.tagArray.length; final int labelSize = allLabel.length; int[][] preMatrix = new int[sentenceLength][labelSize]; double[][] scoreMatrix = new double[2][labelSize]; for (int i = 0; i < sentenceLength; i++) { int _i = i & 1; int _i_1 = 1 - _i; int[] allFeature = instance.getFeatureAt(i); final int transitionFeatureIndex = allFeature.length - 1; if (0 == i) { allFeature[transitionFeatureIndex] = bos; for (int j = 0; j < allLabel.length; j++) { preMatrix[0][j] = j; double score = score(allFeature, j); scoreMatrix[0][j] = score; } } else { for (int curLabel = 0; curLabel < allLabel.length; curLabel++) { double maxScore = Integer.MIN_VALUE; for (int preLabel = 0; preLabel < allLabel.length; preLabel++) { allFeature[transitionFeatureIndex] = preLabel; double score = score(allFeature, curLabel); double curScore = scoreMatrix[_i_1][preLabel] + score; if (maxScore < curScore) { maxScore = curScore; preMatrix[i][curLabel] = preLabel; scoreMatrix[_i][curLabel] = maxScore; } } } } } int maxIndex = 0; double maxScore = scoreMatrix[(sentenceLength - 1) & 1][0]; for (int index = 1; index < allLabel.length; index++) { if (maxScore < scoreMatrix[(sentenceLength - 1) & 1][index]) { maxIndex = index; maxScore = scoreMatrix[(sentenceLength - 1) & 1][index]; } } for (int i = sentenceLength - 1; i >= 0; --i) { guessLabel[i] = allLabel[maxIndex]; maxIndex = preMatrix[i][maxIndex]; } return maxScore; }
run|@Override public Void () throws Exception { HadoopJobUtils.killAllSpawnedHadoopJobs(logFilePath, log); return null; }
get charset|public static Charset (HttpMessage message, Charset defaultCharset) { CharSequence contentTypeValue = message.headers().get(HttpHeaderNames.CONTENT_TYPE); if (contentTypeValue != null) { return getCharset(contentTypeValue, defaultCharset); } else { return defaultCharset; } }
add message|@Nonnull public StaticMessageSource (@Nonnull Locale locale, @Nonnull String code, @Nonnull String message) { ArgumentUtils.requireNonNull("locale", locale); if (StringUtils.isNotEmpty(code) && StringUtils.isNotEmpty(message)) { messageMap.put(new MessageKey(locale, code), message); } return this; }
prepare log event|public static LogEvent (final LogEvent logEvent) { val messageModified = TicketIdSanitizationUtils.sanitize(logEvent.getMessage().getFormattedMessage()); val message = new SimpleMessage(messageModified); val newLogEventBuilder = Log4jLogEvent.newBuilder().setLevel(logEvent.getLevel()).setLoggerName(logEvent.getLoggerName()).setLoggerFqcn(logEvent.getLoggerFqcn()).setContextData(new SortedArrayStringMap(logEvent.getContextData())).setContextStack(logEvent.getContextStack()).setEndOfBatch(logEvent.isEndOfBatch()).setIncludeLocation(logEvent.isIncludeLocation()).setMarker(logEvent.getMarker()).setMessage(message).setNanoTime(logEvent.getNanoTime()).setThreadName(logEvent.getThreadName()).setThrownProxy(logEvent.getThrownProxy()).setThrown(logEvent.getThrown()).setTimeMillis(logEvent.getTimeMillis()); try { newLogEventBuilder.setSource(logEvent.getSource()); } catch (final Exception e) { newLogEventBuilder.setSource(null); } return newLogEventBuilder.build(); }
project tuple|public <T0, T1, T2, T3, T4> SingleOutputStreamOperator<Tuple5<T0, T1, T2, T3, T4>> () { TypeInformation<?>[] fTypes = extractFieldTypes(fieldIndexes, dataStream.getType()); TupleTypeInfo<Tuple5<T0, T1, T2, T3, T4>> tType = new TupleTypeInfo<Tuple5<T0, T1, T2, T3, T4>>(fTypes); return dataStream.transform("Projection", tType, new StreamProject<IN, Tuple5<T0, T1, T2, T3, T4>>(fieldIndexes, tType.createSerializer(dataStream.getExecutionConfig()))); }
create internal type info from internal type|public static TypeInformation (InternalType type) { TypeInformation typeInfo = INTERNAL_TYPE_TO_INTERNAL_TYPE_INFO.get(type); if (typeInfo != null) { return typeInfo; } if (type instanceof RowType) { RowType rowType = (RowType) type; return new BaseRowTypeInfo(rowType.getFieldTypes(), rowType.getFieldNames()); } else if (type instanceof ArrayType) { return new BinaryArrayTypeInfo(((ArrayType) type).getElementType()); } else if (type instanceof MapType) { MapType mapType = (MapType) type; return new BinaryMapTypeInfo(mapType.getKeyType(), mapType.getValueType()); } else if (type instanceof DecimalType) { DecimalType decimalType = (DecimalType) type; return new DecimalTypeInfo(decimalType.precision(), decimalType.scale()); } else if (type instanceof GenericType) { GenericType<?> genericType = (GenericType<?>) type; return new BinaryGenericTypeInfo<>(genericType); } else { throw new UnsupportedOperationException("Not support yet: " + type); } }
list target https proxies|@BetaApi public final ListTargetHttpsProxiesPagedResponse (String project) { ListTargetHttpsProxiesHttpRequest request = ListTargetHttpsProxiesHttpRequest.newBuilder().setProject(project).build(); return listTargetHttpsProxies(request); }
to outcome matrix|public static INDArray (int[] index, long numOutcomes) { INDArray ret = Nd4j.create(index.length, numOutcomes); for (int i = 0; i < ret.rows(); i++) { int[] nums = new int[(int) numOutcomes]; nums[index[i]] = 1; ret.putRow(i, NDArrayUtil.toNDArray(nums)); } return ret; }
intercept forward|public static ServerServiceDefinition (ServerServiceDefinition serviceDef, List<? extends ServerInterceptor> interceptors) { List<? extends ServerInterceptor> copy = new ArrayList<>(interceptors); Collections.reverse(copy); return intercept(serviceDef, copy); }
do switch|public void () { AuthenticationInfo newRunningInfo = (runningInfo.equals(masterInfo) ? standbyInfo : masterInfo); this.doSwitch(newRunningInfo); }
get filtered list of servers|@Override public List<T> (List<T> servers) { List<T> zoneAffinityFiltered = super.getFilteredListOfServers(servers); Set<T> candidates = Sets.newHashSet(zoneAffinityFiltered); Set<T> newSubSet = Sets.newHashSet(currentSubset); LoadBalancerStats lbStats = getLoadBalancerStats(); for (T server : currentSubset) { // this server is either down or out of service if (!candidates.contains(server)) { newSubSet.remove(server); } else { ServerStats stats = lbStats.getSingleServerStat(server); // remove the servers that do not meet health criteria if (stats.getActiveRequestsCount() > eliminationConnectionCountThreshold.get() || stats.getFailureCount() > eliminationFailureCountThreshold.get()) { newSubSet.remove(server); // also remove from the general pool to avoid selecting them again candidates.remove(server); } } } int targetedListSize = sizeProp.get(); int numEliminated = currentSubset.size() - newSubSet.size(); int minElimination = (int) (targetedListSize * eliminationPercent.get()); int numToForceEliminate = 0; if (targetedListSize < newSubSet.size()) { // size is shrinking numToForceEliminate = newSubSet.size() - targetedListSize; } else if (minElimination > numEliminated) { numToForceEliminate = minElimination - numEliminated; } if (numToForceEliminate > newSubSet.size()) { numToForceEliminate = newSubSet.size(); } if (numToForceEliminate > 0) { List<T> sortedSubSet = Lists.newArrayList(newSubSet); Collections.sort(sortedSubSet, this); List<T> forceEliminated = sortedSubSet.subList(0, numToForceEliminate); newSubSet.removeAll(forceEliminated); candidates.removeAll(forceEliminated); } // then we just randomly add servers from the big pool if (newSubSet.size() < targetedListSize) { int numToChoose = targetedListSize - newSubSet.size(); candidates.removeAll(newSubSet); if (numToChoose > candidates.size()) { // Not enough healthy instances to choose, fallback to use the // total server pool candidates = Sets.newHashSet(zoneAffinityFiltered); candidates.removeAll(newSubSet); } List<T> chosen = randomChoose(Lists.newArrayList(candidates), numToChoose); for (T server : chosen) { newSubSet.add(server); } } currentSubset = newSubSet; return Lists.newArrayList(newSubSet); }
to windows command|public ArgumentListBuilder (boolean escapeVars) { ArgumentListBuilder windowsCommand = new ArgumentListBuilder().add("cmd.exe", "/C"); boolean quoted, percent; for (int i = 0; i < args.size(); i++) { StringBuilder quotedArgs = new StringBuilder(); String arg = args.get(i); quoted = percent = false; for (int j = 0; j < arg.length(); j++) { char c = arg.charAt(j); if (!quoted && (c == ' ' || c == '*' || c == '?' || c == ',' || c == ';')) { quoted = startQuoting(quotedArgs, arg, j); } else if (c == '^' || c == '&' || c == '<' || c == '>' || c == '|') { if (!quoted) quoted = startQuoting(quotedArgs, arg, j); // quotedArgs.append('^'); See note in javadoc above } else if (c == '"') { if (!quoted) quoted = startQuoting(quotedArgs, arg, j); quotedArgs.append('"'); } else if (percent && escapeVars && ((c >= 'A' && c <= 'Z') || (c >= 'a' && c <= 'z'))) { if (!quoted) quoted = startQuoting(quotedArgs, arg, j); quotedArgs.append('"').append(c); c = '"'; } percent = (c == '%'); if (quoted) quotedArgs.append(c); } if (i == 0 && quoted) quotedArgs.insert(0, '"'); else if (i == 0 && !quoted) quotedArgs.append('"'); if (quoted) quotedArgs.append('"'); else quotedArgs.append(arg); windowsCommand.add(quotedArgs, mask.get(i)); } // (comment copied from old code in hudson.tasks.Ant) // on Windows, executing batch file can't return the correct error code, // so we need to wrap it into cmd.exe. // double %% is needed because we want ERRORLEVEL to be expanded after // batch file executed, not before. This alone shows how broken Windows is... windowsCommand.add("&&").add("exit").add("%%ERRORLEVEL%%\""); return windowsCommand; }
set fuzzer optin|private void (boolean shouldOptin) { if (shouldOptin) { PassiveScanThread.addApplicableHistoryType(HistoryReference.TYPE_FUZZER); PassiveScanThread.addApplicableHistoryType(HistoryReference.TYPE_FUZZER_TEMPORARY); } else { PassiveScanThread.removeApplicableHistoryType(HistoryReference.TYPE_FUZZER); PassiveScanThread.removeApplicableHistoryType(HistoryReference.TYPE_FUZZER_TEMPORARY); } }
parse unique emails|public static Set<String> (final String emailList, final String splitRegex) { final Set<String> uniqueEmails = new HashSet<>(); if (emailList == null) { return uniqueEmails; } final String[] emails = emailList.trim().split(splitRegex); for (final String email : emails) { if (!email.isEmpty()) { uniqueEmails.add(email); } } return uniqueEmails; }
for files|public static FileBatch (List<File> files) throws IOException { List<String> origPaths = new ArrayList<>(files.size()); List<byte[]> bytes = new ArrayList<>(files.size()); for (File f : files) { bytes.add(FileUtils.readFileToByteArray(f)); origPaths.add(f.toURI().toString()); } return new FileBatch(bytes, origPaths); }
set iam policy snapshot|@BetaApi public final Policy (ProjectGlobalSnapshotResourceName resource, GlobalSetPolicyRequest globalSetPolicyRequestResource) { SetIamPolicySnapshotHttpRequest request = SetIamPolicySnapshotHttpRequest.newBuilder().setResource(resource == null ? null : resource.toString()).setGlobalSetPolicyRequestResource(globalSetPolicyRequestResource).build(); return setIamPolicySnapshot(request); }
event received|@Override public void (EventSubscriptionEntity eventSubscriptionEntity, Object payload, boolean processASync) { if (processASync) { scheduleEventAsync(eventSubscriptionEntity, payload); } else { processEventSync(eventSubscriptionEntity, payload); } }
to json|public String (Object src) { if (src == null) { return toJson(JsonNull.INSTANCE); } return toJson(src, src.getClass()); }
replace|public void (T from, T to) throws IOException { List<T> copy = new ArrayList<>(data.getView()); for (int i = 0; i < copy.size(); i++) { if (copy.get(i).equals(from)) copy.set(i, to); } data.replaceBy(copy); }
register|@Override public final void (Class<?>... annotatedClasses) { Assert.notEmpty(annotatedClasses, "At least one annotated class must be specified"); this.annotatedClasses.addAll(Arrays.asList(annotatedClasses)); }
touch|public static File (File file) throws IORuntimeException { if (null == file) { return null; } if (false == file.exists()) { mkParentDirs(file); try { file.createNewFile(); } catch (Exception e) { throw new IORuntimeException(e); } } return file; }
rebuild hetero|public void (StaplerRequest req, JSONObject formData, Collection<? extends Descriptor<T>> descriptors, String key) throws FormException, IOException { replaceBy(Descriptor.newInstancesFromHeteroList(req, formData, key, descriptors)); }
find stubbings|public static Set<Stubbing> (Iterable<?> mocks) { Set<Stubbing> stubbings = new TreeSet<Stubbing>(new StubbingComparator()); for (Object mock : mocks) { Collection<? extends Stubbing> fromSingleMock = new DefaultMockingDetails(mock).getStubbings(); stubbings.addAll(fromSingleMock); } return stubbings; }
get metadata segments|private JsonParserIterator<DataSegment> (DruidLeaderClient coordinatorClient, ObjectMapper jsonMapper, BytesAccumulatingResponseHandler responseHandler, Set<String> watchedDataSources) { String query = "/druid/coordinator/v1/metadata/segments"; if (watchedDataSources != null && !watchedDataSources.isEmpty()) { log.debug("filtering datasources in published segments based on broker's watchedDataSources[%s]", watchedDataSources); final StringBuilder sb = new StringBuilder(); for (String ds : watchedDataSources) { sb.append("datasources=").append(ds).append("&"); } sb.setLength(sb.length() - 1); query = "/druid/coordinator/v1/metadata/segments?" + sb; } Request request; try { request = coordinatorClient.makeRequest(HttpMethod.GET, StringUtils.format(query), false); } catch (IOException e) { throw new RuntimeException(e); } ListenableFuture<InputStream> future = coordinatorClient.goAsync(request, responseHandler); final JavaType typeRef = jsonMapper.getTypeFactory().constructType(new TypeReference<DataSegment>() { }); return new JsonParserIterator<>(typeRef, future, request.getUrl().toString(), null, request.getUrl().getHost(), jsonMapper, responseHandler); }
join url|public static String (String base, String[] paths) { StringBuilder result = new StringBuilder(base); Arrays.stream(paths).map( path -> path.replaceFirst("^(\\/)+", "")).forEach( p -> { if (result.lastIndexOf("/") != result.length() - 1) { result.append('/'); } result.append(p); }); return result.toString(); }
to summary|public Summary (String query) { List<Term> parse = NlpAnalysis.parse(query).getTerms(); List<Keyword> keywords = new ArrayList<>(); for (Term term : parse) { if (FILTER_SET.contains(term.natrue().natureStr)) { continue; } keywords.add(new Keyword(term.getName(), term.termNatures().allFreq, 1)); } return toSummary(keywords); }
get feature|@SuppressWarnings("PMD.NPathComplexity") protected Feature (JSONObject issue, Team board) { Feature feature = new Feature(); feature.setsId(getString(issue, "id")); feature.setsNumber(getString(issue, "key")); JSONObject fields = (JSONObject) issue.get("fields"); JSONObject epic = (JSONObject) fields.get("epic"); String epicId = getString(fields, featureSettings.getJiraEpicIdFieldName()); feature.setsEpicID(epic != null ? getString(epic, "id") : epicId); JSONObject issueType = (JSONObject) fields.get("issuetype"); if (issueType != null) { feature.setsTypeId(getString(issueType, "id")); feature.setsTypeName(getString(issueType, "name")); } JSONObject status = (JSONObject) fields.get("status"); String sStatus = getStatus(status); feature.setsState(sStatus); feature.setsStatus(feature.getsState()); String summary = getString(fields, "summary"); feature.setsName(summary); feature.setsUrl(featureSettings.getJiraBaseUrl() + (featureSettings.getJiraBaseUrl().endsWith("/") ? "" : "/") + "browse/" + feature.getsNumber()); long aggEstimate = getLong(fields, "aggregatetimeoriginalestimate"); Long estimate = getLong(fields, "timeoriginalestimate"); int originalEstimate = 0; // Tasks use timetracking, stories use aggregatetimeoriginalestimate and aggregatetimeestimate if (estimate != 0) { originalEstimate = estimate.intValue(); } else if (aggEstimate != 0) { // this value is in seconds originalEstimate = Math.round((float) aggEstimate / 3600); } feature.setsEstimateTime(originalEstimate); String storyPoints = getString(fields, featureSettings.getJiraStoryPointsFieldName()); feature.setsEstimate(storyPoints); feature.setChangeDate(getString(fields, "updated")); feature.setIsDeleted("False"); JSONObject project = (JSONObject) fields.get("project"); feature.setsProjectID(project != null ? getString(project, "id") : ""); feature.setsProjectName(project != null ? getString(project, "name") : ""); // sProjectBeginDate - does not exist in Jira feature.setsProjectBeginDate(""); // sProjectEndDate - does not exist in Jira feature.setsProjectEndDate(""); // sProjectChangeDate - does not exist for this asset level in Jira feature.setsProjectChangeDate(""); // sProjectState - does not exist in Jira feature.setsProjectState(""); // sProjectIsDeleted - does not exist in Jira feature.setsProjectIsDeleted("False"); // sProjectPath - does not exist in Jira feature.setsProjectPath(""); if (board != null) { feature.setsTeamID(board.getTeamId()); feature.setsTeamName(board.getName()); } else { JSONObject team = (JSONObject) fields.get(featureSettings.getJiraTeamFieldName()); if (team != null) { feature.setsTeamID(getString(team, "id")); feature.setsTeamName(getString(team, "value")); } } // sTeamChangeDate - not able to retrieve at this asset level from Jira feature.setsTeamChangeDate(""); // sTeamAssetState feature.setsTeamAssetState(""); // sTeamIsDeleted feature.setsTeamIsDeleted("False"); // sOwnersState - does not exist in Jira at this level feature.setsOwnersState(Collections.singletonList("Active")); // sOwnersChangeDate - does not exist in Jira feature.setsOwnersChangeDate(Collections.EMPTY_LIST); // sOwnersIsDeleted - does not exist in Jira feature.setsOwnersIsDeleted(Collections.EMPTY_LIST); // issueLinks JSONArray issueLinkArray = (JSONArray) fields.get("issuelinks"); feature.setIssueLinks(getIssueLinks(issueLinkArray)); Sprint sprint = getSprint(fields); if (sprint != null) { processSprintData(feature, sprint); } JSONObject assignee = (JSONObject) fields.get("assignee"); processAssigneeData(feature, assignee); return feature; }
delete metric async|public boolean (String metricName) throws ExecutionException, InterruptedException { // [START deleteMetricAsync] Future<Boolean> future = logging.deleteMetricAsync(metricName); // ... boolean deleted = future.get(); if (deleted) { // the metric was deleted } else { // the metric was not found } // [END deleteMetricAsync] return deleted; }
map|@Override public void (Chunk chk, NewChunk newChk) { if (chk instanceof C0DChunk) { // all NAs for (int i = 0; i < chk._len; i++) newChk.addNA(); } else { for (int i = 0; i < chk._len; i++) { if (!chk.isNA(i)) newChk.addStr(PrettyPrint.number(chk, chk.atd(i), 4)); else newChk.addNA(); } } }
add input|@Deprecated @SuppressWarnings("unchecked") public void (List<Operator<IN>> inputs) { this.input = Operator.createUnionCascade(this.input, inputs.toArray(new Operator[inputs.size()])); }
get context by param id|public static Context (JSONObject params, String contextIdParamName) throws ApiException { int contextId = getIntParam(params, contextIdParamName); Context context = Model.getSingleton().getSession().getContext(contextId); if (context == null) { throw new ApiException(Type.CONTEXT_NOT_FOUND, contextIdParamName); } return context; }
huber loss|public SDVariable (String name, @NonNull SDVariable label, @NonNull SDVariable predictions, SDVariable weights, @NonNull LossReduce lossReduce, double delta) { validateFloatingPoint("huber loss", "predictions", predictions); validateNumerical("huber loss", "labels", label); if (weights == null) weights = sd.scalar(null, predictions.dataType(), 1.0); SDVariable result = f().lossHuber(label, predictions, weights, lossReduce, delta); result = updateVariableNameAndReference(result, name); result.markAsLoss(); return result; }
create file batches spark|public static void (JavaRDD<String> filePaths, final String rootOutputDir, final int batchSize, JavaSparkContext sc) { createFileBatchesSpark(filePaths, rootOutputDir, batchSize, sc.hadoopConfiguration()); }
load|@Deprecated public static <T> T (ReaderHandler<T> readerHandler, String path, String charset) throws IORuntimeException { return FileReader.create(file(path), CharsetUtil.charset(charset)).read(readerHandler); }
list|// called through reflection by RequestServer @SuppressWarnings("unused") public water.automl.api.schemas3.LeaderboardsV99 (int version, water.automl.api.schemas3.LeaderboardsV99 s) { Leaderboards m = s.createAndFillImpl(); m.leaderboards = Leaderboards.fetchAll(); return s.fillFromImpl(m); }
set context lifecycle listeners|public void (Collection<? extends LifecycleListener> contextLifecycleListeners) { Assert.notNull(contextLifecycleListeners, "ContextLifecycleListeners must not be null"); this.contextLifecycleListeners = new ArrayList<>(contextLifecycleListeners); }
apply registrations|public static void (Kryo kryo, Collection<KryoRegistration> resolvedRegistrations) { Serializer<?> serializer; for (KryoRegistration registration : resolvedRegistrations) { serializer = registration.getSerializer(kryo); if (serializer != null) { kryo.register(registration.getRegisteredClass(), serializer, kryo.getNextRegistrationId()); } else { kryo.register(registration.getRegisteredClass(), kryo.getNextRegistrationId()); } } }
of|public static RegionAddressId (String project, String region, String address) { return new RegionAddressId(project, region, address); }
get project|public Project (final String name) { Project fetchedProject = this.projectsByName.get(name); if (fetchedProject == null) { try { fetchedProject = this.projectLoader.fetchProjectByName(name); if (fetchedProject != null) { logger.info("Project " + name + " not found in cache, fetched from DB."); } else { logger.info("No active project with name " + name + " exists in cache or DB."); } } catch (final ProjectManagerException e) { logger.error("Could not load project from store.", e); } } return fetchedProject; }
get btn delete|private JButton () { if (btnDelete == null) { btnDelete = new JButton(); btnDelete.setText(Constant.messages.getString("history.managetags.button.delete")); btnDelete.setMinimumSize(new java.awt.Dimension(75, 30)); btnDelete.setPreferredSize(new java.awt.Dimension(75, 30)); btnDelete.setMaximumSize(new java.awt.Dimension(100, 40)); btnDelete.setEnabled(true); btnDelete.addActionListener(new java.awt.event.ActionListener() { @Override public void actionPerformed(java.awt.event.ActionEvent e) { deleteTags(tagList.getSelectedValuesList()); } }); } return btnDelete; }
get bean for constructor argument|@SuppressWarnings("unused") @Internal @UsedByGeneratedCode protected final Object (BeanResolutionContext resolutionContext, BeanContext context, int argIndex) { ConstructorInjectionPoint<T> constructorInjectionPoint = getConstructor(); Argument<?> argument = constructorInjectionPoint.getArguments()[argIndex]; if (argument instanceof DefaultArgument) { argument = new EnvironmentAwareArgument((DefaultArgument) argument); instrumentAnnotationMetadata(context, argument); } Class argumentType = argument.getType(); if (argumentType == BeanResolutionContext.class) { return resolutionContext; } else if (argumentType.isArray()) { Collection beansOfType = getBeansOfTypeForConstructorArgument(resolutionContext, context, constructorInjectionPoint, argument); return beansOfType.toArray((Object[]) Array.newInstance(argumentType.getComponentType(), beansOfType.size())); } else if (Collection.class.isAssignableFrom(argumentType)) { Collection beansOfType = getBeansOfTypeForConstructorArgument(resolutionContext, context, constructorInjectionPoint, argument); return coerceCollectionToCorrectType(argumentType, beansOfType); } else if (Stream.class.isAssignableFrom(argumentType)) { return streamOfTypeForConstructorArgument(resolutionContext, context, constructorInjectionPoint, argument); } else if (Provider.class.isAssignableFrom(argumentType)) { return getBeanProviderForConstructorArgument(resolutionContext, context, constructorInjectionPoint, argument); } else if (Optional.class.isAssignableFrom(argumentType)) { return findBeanForConstructorArgument(resolutionContext, context, constructorInjectionPoint, argument); } else { BeanResolutionContext.Path path = resolutionContext.getPath(); BeanResolutionContext.Segment current = path.peek(); boolean isNullable = argument.isDeclaredAnnotationPresent(Nullable.class); if (isNullable && current != null && current.getArgument().equals(argument)) { return null; } else { path.pushConstructorResolve(this, argument); try { Object bean; Qualifier qualifier = resolveQualifier(resolutionContext, argument, isInnerConfiguration(argumentType)); //noinspection unchecked bean = ((DefaultBeanContext) context).getBean(resolutionContext, argumentType, qualifier); path.pop(); return bean; } catch (NoSuchBeanException e) { if (isNullable) { path.pop(); return null; } throw new DependencyInjectionException(resolutionContext, argument, e); } } } }
not followed by|public Pattern<T, T> (final String name) { if (quantifier.hasProperty(Quantifier.QuantifierProperty.OPTIONAL)) { throw new UnsupportedOperationException("Specifying a pattern with an optional path to NOT condition is not supported yet. " + "You can simulate such pattern with two independent patterns, one with and the other without " + "the optional part."); } return new Pattern<>(name, this, ConsumingStrategy.NOT_FOLLOW, afterMatchSkipStrategy); }
read int|public static int (InputStream source) throws IOException { int b1 = source.read(); int b2 = source.read(); int b3 = source.read(); int b4 = source.read(); if ((b1 | b2 | b3 | b4) < 0) { throw new EOFException(); } else { return (b1 << 24) + (b2 << 16) + (b3 << 8) + b4; } }
create launcher|@Nonnull public Launcher (TaskListener listener) { SlaveComputer c = getComputer(); if (c == null) { listener.error("Issue with creating launcher for agent " + name + ". Computer has been disconnected"); return new Launcher.DummyLauncher(listener); } else { // TODO: ideally all the logic below should be inside the SlaveComputer class with proper locking to prevent race conditions, // but so far there is no locks for setNode() hence it requires serious refactoring // Ensure that the Computer instance still points to this node // Otherwise we may end up running the command on a wrong (reconnected) Node instance. Slave node = c.getNode(); if (node != this) { String message = "Issue with creating launcher for agent " + name + ". Computer has been reconnected"; if (LOGGER.isLoggable(Level.WARNING)) { LOGGER.log(Level.WARNING, message, new IllegalStateException("Computer has been reconnected, this Node instance cannot be used anymore")); } return new Launcher.DummyLauncher(listener); } // RemoteLauncher requires an active Channel instance to operate correctly final Channel channel = c.getChannel(); if (channel == null) { reportLauncherCreateError("The agent has not been fully initialized yet", "No remoting channel to the agent OR it has not been fully initialized yet", listener); return new Launcher.DummyLauncher(listener); } if (channel.isClosingOrClosed()) { reportLauncherCreateError("The agent is being disconnected", "Remoting channel is either in the process of closing down or has closed down", listener); return new Launcher.DummyLauncher(listener); } final Boolean isUnix = c.isUnix(); if (isUnix == null) { // isUnix is always set when the channel is not null, so it should never happen reportLauncherCreateError("The agent has not been fully initialized yet", "Cannot determing if the agent is a Unix one, the System status request has not completed yet. " + "It is an invalid channel state, please report a bug to Jenkins if you see it.", listener); return new Launcher.DummyLauncher(listener); } return new RemoteLauncher(listener, channel, isUnix).decorateFor(this); } }
copy long|public static ByteBuf (long... values) { if (values == null || values.length == 0) { return EMPTY_BUFFER; } ByteBuf buffer = buffer(values.length * 8); for (long v : values) { buffer.writeLong(v); } return buffer; }
connection dropped|@Override public void () { log.warn(requestId, "readTable {} Connection dropped", tableName); result.completeExceptionally(new WireCommandFailedException(type, WireCommandFailedException.Reason.ConnectionDropped)); }
of|static ConfigurationPropertyName (CharSequence name, boolean returnNullIfInvalid) { Elements elements = elementsOf(name, returnNullIfInvalid); return (elements != null) ? new ConfigurationPropertyName(elements) : null; }
list usable subnetworks|@BetaApi public final ListUsableSubnetworksPagedResponse (ProjectName project) { ListUsableSubnetworksHttpRequest request = ListUsableSubnetworksHttpRequest.newBuilder().setProject(project == null ? null : project.toString()).build(); return listUsableSubnetworks(request); }
is all suitable nodes offline|private boolean (R build) { Label label = getAssignedLabel(); List<Node> allNodes = Jenkins.getInstance().getNodes(); if (label != null) { //Invalid label. Put in queue to make administrator fix if (label.getNodes().isEmpty()) { return false; } //Returns true, if all suitable nodes are offline return label.isOffline(); } else { if (canRoam) { for (Node n : Jenkins.getInstance().getNodes()) { Computer c = n.toComputer(); if (c != null && c.isOnline() && c.isAcceptingTasks() && n.getMode() == Mode.NORMAL) { // Some executor is online that is ready and this job can run anywhere return false; } } //We can roam, check that the master is set to be used as much as possible, and not tied jobs only. return Jenkins.getInstance().getMode() == Mode.EXCLUSIVE; } } return true; }
single|public synchronized void (long weight) throws InterruptedException { this.weights.remove(weight); // 触发下一个可运行的weight Long nextWeight = this.weights.peek(); if (nextWeight != null) { barrier.single(nextWeight); } }
sample|public static <T> MapPartitionOperator<T, T> (DataSet<T> input, final boolean withReplacement, final double fraction, final long seed) { return input.mapPartition(new SampleWithFraction<T>(withReplacement, fraction, seed)); }
intercept forward|public static ServerServiceDefinition (ServerServiceDefinition serviceDef, ServerInterceptor... interceptors) { return interceptForward(serviceDef, Arrays.asList(interceptors)); }
sanitize default port|public static String (String url) { int afterSchemeIndex = url.indexOf("://"); if (afterSchemeIndex < 0) { return url; } String scheme = url.substring(0, afterSchemeIndex); int fromIndex = scheme.length() + 3; //Let's see if it is an IPv6 Address int ipv6StartIndex = url.indexOf('[', fromIndex); if (ipv6StartIndex > 0) { fromIndex = url.indexOf(']', ipv6StartIndex); } int portIndex = url.indexOf(':', fromIndex); if (portIndex >= 0) { int port = Integer.parseInt(url.substring(portIndex + 1)); if (isDefaultPort(port, scheme)) { return url.substring(0, portIndex); } } return url; }
hash|public INDArray (INDArray data) { if (data.shape()[1] != inDimension) { throw new ND4JIllegalStateException(String.format("Invalid shape: Requested INDArray shape %s, this table expects dimension %d", Arrays.toString(data.shape()), inDimension)); } INDArray projected = data.mmul(randomProjection); INDArray res = Nd4j.getExecutioner().exec(new Sign(projected)); return res; }
wrong host|@Override public void (WireCommands.WrongHost wrongHost) { log.warn(requestId, "readTable {} wrong host", tableName); result.completeExceptionally(new WireCommandFailedException(type, WireCommandFailedException.Reason.UnknownHost)); }
reload library|public static void (String key) { if (key.startsWith(DicLibrary.DEFAULT)) { DicLibrary.reload(key); } else if (key.startsWith(StopLibrary.DEFAULT)) { StopLibrary.reload(key); } else if (key.startsWith(SynonymsLibrary.DEFAULT)) { SynonymsLibrary.reload(key); } else if (key.startsWith(AmbiguityLibrary.DEFAULT)) { AmbiguityLibrary.reload(key); } else if (key.startsWith(CrfLibrary.DEFAULT)) { CrfLibrary.reload(key); } else { throw new LibraryException(key + " type err must start with dic,stop,ambiguity,synonyms"); } }
no such segment|@Override public void (WireCommands.NoSuchSegment noSuchSegment) { log.warn(requestId, "readTable {} NoSuchSegment", tableName); result.completeExceptionally(new WireCommandFailedException(type, WireCommandFailedException.Reason.SegmentDoesNotExist)); }
offset date|@Deprecated public static DateTime (Date date, DateField dateField, int offset) { return offset(date, dateField, offset); }
check subtract|public static boolean (INDArray first, INDArray second, double maxRelativeDifference, double minAbsDifference) { RealMatrix rmFirst = convertToApacheMatrix(first); RealMatrix rmSecond = convertToApacheMatrix(second); INDArray result = first.sub(second); RealMatrix rmResult = rmFirst.subtract(rmSecond); if (!checkShape(rmResult, result)) return false; boolean ok = checkEntries(rmResult, result, maxRelativeDifference, minAbsDifference); if (!ok) { INDArray onCopies = Shape.toOffsetZeroCopy(first).sub(Shape.toOffsetZeroCopy(second)); printFailureDetails(first, second, rmResult, result, onCopies, "sub"); } return ok; }
start|void () { Runnable r; for (int i = 0; i < threadCount; i++) { r = null; switch(config.getPayloadConfig().getPayloadCase()) { case SIMPLE_PARAMS: { if (config.getClientType() == Control.ClientType.SYNC_CLIENT) { if (config.getRpcType() == Control.RpcType.UNARY) { r = new BlockingUnaryWorker(blockingStubs[i % blockingStubs.length]); } } else if (config.getClientType() == Control.ClientType.ASYNC_CLIENT) { if (config.getRpcType() == Control.RpcType.UNARY) { r = new AsyncUnaryWorker(asyncStubs[i % asyncStubs.length]); } else if (config.getRpcType() == Control.RpcType.STREAMING) { r = new AsyncPingPongWorker(asyncStubs[i % asyncStubs.length]); } } break; } case BYTEBUF_PARAMS: { if (config.getClientType() == Control.ClientType.SYNC_CLIENT) { if (config.getRpcType() == Control.RpcType.UNARY) { r = new GenericBlockingUnaryWorker(channels[i % channels.length]); } } else if (config.getClientType() == Control.ClientType.ASYNC_CLIENT) { if (config.getRpcType() == Control.RpcType.UNARY) { r = new GenericAsyncUnaryWorker(channels[i % channels.length]); } else if (config.getRpcType() == Control.RpcType.STREAMING) { r = new GenericAsyncPingPongWorker(channels[i % channels.length]); } } break; } default: { throw Status.UNIMPLEMENTED.withDescription("Unknown payload case " + config.getPayloadConfig().getPayloadCase().name()).asRuntimeException(); } } if (r == null) { throw new IllegalStateException(config.getRpcType().name() + " not supported for client type " + config.getClientType()); } fixedThreadPool.execute(r); } if (osBean != null) { lastMarkCpuTime = osBean.getProcessCpuTime(); } }
download file|public static long (String url, String dest) { return downloadFile(url, FileUtil.file(dest)); }
get num of renews in last min|@com.netflix.servo.annotations.Monitor(name = "numOfRenewsInLastMin", description = "Number of total heartbeats received in the last minute", type = DataSourceType.GAUGE) @Override public long () { return renewsLastMin.getCount(); }
accumulate chunk|@Override public void (NDArrayMessageChunk chunk) { String id = chunk.getId(); if (!chunks.containsKey(id)) { List<NDArrayMessageChunk> list = new ArrayList<>(); list.add(chunk); chunks.put(id, list); } else { List<NDArrayMessageChunk> chunkList = chunks.get(id); chunkList.add(chunk); } log.debug("Accumulating chunk for id " + chunk.getId()); }
write|public AioSession (ByteBuffer data, CompletionHandler<Integer, AioSession> handler) { this.channel.write(data, Math.max(this.writeTimeout, 0L), TimeUnit.MILLISECONDS, this, handler); return this; }
is null or whitespace only|public static boolean (String str) { if (str == null || str.length() == 0) { return true; } final int len = str.length(); for (int i = 0; i < len; i++) { if (!Character.isWhitespace(str.charAt(i))) { return false; } } return true; }
create cipher|public static Cipher (String algorithm) { final Provider provider = GlobalBouncyCastleProvider.INSTANCE.getProvider(); Cipher cipher; try { cipher = (null == provider) ? Cipher.getInstance(algorithm) : Cipher.getInstance(algorithm, provider); } catch (Exception e) { throw new CryptoException(e); } return cipher; }
update summary|void (CompletedCheckpointStats completed) { stateSize.add(completed.getStateSize()); duration.add(completed.getEndToEndDuration()); alignmentBuffered.add(completed.getAlignmentBuffered()); }
previous schema serializer|@Nonnull public final TypeSerializer<T> () { if (cachedRestoredSerializer != null) { return cachedRestoredSerializer; } if (previousSerializerSnapshot == null) { throw new UnsupportedOperationException("This provider does not contain the state's previous serializer's snapshot. Cannot provider a serializer for previous schema."); } this.cachedRestoredSerializer = previousSerializerSnapshot.restoreSerializer(); return cachedRestoredSerializer; }
service http client|@EachBean(ServiceHttpClientConfiguration.class) @Requires(condition = ServiceHttpClientCondition.class) DefaultHttpClient (@Parameter ServiceHttpClientConfiguration configuration, @Parameter StaticServiceInstanceList instanceList) { List<URI> originalURLs = configuration.getUrls(); Collection<URI> loadBalancedURIs = instanceList.getLoadBalancedURIs(); boolean isHealthCheck = configuration.isHealthCheck(); LoadBalancer loadBalancer = loadBalancerFactory.create(instanceList); Optional<String> path = configuration.getPath(); DefaultHttpClient httpClient; if (path.isPresent()) { httpClient = beanContext.createBean(DefaultHttpClient.class, loadBalancer, configuration, path.get()); } else { httpClient = beanContext.createBean(DefaultHttpClient.class, loadBalancer, configuration); } httpClient.setClientIdentifiers(configuration.getServiceId()); if (isHealthCheck) { taskScheduler.scheduleWithFixedDelay(configuration.getHealthCheckInterval(), configuration.getHealthCheckInterval(), () -> Flowable.fromIterable(originalURLs).flatMap( originalURI -> { URI healthCheckURI = originalURI.resolve(configuration.getHealthCheckUri()); return httpClient.exchange(HttpRequest.GET(healthCheckURI)).onErrorResumeNext( throwable -> { if (throwable instanceof HttpClientResponseException) { HttpClientResponseException responseException = (HttpClientResponseException) throwable; HttpResponse response = responseException.getResponse(); return Flowable.just(response); } return Flowable.just(HttpResponse.serverError()); }).map( response -> Collections.singletonMap(originalURI, response.getStatus())); }).subscribe( uriToStatusMap -> { Map.Entry<URI, HttpStatus> entry = uriToStatusMap.entrySet().iterator().next(); URI uri = entry.getKey(); HttpStatus status = entry.getValue(); if (status.getCode() >= 300) { loadBalancedURIs.remove(uri); } else if (!loadBalancedURIs.contains(uri)) { loadBalancedURIs.add(uri); } })); } return httpClient; }
close|@Override public void () throws IOException { // Skip over the remaining bytes. Do not close the underlying InputStream. if (this.remaining > 0) { int toSkip = this.remaining; long skipped = skip(toSkip); if (skipped != toSkip) { throw new SerializationException(String.format("Read %d fewer byte(s) than expected only able to skip %d.", toSkip, skipped)); } } else if (this.remaining < 0) { throw new SerializationException(String.format("Read more bytes than expected (%d).", -this.remaining)); } }
action performed|@Override public void (java.awt.event.ActionEvent e) { deleteTags(tagList.getSelectedValuesList()); }
get response baggage|public String (String key) { if (BAGGAGE_ENABLE && key != null) { return responseBaggage.get(key); } return null; }
min by|@SuppressWarnings({ "unchecked", "rawtypes" }) public ReduceOperator<T> (int... fields) { if (!getType().isTupleType()) { throw new InvalidProgramException("DataSet#minBy(int...) only works on Tuple types."); } return new ReduceOperator<>(this, new SelectByMinFunction((TupleTypeInfo) getType(), fields), Utils.getCallLocationName()); }
get|public String (CharSequence name, String defaultValue) { String value = get(name); if (value == null) { return defaultValue; } return value; }
execute event|private List<EventExecution> (String event, Message msg) throws Exception { List<EventHandler> eventHandlerList = metadataService.getEventHandlersForEvent(event, true); Object payloadObject = getPayloadObject(msg.getPayload()); List<EventExecution> transientFailures = new ArrayList<>(); for (EventHandler eventHandler : eventHandlerList) { String condition = eventHandler.getCondition(); if (StringUtils.isNotEmpty(condition)) { logger.debug("Checking condition: {} for event: {}", condition, event); Boolean success = ScriptEvaluator.evalBool(condition, jsonUtils.expand(payloadObject)); if (!success) { String id = msg.getId() + "_" + 0; EventExecution eventExecution = new EventExecution(id, msg.getId()); eventExecution.setCreated(System.currentTimeMillis()); eventExecution.setEvent(eventHandler.getEvent()); eventExecution.setName(eventHandler.getName()); eventExecution.setStatus(Status.SKIPPED); eventExecution.getOutput().put("msg", msg.getPayload()); eventExecution.getOutput().put("condition", condition); executionService.addEventExecution(eventExecution); logger.debug("Condition: {} not successful for event: {} with payload: {}", condition, eventHandler.getEvent(), msg.getPayload()); continue; } } CompletableFuture<List<EventExecution>> future = executeActionsForEventHandler(eventHandler, msg); future.whenComplete(( result, error) -> result.forEach( eventExecution -> { if (error != null || eventExecution.getStatus() == Status.IN_PROGRESS) { executionService.removeEventExecution(eventExecution); transientFailures.add(eventExecution); } else { executionService.updateEventExecution(eventExecution); } })).get(); } return transientFailures; }
get popup menu find|private PopupFindMenu () { if (popupFindMenu == null) { popupFindMenu = new PopupFindMenu(); // ZAP: i18n popupFindMenu.setText(Constant.messages.getString("edit.find.popup")); popupFindMenu.addActionListener(new java.awt.event.ActionListener() { @Override public void actionPerformed(java.awt.event.ActionEvent e) { JTextComponent component = popupFindMenu.getLastInvoker(); Window window = getWindowAncestor(component); if (window != null) { showFindDialog(window, component); } } }); } return popupFindMenu; }
version info string|public static String (Detail detail) { StringBuilder sb = new StringBuilder(); for (VersionInfo grp : getVersionInfos()) { sb.append(grp.getGroupId()).append(" : ").append(grp.getArtifactId()).append(" : ").append(grp.getBuildVersion()); switch(detail) { case FULL: case GAVC: sb.append(" - ").append(grp.getCommitIdAbbrev()); if (detail != Detail.FULL) break; sb.append("buildTime=").append(grp.getBuildTime()).append("branch=").append(grp.getBranch()).append("commitMsg=").append(grp.getCommitMessageShort()); } sb.append("\n"); } return sb.toString(); }
mem or load|public final byte[] () { byte[] // Read once! mem = _mem; if (mem != null) return mem; // Read once! Freezable pojo = _pojo; if (// Has the POJO, make raw bytes pojo != null) return _mem = pojo.asBytes(); if (_max == 0) return (_mem = new byte[0]); return (_mem = loadPersist()); }
convert primitive field|@Nullable private static Object (Group g, int fieldIndex, boolean binaryAsString) { PrimitiveType pt = (PrimitiveType) g.getType().getFields().get(fieldIndex); if (pt.isRepetition(Type.Repetition.REPEATED) && g.getFieldRepetitionCount(fieldIndex) > 1) { List<Object> vals = new ArrayList<>(); for (int i = 0; i < g.getFieldRepetitionCount(fieldIndex); i++) { vals.add(convertPrimitiveField(g, fieldIndex, i, binaryAsString)); } return vals; } return convertPrimitiveField(g, fieldIndex, 0, binaryAsString); }
pull last time steps|public static Pair<INDArray, int[]> (INDArray pullFrom, INDArray mask) { //Also: record where they came from (so we can do errors later) int[] fwdPassTimeSteps; INDArray out; if (mask == null) { // FIXME: int cast //No mask array -> extract same (last) column for all int lastTS = (int) pullFrom.size(2) - 1; out = pullFrom.get(NDArrayIndex.all(), NDArrayIndex.all(), NDArrayIndex.point(lastTS)); //Null -> last time step for all examples fwdPassTimeSteps = null; } else { val outShape = new long[] { pullFrom.size(0), pullFrom.size(1) }; out = Nd4j.create(outShape); //Want the index of the last non-zero entry in the mask array INDArray lastStepArr = BooleanIndexing.lastIndex(mask, Conditions.epsNotEquals(0.0), 1); fwdPassTimeSteps = lastStepArr.data().asInt(); //Now, get and assign the corresponding subsets of 3d activations: for (int i = 0; i < fwdPassTimeSteps.length; i++) { //TODO can optimize using reshape + pullRows out.putRow(i, pullFrom.get(NDArrayIndex.point(i), NDArrayIndex.all(), NDArrayIndex.point(fwdPassTimeSteps[i]))); } } return new Pair<>(out, fwdPassTimeSteps); }
replace ip with domain name|public URLNormalizer () { URL u = toURL(); if (!PATTERN_DOMAIN.matcher(u.getHost()).matches()) { try { InetAddress addr = InetAddress.getByName(u.getHost()); String host = addr.getHostName(); if (!u.getHost().equalsIgnoreCase(host)) { url = url.replaceFirst(u.getHost(), host); } } catch (UnknownHostException e) { logger.debug("Cannot resolve IP to host for :" + u.getHost(), e); } } return this; }
report failed checkpoint|void (long failureTimestamp, @Nullable Throwable cause) { FailedCheckpointStats failed = new FailedCheckpointStats(checkpointId, triggerTimestamp, props, numberOfSubtasks, new HashMap<>(taskStats), currentNumAcknowledgedSubtasks, currentStateSize, currentAlignmentBuffered, failureTimestamp, latestAcknowledgedSubtask, cause); trackerCallback.reportFailedCheckpoint(failed); }
collect|@Override public List<MetricFamilySamples> () { final GaugeMetricFamily states = new GaugeMetricFamily(prefix + "_states", "Circuit Breaker States", asList("name", "state")); final GaugeMetricFamily calls = new GaugeMetricFamily(prefix + "_calls", "Circuit Breaker Call Stats", asList("name", "call_result")); for (CircuitBreaker circuitBreaker : circuitBreakersSupplier.get()) { STATE_NAME_MAP.forEach( e -> { final CircuitBreaker.State state = e._1; final String name = e._2; final double value = state == circuitBreaker.getState() ? 1.0 : 0.0; states.addMetric(asList(circuitBreaker.getName(), name), value); }); final CircuitBreaker.Metrics metrics = circuitBreaker.getMetrics(); calls.addMetric(asList(circuitBreaker.getName(), "successful"), metrics.getNumberOfSuccessfulCalls()); calls.addMetric(asList(circuitBreaker.getName(), "failed"), metrics.getNumberOfFailedCalls()); calls.addMetric(asList(circuitBreaker.getName(), "not_permitted"), metrics.getNumberOfNotPermittedCalls()); calls.addMetric(asList(circuitBreaker.getName(), "buffered"), metrics.getNumberOfBufferedCalls()); calls.addMetric(asList(circuitBreaker.getName(), "buffered_max"), metrics.getMaxNumberOfBufferedCalls()); } return asList(calls, states); }
get tenant|public final Tenant (TenantName name) { GetTenantRequest request = GetTenantRequest.newBuilder().setName(name == null ? null : name.toString()).build(); return getTenant(request); }
insert backend service|@BetaApi public final Operation (String project, BackendService backendServiceResource) { InsertBackendServiceHttpRequest request = InsertBackendServiceHttpRequest.newBuilder().setProject(project).setBackendServiceResource(backendServiceResource).build(); return insertBackendService(request); }
find throwable with message|public static Optional<Throwable> (Throwable throwable, String searchMessage) { if (throwable == null || searchMessage == null) { return Optional.empty(); } Throwable t = throwable; while (t != null) { if (t.getMessage() != null && t.getMessage().contains(searchMessage)) { return Optional.of(t); } else { t = t.getCause(); } } return Optional.empty(); }
set broadcast inputs|public void (Map<Operator<?>, OptimizerNode> operatorToNode, ExecutionMode defaultExchangeMode) { // skip for Operators that don't support broadcast variables if (!(getOperator() instanceof AbstractUdfOperator<?, ?>)) { return; } // get all broadcast inputs AbstractUdfOperator<?, ?> operator = ((AbstractUdfOperator<?, ?>) getOperator()); // create connections and add them for (Map.Entry<String, Operator<?>> input : operator.getBroadcastInputs().entrySet()) { OptimizerNode predecessor = operatorToNode.get(input.getValue()); DagConnection connection = new DagConnection(predecessor, this, ShipStrategyType.BROADCAST, defaultExchangeMode); addBroadcastConnection(input.getKey(), connection); predecessor.addOutgoingConnection(connection); } }
print and log version|private static void (String[] arguments) { Log.init(ARGS.log_level, ARGS.quiet); Log.info("----- H2O started " + (ARGS.client ? "(client)" : "") + " -----"); Log.info("Build git branch: " + ABV.branchName()); Log.info("Build git hash: " + ABV.lastCommitHash()); Log.info("Build git describe: " + ABV.describe()); Log.info("Build project version: " + ABV.projectVersion()); Log.info("Build age: " + PrettyPrint.toAge(ABV.compiledOnDate(), new Date())); Log.info("Built by: '" + ABV.compiledBy() + "'"); Log.info("Built on: '" + ABV.compiledOn() + "'"); if (ABV.isTooOld()) { Log.warn("\n*** Your H2O version is too old! Please download the latest version from http://h2o.ai/download/ ***"); Log.warn(""); } Log.info("Found H2O Core extensions: " + extManager.getCoreExtensions()); Log.info("Processed H2O arguments: ", Arrays.toString(arguments)); Runtime runtime = Runtime.getRuntime(); Log.info("Java availableProcessors: " + runtime.availableProcessors()); Log.info("Java heap totalMemory: " + PrettyPrint.bytes(runtime.totalMemory())); Log.info("Java heap maxMemory: " + PrettyPrint.bytes(runtime.maxMemory())); Log.info("Java version: Java " + System.getProperty("java.version") + " (from " + System.getProperty("java.vendor") + ")"); List<String> launchStrings = ManagementFactory.getRuntimeMXBean().getInputArguments(); Log.info("JVM launch parameters: " + launchStrings); Log.info("OS version: " + System.getProperty("os.name") + " " + System.getProperty("os.version") + " (" + System.getProperty("os.arch") + ")"); long totalMemory = OSUtils.getTotalPhysicalMemory(); Log.info("Machine physical memory: " + (totalMemory == -1 ? "NA" : PrettyPrint.bytes(totalMemory))); }
action performed|@Override public void (java.awt.event.ActionEvent e) { JTextComponent component = popupFindMenu.getLastInvoker(); Window window = getWindowAncestor(component); if (window != null) { showFindDialog(window, component); } }
get all|public static List<EntityField> (Class<?> entityClass) { List<EntityField> fields = fieldHelper.getFields(entityClass); List<EntityField> properties = fieldHelper.getProperties(entityClass); //拼到一起，名字相同的合并 List<EntityField> all = new ArrayList<EntityField>(); Set<EntityField> usedSet = new HashSet<EntityField>(); for (EntityField field : fields) { for (EntityField property : properties) { if (!usedSet.contains(property) && field.getName().equals(property.getName())) { field.copyFromPropertyDescriptor(property); usedSet.add(property); break; } } all.add(field); } for (EntityField property : properties) { if (!usedSet.contains(property)) { all.add(property); } } return all; }
load file|@Nonnull @Deprecated public static String (@Nonnull File logfile) throws IOException { return loadFile(logfile, Charset.defaultCharset()); }
press text|public static void (ImageInputStream srcStream, ImageOutputStream destStream, String pressText, Color color, Font font, int x, int y, float alpha) { pressText(read(srcStream), destStream, pressText, color, font, x, y, alpha); }
get selected session|public HttpSession () { final int selectedRow = this.sessionsTable.getSelectedRow(); if (selectedRow == -1) { // No row selected return null; } final int rowIndex = sessionsTable.convertRowIndexToModel(selectedRow); return this.sessionsModel.getHttpSessionAt(rowIndex); }
get skip expression from service task|protected Expression (ServiceTask serviceTask) { Expression result = null; if (StringUtils.isNotEmpty(serviceTask.getSkipExpression())) { result = expressionManager.createExpression(serviceTask.getSkipExpression()); } return result; }
split to list|private List<String> (String input) { Preconditions.checkNotNull(input); Iterator<String> iterator = splitter.split(input).iterator(); List<String> result = new ArrayList<String>(); while (iterator.hasNext()) { result.add(iterator.next()); } return Collections.unmodifiableList(result); }
do read bytes|protected final int (ByteBuf byteBuf) throws Exception { int writerIndex = byteBuf.writerIndex(); int localReadAmount; unsafe().recvBufAllocHandle().attemptedBytesRead(byteBuf.writableBytes()); if (byteBuf.hasMemoryAddress()) { localReadAmount = socket.readAddress(byteBuf.memoryAddress(), writerIndex, byteBuf.capacity()); } else { ByteBuffer buf = byteBuf.internalNioBuffer(writerIndex, byteBuf.writableBytes()); localReadAmount = socket.read(buf, buf.position(), buf.limit()); } if (localReadAmount > 0) { byteBuf.writerIndex(writerIndex + localReadAmount); } return localReadAmount; }
notify checkpoint complete|public void (long checkpointId, long timestamp) { final LogicalSlot slot = assignedResource; if (slot != null) { final TaskManagerGateway taskManagerGateway = slot.getTaskManagerGateway(); taskManagerGateway.notifyCheckpointComplete(attemptId, getVertex().getJobId(), checkpointId, timestamp); } else { LOG.debug("The execution has no slot assigned. This indicates that the execution is " + "no longer running."); } }
wrap|public static <T> Iterable<T> (final Iterable<T> base) { return new Iterable<T>() { public Iterator<T> iterator() { final Iterator<T> itr = base.iterator(); return new Iterator<T>() { public boolean hasNext() { return itr.hasNext(); } public T next() { return itr.next(); } public void remove() { itr.remove(); } }; } }; }
find data media pair by source name|public static DataMediaPair (Pipeline pipeline, String namespace, String name) { return findDataMediaPairBySourceName(pipeline, namespace, name, false); }
short to bytes|static byte[] (short value) { byte[] bytes = new byte[2]; if (PlatformDependent.BIG_ENDIAN_NATIVE_ORDER) { bytes[1] = (byte) ((value >> 8) & 0xff); bytes[0] = (byte) (value & 0xff); } else { bytes[0] = (byte) ((value >> 8) & 0xff); bytes[1] = (byte) (value & 0xff); } return bytes; }
do restart status|// WebOnly @Restricted(DoNotUse.class) public HttpResponse () throws IOException { JSONObject response = new JSONObject(); Jenkins jenkins = Jenkins.get(); response.put("restartRequired", jenkins.getUpdateCenter().isRestartRequiredForCompletion()); response.put("restartSupported", jenkins.getLifecycle().canRestart()); return HttpResponses.okJSON(response); }
get real client ip addr|public static String (final Map<String, String> httpHeaders, final String remoteAddr) { // If some upstream device added an X-Forwarded-For header // use it for the client ip // This will support scenarios where load balancers or gateways // front the Azkaban web server and a changing Ip address invalidates the session String clientIp = httpHeaders.getOrDefault(X_FORWARDED_FOR_HEADER, null); if (clientIp == null) { clientIp = remoteAddr; } else { // header can contain comma separated list of upstream servers - get the first one final String[] ips = clientIp.split(","); clientIp = ips[0]; } // Strip off port and only get IP address final String[] parts = clientIp.split(":"); clientIp = parts[0]; return clientIp; }
get roc curve|public RocCurve () { if (rocCurve != null) { return rocCurve; } if (isExact) { INDArray pl = getProbAndLabelUsed(); INDArray sorted = Nd4j.sortRows(pl, 0, false); INDArray isPositive = sorted.getColumn(1, true); INDArray isNegative = sorted.getColumn(1, true).rsub(1.0); INDArray cumSumPos = isPositive.cumsum(-1); INDArray cumSumNeg = isNegative.cumsum(-1); val length = sorted.size(0); INDArray t = Nd4j.create(DataType.DOUBLE, length + 2, 1); t.put(new INDArrayIndex[] { interval(1, length + 1), all() }, sorted.getColumn(0, true)); INDArray fpr = Nd4j.create(DataType.DOUBLE, length + 2, 1); fpr.put(new INDArrayIndex[] { interval(1, length + 1), all() }, cumSumNeg.div(countActualNegative)); INDArray tpr = Nd4j.create(DataType.DOUBLE, length + 2, 1); tpr.put(new INDArrayIndex[] { interval(1, length + 1), all() }, cumSumPos.div(countActualPositive)); t.putScalar(0, 0, 1.0); fpr.putScalar(0, 0, 0.0); tpr.putScalar(0, 0, 0.0); fpr.putScalar(length + 1, 0, 1.0); tpr.putScalar(length + 1, 0, 1.0); double[] x_fpr_out = fpr.data().asDouble(); double[] y_tpr_out = tpr.data().asDouble(); double[] tOut = t.data().asDouble(); //These can be omitted, without changing the area (as long as we keep the edge points) if (rocRemoveRedundantPts) { Pair<double[][], int[][]> p = removeRedundant(tOut, x_fpr_out, y_tpr_out, null, null, null); double[][] temp = p.getFirst(); tOut = temp[0]; x_fpr_out = temp[1]; y_tpr_out = temp[2]; } this.rocCurve = new RocCurve(tOut, x_fpr_out, y_tpr_out); return rocCurve; } else { double[][] out = new double[3][thresholdSteps + 1]; int i = 0; for (Map.Entry<Double, CountsForThreshold> entry : counts.entrySet()) { CountsForThreshold c = entry.getValue(); double tpr = c.getCountTruePositive() / ((double) countActualPositive); double fpr = c.getCountFalsePositive() / ((double) countActualNegative); out[0][i] = c.getThreshold(); out[1][i] = fpr; out[2][i] = tpr; i++; } return new RocCurve(out[0], out[1], out[2]); } }
create plan from program|private static Plan (Program program, String[] options) throws ProgramInvocationException { try { return program.getPlan(options); } catch (Throwable t) { throw new ProgramInvocationException("Error while calling the program: " + t.getMessage(), t); } }
copy|public static ByteBuffer (ByteBuffer src, int srcStart, ByteBuffer dest, int destStart, int length) { System.arraycopy(src.array(), srcStart, dest.array(), destStart, length); return dest; }
collect|public void (AWSCloudCollector collector) { log("Starting AWS collection..."); log("Collecting AWS Cloud Data..."); Map<String, List<CloudInstance>> accountToInstanceMap = collectInstances(); Map<String, String> instanceToAccountMap = new HashMap<>(); for (String account : accountToInstanceMap.keySet()) { Collection<CloudInstance> instanceList = accountToInstanceMap.get(account); for (CloudInstance ci : instanceList) { instanceToAccountMap.put(ci.getInstanceId(), account); } } collectVolume(instanceToAccountMap); log("Finished Cloud collection."); }
filter|public boolean (Term term) { if (!stop.isEmpty() && (stop.contains(term.getName()))) { return true; } if (!natureStop.isEmpty() && (natureStop.contains(term.natrue().natureStr))) { return true; } if (!regexList.isEmpty()) { for (Pattern stopwordPattern : regexList) { if (stopwordPattern.matcher(term.getName()).matches()) { return true; } } } return false; }
record identity link created|@Override public void (IdentityLinkEntity identityLink) { // to a process-definition only as this is never kept in history if (isHistoryLevelAtLeast(HistoryLevel.AUDIT) && (identityLink.getProcessInstanceId() != null || identityLink.getTaskId() != null)) { HistoricIdentityLinkEntity historicIdentityLinkEntity = getHistoricIdentityLinkEntityManager().create(); historicIdentityLinkEntity.setId(identityLink.getId()); historicIdentityLinkEntity.setGroupId(identityLink.getGroupId()); historicIdentityLinkEntity.setProcessInstanceId(identityLink.getProcessInstanceId()); historicIdentityLinkEntity.setTaskId(identityLink.getTaskId()); historicIdentityLinkEntity.setType(identityLink.getType()); historicIdentityLinkEntity.setUserId(identityLink.getUserId()); getHistoricIdentityLinkEntityManager().insert(historicIdentityLinkEntity, false); } }
unpin|public VoidAggregation (@NonNull VoidAggregation aggregation) { return unpin(aggregation.getOriginatorId(), aggregation.getTaskId()); }
record counter and timer|private void (MixinMetric mixinMetric, RpcAbstractLookoutModel model) { Counter totalCounter = mixinMetric.counter("total_count"); Timer totalTimer = mixinMetric.timer("total_time"); Long elapsedTime = model.getElapsedTime(); totalCounter.inc(); if (elapsedTime != null) { totalTimer.record(elapsedTime, TimeUnit.MILLISECONDS); } if (!model.getSuccess()) { Counter failCounter = mixinMetric.counter("fail_count"); Timer failTimer = mixinMetric.timer("fail_time"); failCounter.inc(); if (elapsedTime != null) { failTimer.record(elapsedTime, TimeUnit.MILLISECONDS); } } }
sort|public static int (LongArray array, long numRecords, int startByteIndex, int endByteIndex, boolean desc, boolean signed) { assert startByteIndex >= 0 : "startByteIndex (" + startByteIndex + ") should >= 0"; assert endByteIndex <= 7 : "endByteIndex (" + endByteIndex + ") should <= 7"; assert endByteIndex > startByteIndex; assert numRecords * 2 <= array.size(); long inIndex = 0; long outIndex = numRecords; if (numRecords > 0) { long[][] counts = getCounts(array, numRecords, startByteIndex, endByteIndex); for (int i = startByteIndex; i <= endByteIndex; i++) { if (counts[i] != null) { sortAtByte(array, numRecords, counts[i], i, inIndex, outIndex, desc, signed && i == endByteIndex); long tmp = inIndex; inIndex = outIndex; outIndex = tmp; } } } return Ints.checkedCast(inIndex); }
random exponential|public SDVariable (double lambda, SDVariable shape) { return new RandomExponential(sameDiff(), shape, lambda).outputVariable(); }
reload async|public Metric () throws ExecutionException, InterruptedException { // [START reloadAsync] Future<Metric> future = metric.reloadAsync(); // ... Metric latestMetric = future.get(); if (latestMetric == null) { // the metric was not found } // [END reloadAsync] return latestMetric; }
get builds by timestamp|@WithBridgeMethods(List.class) @Deprecated public RunList<RunT> (long start, long end) { return getBuilds().byTimestamp(start, end); }
status from trailers|private Status (Metadata trailers) { Status status = trailers.get(InternalStatus.CODE_KEY); if (status != null) { return status.withDescription(trailers.get(InternalStatus.MESSAGE_KEY)); } // No status; something is broken. Try to provide a resonanable error. if (headersReceived) { return Status.UNKNOWN.withDescription("missing GRPC status in response"); } Integer httpStatus = trailers.get(HTTP2_STATUS); if (httpStatus != null) { status = GrpcUtil.httpStatusToGrpcStatus(httpStatus); } else { status = Status.INTERNAL.withDescription("missing HTTP status code"); } return status.augmentDescription("missing GRPC status, inferred error from HTTP status code"); }
index of|public static <T> int (T[] array, Object value) { if (null != array) { for (int i = 0; i < array.length; i++) { if (ObjectUtil.equal(value, array[i])) { return i; } } } return INDEX_NOT_FOUND; }
min|public static <T> T (Collection<? extends T> coll, Comparator<? super T> comp) { return Collections.min(coll, comp); }
sizeof|public static long (final Object obj) { if (null == obj || isSharedFlyweight(obj)) { return 0; } final IdentityHashMap visited = new IdentityHashMap(80000); try { return computeSizeof(obj, visited, CLASS_METADATA_CACHE); } catch (RuntimeException re) { return -1; } catch (NoClassDefFoundError ncdfe) { return -1; } }
merge paths|private static String (String path1, String path2) { return path1 + (path1.endsWith(Path.SEPARATOR) ? "" : Path.SEPARATOR) + path2; }
register handle|synchronized String (AbstractAppHandle handle) { String secret = createSecret(); secretToPendingApps.put(secret, handle); return secret; }
with compression|@ExperimentalApi("https://github.com/grpc/grpc-java/issues/1704") public final S (String compressorName) { return build(channel, callOptions.withCompression(compressorName)); }
get next stream position|CompletableFuture<Integer> () { return store.createEphemeralSequentialZNode(counterPath).thenApply( counterStr -> Integer.parseInt(counterStr.replace(counterPath, ""))); }
compute auc|private double () { //special case if (_fps[_nBins - 1] == 0) return 1.0; //special case if (_tps[_nBins - 1] == 0) return 0.0; // All math is computed scaled by TP and FP. We'll descale once at the // end. Trapezoids from (tps[i-1],fps[i-1]) to (tps[i],fps[i]) double tp0 = 0, fp0 = 0; double area = 0; for (int i = 0; i < _nBins; i++) { // Trapezoid area += (_fps[i] - fp0) * (_tps[i] + tp0) / 2.0; tp0 = _tps[i]; fp0 = _fps[i]; } // Descale return area / _p / _n; }
as matrix|public INDArray (BufferedImage image, boolean flipChannels) throws IOException { if (converter == null) { converter = new OpenCVFrameConverter.ToMat(); } return asMatrix(converter.convert(converter2.getFrame(image, 1.0, flipChannels))); }
new rule|public static AlternateTypeRule (Type original, Type alternate, int order) { TypeResolver resolver = new TypeResolver(); return new AlternateTypeRule(resolver.resolve(original), resolver.resolve(alternate), order); }
register annotation type|@SuppressWarnings("unchecked") static void (AnnotationClassValue<?> annotationClassValue) { final String name = annotationClassValue.getName(); if (!ANNOTATION_TYPES.containsKey(name)) { annotationClassValue.getType().ifPresent((Consumer<Class<?>>) aClass -> { if (Annotation.class.isAssignableFrom(aClass)) { ANNOTATION_TYPES.put(name, (Class<? extends Annotation>) aClass); } }); } }
fit|public void (@NonNull MultiDataSetIterator iterator) { List<S.Builder> featureNormBuilders = new ArrayList<>(); List<S.Builder> labelNormBuilders = new ArrayList<>(); iterator.reset(); while (iterator.hasNext()) { MultiDataSet next = iterator.next(); fitPartial(next, featureNormBuilders, labelNormBuilders); } featureStats = buildList(featureNormBuilders); if (isFitLabel()) { labelStats = buildList(labelNormBuilders); } }
get field|public static Field (String fieldName, Class<?> clazz) { Field field = null; try { field = clazz.getDeclaredField(fieldName); } catch (SecurityException e) { throw new ActivitiException("not allowed to access field " + field + " on class " + clazz.getCanonicalName()); } catch (NoSuchFieldException e) { Class<?> superClass = clazz.getSuperclass(); if (superClass != null) { return getField(fieldName, superClass); } } return field; }
join|public static String (List<String> list, String delim) { StringBuilder sb = new StringBuilder(); String loopDelim = ""; for (String s : list) { sb.append(loopDelim); sb.append(s); loopDelim = delim; } return sb.toString(); }
generate|public static void (String content, int width, int height, String imageType, OutputStream out) { final BufferedImage image = generate(content, width, height); ImgUtil.write(image, imageType, out); }
table read|@Override public void (WireCommands.TableRead tableRead) { log.debug(requestId, "readTable {} successful.", tableName); List<TableEntry<byte[], byte[]>> tableEntries = tableRead.getEntries().getEntries().stream().map( e -> new TableEntryImpl<>(convertFromWireCommand(e.getKey()), getArray(e.getValue().getData()))).collect(Collectors.toList()); result.complete(tableEntries); }
hash code ascii compute|private static int (CharSequence value, int offset, int hash) { if (BIG_ENDIAN_NATIVE_ORDER) { return hash * HASH_CODE_C1 + // Low order int hashCodeAsciiSanitizeInt(value, offset + 4) * HASH_CODE_C2 + // High order int hashCodeAsciiSanitizeInt(value, offset); } return hash * HASH_CODE_C1 + // Low order int hashCodeAsciiSanitizeInt(value, offset) * HASH_CODE_C2 + // High order int hashCodeAsciiSanitizeInt(value, offset + 4); }
authorize resource action|public static Access (final HttpServletRequest request, final ResourceAction resourceAction, final AuthorizerMapper authorizerMapper) { return authorizeAllResourceActions(request, Collections.singletonList(resourceAction), authorizerMapper); }
get select columns|public static String (Class<?> entityClass) { EntityTable entityTable = getEntityTable(entityClass); if (entityTable.getBaseSelect() != null) { return entityTable.getBaseSelect(); } Set<EntityColumn> columnList = getColumns(entityClass); StringBuilder selectBuilder = new StringBuilder(); boolean skipAlias = Map.class.isAssignableFrom(entityClass); for (EntityColumn entityColumn : columnList) { selectBuilder.append(entityColumn.getColumn()); if (!skipAlias && !entityColumn.getColumn().equalsIgnoreCase(entityColumn.getProperty())) { //不等的时候分几种情况，例如`DESC` if (entityColumn.getColumn().substring(1, entityColumn.getColumn().length() - 1).equalsIgnoreCase(entityColumn.getProperty())) { selectBuilder.append(","); } else { selectBuilder.append(" AS ").append(entityColumn.getProperty()).append(","); } } else { selectBuilder.append(","); } } entityTable.setBaseSelect(selectBuilder.substring(0, selectBuilder.length() - 1)); return entityTable.getBaseSelect(); }
deprecate|public Operation (DeprecationStatus<ImageId> deprecationStatus, OperationOption... options) { return compute.deprecate(getImageId(), deprecationStatus, options); }
get direct byte buffer|public static ByteBuffer (DirectBuffer directBuffer) { return directBuffer.byteBuffer() == null ? ByteBuffer.allocateDirect(directBuffer.capacity()).put(directBuffer.byteArray()) : directBuffer.byteBuffer(); }
escape xml|private static String (String nonEscapedXmlStr) { StringBuilder escapedXML = new StringBuilder(); for (int i = 0; i < nonEscapedXmlStr.length(); i++) { char c = nonEscapedXmlStr.charAt(i); switch(c) { case '<': escapedXML.append("&lt;"); break; case '>': escapedXML.append("&gt;"); break; case '\"': escapedXML.append("&quot;"); break; case '&': escapedXML.append("&amp;"); break; case '\'': escapedXML.append("&apos;"); break; default: if (c > 0x7e) { escapedXML.append("&#" + ((int) c) + ";"); } else { escapedXML.append(c); } } } return escapedXML.toString(); }
get ico moon glyph|public static SVGGlyph (String glyphName) throws Exception { SVGGlyphBuilder builder = glyphsMap.get(glyphName); if (builder == null) throw new Exception("Glyph '" + glyphName + "' not found!"); SVGGlyph glyph = builder.build(); // we need to apply transformation to correct the icon since // its being inverted after importing from icomoon glyph.getTransforms().add(new Scale(1, -1)); Translate height = new Translate(); height.yProperty().bind(Bindings.createDoubleBinding(() -> -glyph.getHeight(), glyph.heightProperty())); glyph.getTransforms().add(height); return glyph; }
logic delete column equals value|public static String (EntityColumn column, boolean isDeleted) { String result = ""; if (column.getEntityField().isAnnotationPresent(LogicDelete.class)) { result = column.getColumn() + " = " + getLogicDeletedValue(column, isDeleted); } return result; }
get extractor rule definition file name|public static String (final String rootDir, final DatabaseType databaseType) { return Joiner.on('/').join(rootDir, databaseType.name().toLowerCase(), EXTRACTOR_RULE_DEFINITION_FILE_NAME); }
key press string|public static void (String str) { ClipboardUtil.setStr(str); // 粘贴 keyPressWithCtrl(KeyEvent.VK_V); delay(); }
set algorithm|public void (final String alg) { if (StringUtils.isNotBlank(alg)) { LOGGER.debug("Configured Jasypt algorithm [{}]", alg); jasyptInstance.setAlgorithm(alg); } }
get one time nonce|public String (String apiUrl) { String nonce = Long.toHexString(random.nextLong()); this.nonces.put(nonce, new Nonce(nonce, apiUrl, true)); return nonce; }
is token ntlm|private static boolean (final byte[] token) { if (token == null || token.length < NTLM_TOKEN_MAX_LENGTH) { return false; } return IntStream.range(0, NTLM_TOKEN_MAX_LENGTH).noneMatch( i -> NTLMSSP_SIGNATURE[i] != token[i]); }
get new ticket id|@Override public String (final String prefix) { val number = this.numericGenerator.getNextNumberAsString(); val ticketBody = this.randomStringGenerator.getNewString().replace('_', '-'); val origSuffix = StringUtils.defaultString(this.suffix); val finalizedSuffix = StringUtils.isEmpty(origSuffix) ? origSuffix : '-' + origSuffix; return prefix + '-' + number + '-' + ticketBody + finalizedSuffix; }
delete recursive|public static void (@Nonnull File dir) throws IOException { deleteRecursive(fileToPath(dir), PathRemover.PathChecker.ALLOW_ALL); }
trigger event|@Override public void () { // delete all outdated events try { lock.writeLock().lock(); long currentTime = System.currentTimeMillis(); if (latestEvent.get() == 0) this.latestEvent.set(currentTime); actualizeCounts(currentTime); int currentBin = (int) TimeUnit.SECONDS.convert(currentTime, TimeUnit.MILLISECONDS) % buckets.length; buckets[currentBin]++; // nullify next bin if (currentBin == buckets.length - 1) buckets[0] = 0; else buckets[currentBin + 1] = 0; // set new time this.latestEvent.set(currentTime); } finally { lock.writeLock().unlock(); } }
create file|public LimitedOutputStream () throws IOException { if (bytesUsed.get() >= maxBytesUsed) { throw new TemporaryStorageFullException(maxBytesUsed); } synchronized (files) { if (closed) { throw new ISE("Closed"); } FileUtils.forceMkdir(storageDirectory); if (!createdStorageDirectory) { createdStorageDirectory = true; } final File theFile = new File(storageDirectory, StringUtils.format("%08d.tmp", files.size())); final EnumSet<StandardOpenOption> openOptions = EnumSet.of(StandardOpenOption.CREATE_NEW, StandardOpenOption.WRITE); final FileChannel channel = FileChannel.open(theFile.toPath(), openOptions); files.add(theFile); return new LimitedOutputStream(theFile, Channels.newOutputStream(channel)); } }
append|int (ByteArraySegment data) { ensureAppendConditions(); int actualLength = Math.min(data.getLength(), getAvailableLength()); if (actualLength > 0) { this.contents.copyFrom(data, writePosition, actualLength); writePosition += actualLength; } return actualLength; }
process|private void (long key) { IntPriorityQueue times = accessTimes.get(key); int lastAccess = times.dequeueInt(); boolean found = data.remove(lastAccess); if (times.isEmpty()) { data.add(infiniteTimestamp--); accessTimes.remove(key); } else { data.add(times.firstInt()); } if (found) { policyStats.recordHit(); } else { policyStats.recordMiss(); if (data.size() > maximumSize) { evict(); } } }
add end slash|protected String (@NonNull String target) { if (!target.endsWith(File.separator)) target = target + File.separator; return target; }
get attribute repository|@JsonIgnore protected static IPersonAttributeDao () { val repositories = ApplicationContextProvider.getAttributeRepository(); return repositories.orElse(null); }
list|static <T> List<T> (T... elements) { return new ArrayList<>(Arrays.asList(elements)); }
set zones|public void (Map<String, List<URL>> zones) { if (zones != null) { this.otherZones = zones.entrySet().stream().flatMap((Function<Map.Entry<String, List<URL>>, Stream<ServiceInstance>>) entry -> entry.getValue().stream().map(uriMapper()).map( uri -> ServiceInstance.builder(getServiceID(), uri).zone(entry.getKey()).build())).collect(Collectors.toList()); } }
parse option|void (String option) { if (option.isEmpty()) { return; } @SuppressWarnings("StringSplitter") String[] keyAndValue = option.split(SPLIT_KEY_VALUE); requireArgument(keyAndValue.length <= 2, "key-value pair %s with more than one equals sign", option); String key = keyAndValue[0].trim(); String value = (keyAndValue.length == 1) ? null : keyAndValue[1].trim(); configure(key, value); }
is birthday|public static boolean (int year, int month, int day) { // 验证年 int thisYear = DateUtil.thisYear(); if (year < 1900 || year > thisYear) { return false; } // 验证月 if (month < 1 || month > 12) { return false; } // 验证日 if (day < 1 || day > 31) { return false; } if ((month == 4 || month == 6 || month == 9 || month == 11) && day == 31) { return false; } if (month == 2) { if (day > 29 || (day == 29 && false == DateUtil.isLeapYear(year))) { return false; } } return true; }
find by principal|@View(name = "by_surrogate", map = "function(doc) { if(doc.surrogate && doc.principal) { emit(doc.principal, doc.surrogate) } }") public List<String> (final String surrogate) { val view = createQuery("by_surrogate").key(surrogate); return db.queryView(view, String.class); }
read values|private void () throws IOException { lastReadInputCheckpoint = input.getCheckpoint(); int control = input.read(); if (control == -1) { throw new OrcCorruptionException(input.getOrcDataSourceId(), "Read past end of RLE integer"); } if (control < 0x80) { numLiterals = control + MIN_REPEAT_SIZE; used = 0; repeat = true; delta = input.read(); if (delta == -1) { throw new OrcCorruptionException(input.getOrcDataSourceId(), "End of stream in RLE Integer"); } // convert from 0 to 255 to -128 to 127 by converting to a signed byte // noinspection SillyAssignment delta = (byte) delta; literals[0] = LongDecode.readVInt(signed, input); } else { numLiterals = 0x100 - control; used = 0; repeat = false; for (int i = 0; i < numLiterals; ++i) { literals[i] = LongDecode.readVInt(signed, input); } } }
obtain matching redirect|private String (Set<String> redirectUris, String requestedRedirect) { Assert.notEmpty(redirectUris, "Redirect URIs cannot be empty"); if (redirectUris.size() == 1 && requestedRedirect == null) { return redirectUris.iterator().next(); } for (String redirectUri : redirectUris) { if (requestedRedirect != null && redirectMatches(requestedRedirect, redirectUri)) { // Initialize with the registered redirect-uri UriComponentsBuilder redirectUriBuilder = UriComponentsBuilder.fromUriString(redirectUri); UriComponents requestedRedirectUri = UriComponentsBuilder.fromUriString(requestedRedirect).build(); if (this.matchSubdomains) { redirectUriBuilder.host(requestedRedirectUri.getHost()); } if (!this.matchPorts) { redirectUriBuilder.port(requestedRedirectUri.getPort()); } // retain additional params (if any) redirectUriBuilder.replaceQuery(requestedRedirectUri.getQuery()); redirectUriBuilder.fragment(null); return redirectUriBuilder.build().toUriString(); } } throw new RedirectMismatchException("Invalid redirect: " + requestedRedirect + " does not match one of the registered values."); }
patch network|@BetaApi public final Operation (ProjectGlobalNetworkName network, Network networkResource, List<String> fieldMask) { PatchNetworkHttpRequest request = PatchNetworkHttpRequest.newBuilder().setNetwork(network == null ? null : network.toString()).setNetworkResource(networkResource).addAllFieldMask(fieldMask).build(); return patchNetwork(request); }
from completable future|public static <T> Publisher<T> (Supplier<CompletableFuture<T>> futureSupplier) { return new CompletableFuturePublisher<>(futureSupplier); }
find key|private int (K key) { int i = 0; for (K otherKey : this.keys) { if (this.comparator.compare(key, otherKey) == 0) { return i; } i++; } return -1; }
is detached plugin|public static boolean (@Nonnull String pluginId) { for (DetachedPlugin detachedPlugin : getDetachedPlugins()) { if (detachedPlugin.getShortName().equals(pluginId)) { return true; } } return false; }
get router status router|@BetaApi public final RouterStatusResponse (ProjectRegionRouterName router) { GetRouterStatusRouterHttpRequest request = GetRouterStatusRouterHttpRequest.newBuilder().setRouter(router == null ? null : router.toString()).build(); return getRouterStatusRouter(request); }
degrade weight|public static boolean (ProviderInfo providerInfo, int weight) { providerInfo.setStatus(ProviderStatus.DEGRADED); providerInfo.setWeight(weight); return true; }
list datasets|public Page<Dataset> () { // [START bigquery_list_datasets] // List datasets in the default project Page<Dataset> datasets = bigquery.listDatasets(DatasetListOption.pageSize(100)); for (Dataset dataset : datasets.iterateAll()) { // do something with the dataset } // [END bigquery_list_datasets] return datasets; }
get j panel|private JPanel () { if (jPanel1 == null) { jPanel1 = new JPanel(); jPanel1.setLayout(new CardLayout()); jPanel1.add(getJScrollPane(), getJScrollPane().getName()); } return jPanel1; }
register custom layer|public static void (String layerName, Class<? extends KerasLayer> configClass) { customLayers.put(layerName, configClass); }
of|public static SnapshotInfo (SnapshotId snapshotId, DiskId source) { return newBuilder(snapshotId, source).build(); }
fail task|private void (final ExecutionAttemptID executionAttemptID, final Throwable cause) { final Task task = taskSlotTable.getTask(executionAttemptID); if (task != null) { try { task.failExternally(cause); } catch (Throwable t) { log.error("Could not fail task {}.", executionAttemptID, t); } } else { log.debug("Cannot find task to fail for execution {}.", executionAttemptID); } }
cli disconnect|@Deprecated public void (String cause) throws ExecutionException, InterruptedException { checkPermission(DISCONNECT); disconnect(new ByCLI(cause)).get(); }
keys for bucket|public List<String> (String bucket) { AmazonS3 s3 = getClient(); List<String> ret = new ArrayList<>(); ListObjectsRequest listObjectsRequest = new ListObjectsRequest().withBucketName(bucket); ObjectListing objectListing; do { objectListing = s3.listObjects(listObjectsRequest); for (S3ObjectSummary objectSummary : objectListing.getObjectSummaries()) { ret.add(objectSummary.getKey()); } listObjectsRequest.setMarker(objectListing.getNextMarker()); } while (objectListing.isTruncated()); return ret; }
new convertible values|private ConvertibleValues<Object> (Map<CharSequence, Object> values) { if (CollectionUtils.isEmpty(values)) { return ConvertibleValues.EMPTY; } else { return ConvertibleValues.of(values); } }
scan file|private void (File file, String rootDir) { if (file.isFile()) { final String fileName = file.getAbsolutePath(); if (fileName.endsWith(FileUtil.CLASS_EXT)) { final String className = // fileName.substring(rootDir.length(), // fileName.length() - 6).replace(File.separatorChar, // CharUtil.DOT); //加入满足条件的类 addIfAccept(className); } else if (fileName.endsWith(FileUtil.JAR_FILE_EXT)) { try { scanJar(new JarFile(file)); } catch (IOException e) { throw new IORuntimeException(e); } } } else if (file.isDirectory()) { for (File subFile : file.listFiles()) { scanFile(subFile, (null == rootDir) ? subPathBeforePackage(file) : rootDir); } } }
init keys|@SuppressWarnings("unchecked") public T () { KeyPair keyPair = SecureUtil.generateKeyPair(this.algorithm); this.publicKey = keyPair.getPublic(); this.privateKey = keyPair.getPrivate(); return (T) this; }
unchecked|public static RuntimeException (@Nullable Throwable t) { if (t instanceof RuntimeException) { throw (RuntimeException) t; } if (t instanceof Error) { throw (Error) t; } throw new UncheckedException(t); }
add urls|public void (Collection<URL> urls) { Assert.notNull(urls, "Urls must not be null"); this.urls.addAll(urls); }
wrapped buffer|public static ByteBuf (ByteBuf buffer) { if (buffer.isReadable()) { return buffer.slice(); } else { buffer.release(); return EMPTY_BUFFER; } }
utf encode|public static byte[] (CharSequence string) { try { ByteBuffer bytes = UTF8.newEncoder().encode(CharBuffer.wrap(string)); byte[] bytesCopy = new byte[bytes.limit()]; System.arraycopy(bytes.array(), 0, bytesCopy, 0, bytes.limit()); return bytesCopy; } catch (CharacterCodingException e) { throw new RuntimeException(e); } }
parse groovy script|public static GroovyObject (final Resource groovyScript, final boolean failOnError) { return AccessController.doPrivileged((PrivilegedAction<GroovyObject>) () -> { val parent = ScriptingUtils.class.getClassLoader(); try (val loader = new GroovyClassLoader(parent)) { val groovyFile = groovyScript.getFile(); if (groovyFile.exists()) { val groovyClass = loader.parseClass(groovyFile); LOGGER.trace("Creating groovy object instance from class [{}]", groovyFile.getCanonicalPath()); return (GroovyObject) groovyClass.getDeclaredConstructor().newInstance(); } LOGGER.trace("Groovy script at [{}] does not exist", groovyScript); } catch (final Exception e) { if (failOnError) { throw new RuntimeException(e); } LOGGER.error(e.getMessage(), e); } return null; }); }
build table attribute values map from ticket|public Map<String, AttributeValue> (final Ticket ticket, final Ticket encTicket) { val values = new HashMap<String, AttributeValue>(); values.put(ColumnNames.ID.getColumnName(), new AttributeValue(encTicket.getId())); values.put(ColumnNames.PREFIX.getColumnName(), new AttributeValue(ticket.getPrefix())); values.put(ColumnNames.CREATION_TIME.getColumnName(), new AttributeValue(ticket.getCreationTime().toString())); values.put(ColumnNames.COUNT_OF_USES.getColumnName(), new AttributeValue().withN(Integer.toString(ticket.getCountOfUses()))); values.put(ColumnNames.TIME_TO_LIVE.getColumnName(), new AttributeValue().withN(Long.toString(ticket.getExpirationPolicy().getTimeToLive()))); values.put(ColumnNames.TIME_TO_IDLE.getColumnName(), new AttributeValue().withN(Long.toString(ticket.getExpirationPolicy().getTimeToIdle()))); values.put(ColumnNames.ENCODED.getColumnName(), new AttributeValue().withB(ByteBuffer.wrap(SerializationUtils.serialize(encTicket)))); LOGGER.debug("Created attribute values [{}] based on provided ticket [{}]", values, encTicket.getId()); return values; }
get session management methods combo box|protected JComboBox<SessionManagementMethodType> () { if (sessionManagementMethodsComboBox == null) { Vector<SessionManagementMethodType> methods = new Vector<>(extension.getSessionManagementMethodTypes()); sessionManagementMethodsComboBox = new JComboBox<>(methods); sessionManagementMethodsComboBox.setSelectedItem(null); // Prepare the listener for the change of selection sessionManagementMethodsComboBox.addItemListener(new ItemListener() { @Override public void itemStateChanged(ItemEvent e) { if (e.getStateChange() == ItemEvent.SELECTED) { // Prepare the new session management method log.debug("Selected new Session Management type: " + e.getItem()); SessionManagementMethodType type = ((SessionManagementMethodType) e.getItem()); // different class, create it now if (selectedMethod == null || !type.isTypeForMethod(selectedMethod)) { // Create the new session management method selectedMethod = type.createSessionManagementMethod(getContextIndex()); } // Show the status panel and configuration button, if needed changeMethodConfigPanel(type); if (type.hasOptionsPanel()) shownConfigPanel.bindMethod(selectedMethod); } } }); } return sessionManagementMethodsComboBox; }
severity for|private static Severity (Level level) { switch(level.toInt()) { // TRACE case 5000: return Severity.DEBUG; // DEBUG case 10000: return Severity.DEBUG; // INFO case 20000: return Severity.INFO; // WARNING case 30000: return Severity.WARNING; // ERROR case 40000: return Severity.ERROR; default: return Severity.DEFAULT; } }
execute|public int (final String user, final List<String> command) throws IOException { log.info("Command: " + command); final Process process = new ProcessBuilder().command(constructExecuteAsCommand(user, command)).inheritIO().start(); int exitCode; try { exitCode = process.waitFor(); } catch (final InterruptedException e) { log.error(e.getMessage(), e); exitCode = 1; } return exitCode; }
next|public ByteBuffer (ByteBuffer inBytes) throws GeneralSecurityException { Preconditions.checkState(!isFinished(), "Handshake has already finished."); HandshakerReq.Builder req = HandshakerReq.newBuilder().setNext(NextHandshakeMessageReq.newBuilder().setInBytes(ByteString.copyFrom(inBytes.duplicate())).build()); HandshakerResp resp; try { resp = handshakerStub.send(req.build()); } catch (IOException | InterruptedException e) { throw new GeneralSecurityException(e); } handleResponse(resp); inBytes.position(inBytes.position() + resp.getBytesConsumed()); return resp.getOutFrames().asReadOnlyByteBuffer(); }
calculate sole transition path length|private int (String str) { Stack<MDAGNode> transitionPathNodeStack = sourceNode.getTransitionPathNodes(str); //The MDAGNode at the top of the stack is not needed transitionPathNodeStack.pop(); //(we are processing the outgoing transitions of nodes inside str's _transition path, //the outgoing transitions of the MDAGNode at the top of the stack are outside this path) transitionPathNodeStack.trimToSize(); //each node in the _transition path has a single outgoing _transition and is not an accept state. while (!transitionPathNodeStack.isEmpty()) { MDAGNode currentNode = transitionPathNodeStack.peek(); if (currentNode.getOutgoingTransitions().size() <= 1 && !currentNode.isAcceptNode()) transitionPathNodeStack.pop(); else break; } return (transitionPathNodeStack.capacity() - transitionPathNodeStack.size()); }
is connection close|public boolean () { boolean result = true; if (mMalformedHeader) { return true; } if (isHttp10()) { // HTTP 1.0 default to close unless keep alive. result = true; try { if (getHeader(CONNECTION).equalsIgnoreCase(_KEEP_ALIVE) || getHeader(PROXY_CONNECTION).equalsIgnoreCase(_KEEP_ALIVE)) { return false; } } catch (NullPointerException e) { } } else if (isHttp11()) { // HTTP 1.1 default to keep alive unless close. result = false; try { if (getHeader(CONNECTION).equalsIgnoreCase(_CLOSE)) { return true; } else if (getHeader(PROXY_CONNECTION).equalsIgnoreCase(_CLOSE)) { return true; } } catch (NullPointerException e) { } } return result; }
get user|@GetMapping("/user") public ResponseEntity<User> (//<1> @VersionApi int version, @RequestParam("id") String id) { throw new UnsupportedOperationException(); }
processing failure|@Override public void (Exception error) { log.error(requestId, "readTable {} failed", tableName, error); handleError(error, result, type); }
list|@SafeVarargs public static <T> List<T> (boolean isLinked, T... values) { if (ArrayUtil.isEmpty(values)) { return list(isLinked); } List<T> arrayList = isLinked ? new LinkedList<T>() : new ArrayList<T>(values.length); for (T t : values) { arrayList.add(t); } return arrayList; }
extract object|@SuppressWarnings("UnusedParameters") protected ByteBuf (ChannelHandlerContext ctx, ByteBuf buffer, int index, int length) { return buffer.retainedSlice(index, length); }
set session id context|public boolean (byte[] sidCtx) { Lock writerLock = context.ctxLock.writeLock(); writerLock.lock(); try { return SSLContext.setSessionIdContext(context.ctx, sidCtx); } finally { writerLock.unlock(); } }
set minimum value|public DoubleParameter (double minimumValue, boolean inclusive) { if (hasDefaultValue) { if (inclusive) { Util.checkParameter(minimumValue <= defaultValue, "Minimum value (" + minimumValue + ") must be less than or equal to default (" + defaultValue + ")"); } else { Util.checkParameter(minimumValue < defaultValue, "Minimum value (" + minimumValue + ") must be less than default (" + defaultValue + ")"); } } else if (hasMaximumValue) { if (inclusive && maximumValueInclusive) { Util.checkParameter(minimumValue <= maximumValue, "Minimum value (" + minimumValue + ") must be less than or equal to maximum (" + maximumValue + ")"); } else { Util.checkParameter(minimumValue < maximumValue, "Minimum value (" + minimumValue + ") must be less than maximum (" + maximumValue + ")"); } } this.hasMinimumValue = true; this.minimumValue = minimumValue; this.minimumValueInclusive = inclusive; return this; }
validate completable return type|private void (Method commandMethod, Class<?> callbackReturnType) { if (Void.TYPE == callbackReturnType) { throw new FallbackDefinitionException(createErrorMsg(commandMethod, method, "fallback cannot return 'void' if command return type is " + Completable.class.getSimpleName())); } }
add|void (long value) { if (value >= 0) { if (count > 0) { min = Math.min(min, value); max = Math.max(max, value); } else { min = value; max = value; } count++; sum += value; } }
add or update|public int (Entity entity) throws SQLException { return null == entity.get(primaryKeyField) ? add(entity) : update(entity); }
is valid authority|protected boolean (String authority) { if (authority == null) { return false; } // check manual authority validation if specified if (authorityValidator != null && authorityValidator.isValid(authority)) { return true; } // convert to ASCII if possible final String authorityASCII = DomainValidator.unicodeToASCII(authority); Matcher authorityMatcher = AUTHORITY_PATTERN.matcher(authorityASCII); if (!authorityMatcher.matches()) { return false; } // We have to process IPV6 separately because that is parsed in a different group String ipv6 = authorityMatcher.group(PARSE_AUTHORITY_IPV6); if (ipv6 != null) { InetAddressValidator inetAddressValidator = InetAddressValidator.getInstance(); if (!inetAddressValidator.isValidInet6Address(ipv6)) { return false; } } else { String hostLocation = authorityMatcher.group(PARSE_AUTHORITY_HOST_IP); // check if authority is hostname or IP address: // try a hostname first since that's much more likely DomainValidator domainValidator = DomainValidator.getInstance(isOn(ALLOW_LOCAL_URLS)); if (!domainValidator.isValid(hostLocation)) { // try an IPv4 address InetAddressValidator inetAddressValidator = InetAddressValidator.getInstance(); if (!inetAddressValidator.isValidInet4Address(hostLocation)) { // isn't IPv4, so the URL is invalid return false; } } String port = authorityMatcher.group(PARSE_AUTHORITY_PORT); if (port != null && port.length() > 0) { try { int iPort = Integer.parseInt(port); if (iPort < 0 || iPort > MAX_UNSIGNED_16_BIT_INT) { return false; } } catch (NumberFormatException nfe) { return false; } } } String extra = authorityMatcher.group(PARSE_AUTHORITY_EXTRA); if (extra != null && extra.trim().length() > 0) { return false; } return true; }
initialize|private void (ImageIcon icon) { this.setLayout(new CardLayout()); if (Model.getSingleton().getOptionsParam().getViewParam().getWmUiHandlingOption() == 0) { this.setSize(474, 251); } this.setName(Constant.messages.getString(prefix + ".panel.title")); this.setIcon(icon); this.add(getPanelCommand(), prefix + ".panel"); scanStatus = new ScanStatus(icon, Constant.messages.getString(prefix + ".panel.title")); if (View.isInitialised()) { View.getSingleton().getMainFrame().getMainFooterPanel().addFooterToolbarRightLabel(scanStatus.getCountLabel()); } }
get handle o data specific parameters|private JCheckBox () { if (handleODataSpecificParameters == null) { handleODataSpecificParameters = new JCheckBox(); handleODataSpecificParameters.setText(Constant.messages.getString("spider.options.label.handlehodataparameters")); } return handleODataSpecificParameters; }
copy with flow control|public static <V> void (final Iterable<V> source, CallStreamObserver<V> target) { Preconditions.checkNotNull(source, "source"); copyWithFlowControl(source.iterator(), target); }
get be ulong|public final long (final int pos) { final int position = origin + pos; if (pos + 4 >= limit || pos < 0) throw new IllegalArgumentException("limit excceed: " + (pos < 0 ? pos : (pos + 4))); byte[] buf = buffer; return ((long) (0xff & buf[position + 4])) | ((long) (0xff & buf[position + 3]) << 8) | ((long) (0xff & buf[position + 2]) << 16) | ((long) (0xff & buf[position + 1]) << 24) | ((long) (0xff & buf[position]) << 32); }
build history|CharSequence () { StringBuilder historyText = new StringBuilder(1000); SQLiteOpenHelper helper = new DBHelper(activity); try (SQLiteDatabase db = helper.getReadableDatabase(); Cursor cursor = db.query(DBHelper.TABLE_NAME, COLUMNS, null, null, null, null, DBHelper.TIMESTAMP_COL + " DESC")) { DateFormat format = DateFormat.getDateTimeInstance(DateFormat.MEDIUM, DateFormat.MEDIUM); while (cursor.moveToNext()) { historyText.append('"').append(massageHistoryField(cursor.getString(0))).append("\","); historyText.append('"').append(massageHistoryField(cursor.getString(1))).append("\","); historyText.append('"').append(massageHistoryField(cursor.getString(2))).append("\","); historyText.append('"').append(massageHistoryField(cursor.getString(3))).append("\","); // Add timestamp again, formatted long timestamp = cursor.getLong(3); historyText.append('"').append(massageHistoryField(format.format(timestamp))).append("\","); // Above we're preserving the old ordering of columns which had formatted data in position 5 historyText.append('"').append(massageHistoryField(cursor.getString(4))).append("\"\r\n"); } } catch (SQLException sqle) { Log.w(TAG, sqle); } return historyText; }
sort candidates|public static List<Pair<Double, Integer>> (INDArray x, INDArray X, List<Integer> candidates, String similarityFunction) { int prevIdx = -1; List<Pair<Double, Integer>> ret = new ArrayList<>(); for (int i = 0; i < candidates.size(); i++) { if (candidates.get(i) != prevIdx) { ret.add(Pair.of(computeDistance(similarityFunction, X.slice(candidates.get(i)), x), candidates.get(i))); } prevIdx = i; } Collections.sort(ret, new Comparator<Pair<Double, Integer>>() { @Override public int compare(Pair<Double, Integer> doubleIntegerPair, Pair<Double, Integer> t1) { return Doubles.compare(doubleIntegerPair.getFirst(), t1.getFirst()); } }); return ret; }
decode timestamp|public static long (long seconds, long serializedNanos, long baseTimestampInSeconds) { long millis = (seconds + baseTimestampInSeconds) * MILLIS_PER_SECOND; long nanos = parseNanos(serializedNanos); if (nanos > 999999999 || nanos < 0) { throw new IllegalArgumentException("nanos field of an encoded timestamp in ORC must be between 0 and 999999999 inclusive, got " + nanos); } // (42)*1000 + 1 = 42001 if (millis < 0 && nanos != 0) { millis -= 1000; } // Truncate nanos to millis and add to mills return millis + (nanos / 1_000_000); }
setup local|@Override public void () { int idx = H2O.SELF.index(); _result = new NodeProfile[H2O.CLOUD.size()]; Map<String, Integer> countedStackTraces = new HashMap<>(); final int repeats = 100; for (int i = 0; i < repeats; ++i) { Map<Thread, StackTraceElement[]> allStackTraces = Thread.getAllStackTraces(); for (Map.Entry<Thread, StackTraceElement[]> el : allStackTraces.entrySet()) { StringBuilder sb = new StringBuilder(); int j = 0; for (StackTraceElement ste : el.getValue()) { String val = ste.toString(); if (j == 0 && (val.equals("sun.misc.Unsafe.park(Native Method)") || val.equals("java.lang.Object.wait(Native Method)") || val.equals("java.lang.Thread.sleep(Native Method)") || val.equals("java.lang.Thread.yield(Native Method)") || val.equals("java.net.PlainSocketImpl.socketAccept(Native Method)") || val.equals("sun.nio.ch.ServerSocketChannelImpl.accept0(Native Method)") || val.equals("sun.nio.ch.DatagramChannelImpl.receive0(Native Method)") || val.equals("java.lang.Thread.dumpThreads(Native Method)"))) { break; } sb.append(ste.toString()); sb.append("\n"); j++; if (j == _stack_depth) break; } String st = sb.toString(); boolean found = false; for (Map.Entry<String, Integer> entry : countedStackTraces.entrySet()) { if (entry.getKey().equals(st)) { entry.setValue(entry.getValue() + 1); found = true; break; } } if (!found) countedStackTraces.put(st, 1); } try { Thread.sleep(1); } catch (InterruptedException e) { e.printStackTrace(); } } int i = 0; _result[idx] = new NodeProfile(countedStackTraces.size()); _result[idx].node_name = H2O.getIpPortString(); _result[idx].timestamp = System.currentTimeMillis(); for (Map.Entry<String, Integer> entry : countedStackTraces.entrySet()) { _result[idx].stacktraces[i] = entry.getKey(); _result[idx].counts[i] = entry.getValue(); i++; } Map<Integer, String> sorted = new TreeMap<>(Collections.reverseOrder()); for (int j = 0; j < _result[idx].counts.length; ++j) { if (_result[idx].stacktraces[j] != null && _result[idx].stacktraces[j].length() > 0) sorted.put(_result[idx].counts[j], _result[idx].stacktraces[j]); } // overwrite results String[] sorted_stacktraces = new String[sorted.entrySet().size()]; int[] sorted_counts = new int[sorted.entrySet().size()]; i = 0; for (Map.Entry<Integer, String> e : sorted.entrySet()) { sorted_stacktraces[i] = e.getValue(); sorted_counts[i] = e.getKey(); i++; } _result[idx].stacktraces = sorted_stacktraces; _result[idx].counts = sorted_counts; }
iterator|public Iterator<T> () { final Iterator<T> itr = base.iterator(); return new Iterator<T>() { public boolean hasNext() { return itr.hasNext(); } public T next() { return itr.next(); } public void remove() { itr.remove(); } }; }
kill all spawned hadoop jobs|public static Set<String> (String logFilePath, Logger log) { Set<String> allSpawnedJobs = findApplicationIdFromLog(logFilePath, log); log.info("applicationIds to kill: " + allSpawnedJobs); for (String appId : allSpawnedJobs) { try { killJobOnCluster(appId, log); } catch (Throwable t) { log.warn("something happened while trying to kill this job: " + appId, t); } } return allSpawnedJobs; }
upload|@Override public void (String path, InputStream payload, long payloadSize) { try { ObjectMetadata objectMetadata = new ObjectMetadata(); objectMetadata.setContentType(CONTENT_TYPE); objectMetadata.setContentLength(payloadSize); PutObjectRequest request = new PutObjectRequest(bucketName, path, payload, objectMetadata); s3Client.putObject(request); } catch (SdkClientException e) { String msg = "Error communicating with S3"; logger.error(msg, e); throw new ApplicationException(ApplicationException.Code.BACKEND_ERROR, msg, e); } }
hash|public MurmurHash (int input) { count++; input *= 0xcc9e2d51; input = Integer.rotateLeft(input, 15); input *= 0x1b873593; hash ^= input; hash = Integer.rotateLeft(hash, 13); hash = hash * 5 + 0xe6546b64; return this; }
run fetch loop|@Override public void () throws Exception { // the map from broker to the thread that is connected to that broker final Map<Node, SimpleConsumerThread<T>> brokerToThread = new HashMap<>(); // this holds possible the exceptions from the concurrent broker connection threads final ExceptionProxy errorHandler = new ExceptionProxy(Thread.currentThread()); // the offset handler handles the communication with ZooKeeper, to commit externally visible offsets final ZookeeperOffsetHandler zookeeperOffsetHandler = new ZookeeperOffsetHandler(kafkaConfig); this.zookeeperOffsetHandler = zookeeperOffsetHandler; PeriodicOffsetCommitter periodicCommitter = null; try { // values yet; replace those with actual offsets, according to what the sentinel value represent. for (KafkaTopicPartitionState<TopicAndPartition> partition : subscribedPartitionStates()) { if (partition.getOffset() == KafkaTopicPartitionStateSentinel.EARLIEST_OFFSET) { // this will be replaced by an actual offset in SimpleConsumerThread partition.setOffset(OffsetRequest.EarliestTime()); } else if (partition.getOffset() == KafkaTopicPartitionStateSentinel.LATEST_OFFSET) { // this will be replaced by an actual offset in SimpleConsumerThread partition.setOffset(OffsetRequest.LatestTime()); } else if (partition.getOffset() == KafkaTopicPartitionStateSentinel.GROUP_OFFSET) { Long committedOffset = zookeeperOffsetHandler.getCommittedOffset(partition.getKafkaTopicPartition()); if (committedOffset != null) { // the committed offset in ZK represents the next record to process, // so we subtract it by 1 to correctly represent internal state partition.setOffset(committedOffset - 1); } else { // if we can't find an offset for a partition in ZK when using GROUP_OFFSETS, // we default to "auto.offset.reset" like the Kafka high-level consumer LOG.warn("No group offset can be found for partition {} in Zookeeper;" + " resetting starting offset to 'auto.offset.reset'", partition); partition.setOffset(invalidOffsetBehavior); } } else { // the partition already has a specific start offset and is ready to be consumed } } // start the periodic offset committer thread, if necessary if (autoCommitInterval > 0) { LOG.info("Starting periodic offset committer, with commit interval of {}ms", autoCommitInterval); periodicCommitter = new PeriodicOffsetCommitter(zookeeperOffsetHandler, subscribedPartitionStates(), errorHandler, autoCommitInterval); periodicCommitter.setName("Periodic Kafka partition offset committer"); periodicCommitter.setDaemon(true); periodicCommitter.start(); } // Main loop polling elements from the unassignedPartitions queue to the threads while (running) { // re-throw any exception from the concurrent fetcher threads errorHandler.checkAndThrowException(); // wait for max 5 seconds trying to get partitions to assign // if threads shut down, this poll returns earlier, because the threads inject the // special marker into the queue List<KafkaTopicPartitionState<TopicAndPartition>> partitionsToAssign = unassignedPartitionsQueue.getBatchBlocking(5000); // note: if there are more markers, remove them all partitionsToAssign.removeIf(MARKER::equals); if (!partitionsToAssign.isEmpty()) { LOG.info("Assigning {} partitions to broker threads", partitionsToAssign.size()); Map<Node, List<KafkaTopicPartitionState<TopicAndPartition>>> partitionsWithLeaders = findLeaderForPartitions(partitionsToAssign, kafkaConfig); // assign the partitions to the leaders (maybe start the threads) for (Map.Entry<Node, List<KafkaTopicPartitionState<TopicAndPartition>>> partitionsWithLeader : partitionsWithLeaders.entrySet()) { final Node leader = partitionsWithLeader.getKey(); final List<KafkaTopicPartitionState<TopicAndPartition>> partitions = partitionsWithLeader.getValue(); SimpleConsumerThread<T> brokerThread = brokerToThread.get(leader); if (!running) { break; } if (brokerThread == null || !brokerThread.getNewPartitionsQueue().isOpen()) { // start new thread brokerThread = createAndStartSimpleConsumerThread(partitions, leader, errorHandler); brokerToThread.put(leader, brokerThread); } else { // put elements into queue of thread ClosableBlockingQueue<KafkaTopicPartitionState<TopicAndPartition>> newPartitionsQueue = brokerThread.getNewPartitionsQueue(); for (KafkaTopicPartitionState<TopicAndPartition> fp : partitions) { if (!newPartitionsQueue.addIfOpen(fp)) { // we were unable to add the partition to the broker's queue // the broker has closed in the meantime (the thread will shut down) // create a new thread for connecting to this broker List<KafkaTopicPartitionState<TopicAndPartition>> seedPartitions = new ArrayList<>(); seedPartitions.add(fp); brokerThread = createAndStartSimpleConsumerThread(seedPartitions, leader, errorHandler); brokerToThread.put(leader, brokerThread); // update queue for the subsequent partitions newPartitionsQueue = brokerThread.getNewPartitionsQueue(); } } } } } else { // there were no partitions to assign. Check if any broker threads shut down. // we get into this section of the code, if either the poll timed out, or the // blocking poll was woken up by the marker element Iterator<SimpleConsumerThread<T>> bttIterator = brokerToThread.values().iterator(); while (bttIterator.hasNext()) { SimpleConsumerThread<T> thread = bttIterator.next(); if (!thread.getNewPartitionsQueue().isOpen()) { LOG.info("Removing stopped consumer thread {}", thread.getName()); bttIterator.remove(); } } } if (brokerToThread.size() == 0 && unassignedPartitionsQueue.isEmpty()) { if (unassignedPartitionsQueue.close()) { LOG.info("All consumer threads are finished, there are no more unassigned partitions. Stopping fetcher"); break; } // we end up here if somebody added something to the queue in the meantime --> continue to poll queue again } } } catch (InterruptedException e) { errorHandler.checkAndThrowException(); throw e; } finally { this.running = false; this.zookeeperOffsetHandler = null; // if we run a periodic committer thread, shut that down if (periodicCommitter != null) { periodicCommitter.shutdown(); } // clear the interruption flag // this allows the joining on consumer threads (on best effort) to happen in // case the initial interrupt already Thread.interrupted(); // make sure that in any case (completion, abort, error), all spawned threads are stopped try { int runningThreads; do { // check whether threads are alive and cancel them runningThreads = 0; Iterator<SimpleConsumerThread<T>> threads = brokerToThread.values().iterator(); while (threads.hasNext()) { SimpleConsumerThread<?> t = threads.next(); if (t.isAlive()) { t.cancel(); runningThreads++; } else { threads.remove(); } } // wait for the threads to finish, before issuing a cancel call again if (runningThreads > 0) { for (SimpleConsumerThread<?> t : brokerToThread.values()) { t.join(500 / runningThreads + 1); } } } while (runningThreads > 0); } catch (InterruptedException ignored) { Thread.currentThread().interrupt(); } catch (Throwable t) { LOG.error("Exception while shutting down consumer threads", t); } try { zookeeperOffsetHandler.close(); } catch (Throwable t) { LOG.error("Exception while shutting down ZookeeperOffsetHandler", t); } } }
compare|@Override public int (Pair<Double, Integer> doubleIntegerPair, Pair<Double, Integer> t1) { return Doubles.compare(doubleIntegerPair.getFirst(), t1.getFirst()); }
send|@SuppressWarnings("resource") void (final OtpErlangPid from, final String dest, final OtpErlangObject msg) throws IOException { // encode and send the message sendBuf(from, dest, new OtpOutputStream(msg)); }
get help|public String () { StringBuilder sb = new StringBuilder(data.size() * HELP_TEXT_LENGTH_PER_PARAM); sb.append("Required Parameters:"); sb.append(HELP_TEXT_LINE_DELIMITER); for (Option o : data.values()) { sb.append(this.helpText(o)); } sb.append(HELP_TEXT_LINE_DELIMITER); return sb.toString(); }
open|@Override public void (InputSplit ignored) throws IOException { this.session = cluster.connect(); this.resultSet = session.execute(query); }
download and extract|public static void (String name, URL url, File f, File extractToDir, String targetMD5, int maxTries) throws IOException { downloadAndExtract(0, maxTries, name, url, f, extractToDir, targetMD5); }
convert to type info|@SuppressWarnings("unchecked") public static <T> TypeInformation<T> (String avroSchemaString) { Preconditions.checkNotNull(avroSchemaString, "Avro schema must not be null."); final Schema schema; try { schema = new Schema.Parser().parse(avroSchemaString); } catch (SchemaParseException e) { throw new IllegalArgumentException("Could not parse Avro schema string.", e); } return (TypeInformation<T>) convertToTypeInfo(schema); }
format duration|public static String (final long startTime, final long endTime) { if (startTime == -1) { return "-"; } final long durationMS; if (endTime == -1) { durationMS = System.currentTimeMillis() - startTime; } else { durationMS = endTime - startTime; } long seconds = durationMS / 1000; if (seconds < 60) { return seconds + " sec"; } long minutes = seconds / 60; seconds %= 60; if (minutes < 60) { return minutes + "m " + seconds + "s"; } long hours = minutes / 60; minutes %= 60; if (hours < 24) { return hours + "h " + minutes + "m " + seconds + "s"; } final long days = hours / 24; hours %= 24; return days + "d " + hours + "h " + minutes + "m"; }
add|public boolean (Task task) { // Check that this slot has been assigned to the job sending this task Preconditions.checkArgument(task.getJobID().equals(jobId), "The task's job id does not match the " + "job id for which the slot has been allocated."); Preconditions.checkArgument(task.getAllocationId().equals(allocationId), "The task's allocation " + "id does not match the allocation id for which the slot has been allocated."); Preconditions.checkState(TaskSlotState.ACTIVE == state, "The task slot is not in state active."); Task oldTask = tasks.put(task.getExecutionId(), task); if (oldTask != null) { tasks.put(task.getExecutionId(), oldTask); return false; } else { return true; } }
add globals|ValFrame (Frame fr) { _ses.addGlobals(fr); return new ValFrame(new Frame(fr._names.clone(), fr.vecs().clone())); }
variance|public SDVariable (String name, @NonNull SDVariable x, boolean biasCorrected, int... dimensions) { return variance(name, x, biasCorrected, false, dimensions); }
versioned|public static TableEntry (@NonNull ArrayView key, @NonNull ArrayView value, long version) { return new TableEntry(TableKey.versioned(key, version), value); }
start threads|public void () { if (this.sortThread != null) { this.sortThread.start(); } if (this.spillThread != null) { this.spillThread.start(); } if (this.mergeThread != null) { this.mergeThread.start(); } }
obtain loopback ip address|public String () { final NetworkInterface networkInterface = getLoopBackAndIp4Only(); if (networkInterface != null) { return networkInterface.getIp4LoopbackOnly().getHostName(); } final String ipOfIp4LoopBack = getIpOfLoopBackIp4(); if (ipOfIp4LoopBack != null) { return ipOfIp4LoopBack; } if (Platform.getCurrent().is(Platform.UNIX)) { NetworkInterface linuxLoopback = networkInterfaceProvider.getLoInterface(); if (linuxLoopback != null) { final InetAddress netAddress = linuxLoopback.getIp4LoopbackOnly(); if (netAddress != null) { return netAddress.getHostAddress(); } } } throw new WebDriverException("Unable to resolve local loopback address, please file an issue with the full message of this error:\n" + getNetWorkDiags() + "\n==== End of error message"); }
size|@View(name = "", map = "function(doc) { if (doc.service) { emit(doc, doc._id) }}", reduce = "_count") public int size() { val r = db.queryView(createQuery("size")); LOGGER.trace("r.isEmpty [{}]", r.isEmpty()); LOGGER.trace("r.getRows [{}]", r.getRows()); if (r.isEmpty()) { return 0; } return r.getRows().get(0).getValueAsInt(); }
page|public PageResult<Entity> (Entity where, Page page) throws SQLException { return this.page(where.getFieldNames(), where, page); }
do config executors submit|@RequirePOST public synchronized void (StaplerRequest req, StaplerResponse rsp) throws IOException, ServletException, FormException { checkPermission(ADMINISTER); BulkChange bc = new BulkChange(this); try { JSONObject json = req.getSubmittedForm(); ExtensionList.lookupSingleton(MasterBuildConfiguration.class).configure(req, json); getNodeProperties().rebuild(req, json.optJSONObject("nodeProperties"), NodeProperty.all()); } finally { bc.commit(); } updateComputerList(); // back to the computer page rsp.sendRedirect(req.getContextPath() + '/' + toComputer().getUrl()); }
total page|public static int (int totalCount, int pageSize) { if (pageSize == 0) { return 0; } return totalCount % pageSize == 0 ? (totalCount / pageSize) : (totalCount / pageSize + 1); }
get new add ons|public List<AddOn> (AddOnCollection aoc) { List<AddOn> newAddOns = new ArrayList<>(); for (AddOn ao : aoc.getAddOns()) { boolean isNew = true; for (AddOn addOn : addOns) { try { if (ao.isSameAddOn(addOn)) { isNew = false; break; } } catch (Exception e) { logger.error(e.getMessage(), e); } } if (isNew) { newAddOns.add(ao); } } return newAddOns; }
start application|@Override public SparkAppHandle (SparkAppHandle.Listener... listeners) throws IOException { LauncherServer server = LauncherServer.getOrCreateServer(); ChildProcAppHandle handle = new ChildProcAppHandle(server); for (SparkAppHandle.Listener l : listeners) { handle.addListener(l); } String secret = server.registerHandle(handle); String loggerName = getLoggerName(); ProcessBuilder pb = createBuilder(); boolean outputToLog = outputStream == null; boolean errorToLog = !redirectErrorStream && errorStream == null; // redirection. if (loggerName == null && (outputToLog || errorToLog)) { String appName; if (builder.appName != null) { appName = builder.appName; } else if (builder.mainClass != null) { int dot = builder.mainClass.lastIndexOf("."); if (dot >= 0 && dot < builder.mainClass.length() - 1) { appName = builder.mainClass.substring(dot + 1, builder.mainClass.length()); } else { appName = builder.mainClass; } } else if (builder.appResource != null) { appName = new File(builder.appResource).getName(); } else { appName = String.valueOf(COUNTER.incrementAndGet()); } String loggerPrefix = getClass().getPackage().getName(); loggerName = String.format("%s.app.%s", loggerPrefix, appName); } if (outputToLog && errorToLog) { pb.redirectErrorStream(true); } pb.environment().put(LauncherProtocol.ENV_LAUNCHER_PORT, String.valueOf(server.getPort())); pb.environment().put(LauncherProtocol.ENV_LAUNCHER_SECRET, secret); try { Process child = pb.start(); InputStream logStream = null; if (loggerName != null) { logStream = outputToLog ? child.getInputStream() : child.getErrorStream(); } handle.setChildProc(child, loggerName, logStream); } catch (IOException ioe) { handle.kill(); throw ioe; } return handle; }
auth token check failed|@Override public void (WireCommands.AuthTokenCheckFailed authTokenCheckFailed) { result.completeExceptionally(new WireCommandFailedException(new AuthenticationException(authTokenCheckFailed.toString()), type, WireCommandFailedException.Reason.AuthFailed)); }
skip task from workflow|@Service public void (String workflowId, String taskReferenceName, SkipTaskRequest skipTaskRequest) { workflowExecutor.skipTaskFromWorkflow(workflowId, taskReferenceName, skipTaskRequest); }
create transaction|public void (String scope, String streamName, Duration latency) { DYNAMIC_LOGGER.incCounterValue(globalMetricName(CREATE_TRANSACTION), 1); DYNAMIC_LOGGER.incCounterValue(CREATE_TRANSACTION, 1, streamTags(scope, streamName)); createTransactionLatency.reportSuccessValue(latency.toMillis()); }
similarity|@Override public double (String label1, String label2) { if (label1 == null || label2 == null) { log.debug("LABELS: " + label1 + ": " + (label1 == null ? "null" : "exists") + ";" + label2 + " vec2:" + (label2 == null ? "null" : "exists")); return Double.NaN; } INDArray vec1 = getWordVectorMatrix(label1).dup(); INDArray vec2 = getWordVectorMatrix(label2).dup(); if (vec1 == null || vec2 == null) { log.debug(label1 + ": " + (vec1 == null ? "null" : "exists") + ";" + label2 + " vec2:" + (vec2 == null ? "null" : "exists")); return Double.NaN; } if (label1.equals(label2)) return 1.0; vec1 = Transforms.unitVec(vec1); vec2 = Transforms.unitVec(vec2); return Transforms.cosineSim(vec1, vec2); }
get surplus queued task count|public static int () { /* * The aim of this method is to return a cheap heuristic guide * for task partitioning when programmers, frameworks, tools, * or languages have little or no idea about task granularity. * In essence by offering this method, we ask users only about * tradeoffs in overhead vs expected throughput and its * variance, rather than how finely to partition tasks. * * In a steady state strict (tree-structured) computation, * each thread makes available for stealing enough tasks for * other threads to remain active. Inductively, if all threads * play by the same rules, each thread should make available * only a constant number of tasks. * * The minimum useful constant is just 1. But using a value of * 1 would require immediate replenishment upon each steal to * maintain enough tasks, which is infeasible. Further, * partitionings/granularities of offered tasks should * minimize steal rates, which in general means that threads * nearer the top of computation tree should generate more * than those nearer the bottom. In perfect steady state, each * thread is at approximately the same level of computation * tree. However, producing extra tasks amortizes the * uncertainty of progress and diffusion assumptions. * * So, users will want to use values larger, but not much * larger than 1 to both smooth over transient shortages and * hedge against uneven progress; as traded off against the * cost of extra task overhead. We leave the user to pick a * threshold value to compare with the results of this call to * guide decisions, but recommend values such as 3. * * When all threads are active, it is on average OK to * estimate surplus strictly locally. In steady-state, if one * thread is maintaining say 2 surplus tasks, then so are * others. So we can just use estimated queue length. * However, this strategy alone leads to serious mis-estimates * in some non-steady-state conditions (ramp-up, ramp-down, * other stalls). We can detect many of these by further * considering the number of "idle" threads, that are known to * have zero queued tasks, so compensate by a factor of * (#idle/#active) threads. */ ForkJoinWorkerThread wt = (ForkJoinWorkerThread) Thread.currentThread(); return wt.workQueue.queueSize() - wt.pool.idlePerActive(); }
get drives on windows|public static List<String> () throws Throwable { loadWindowsDriveInfoLib(); List<String> drives = new ArrayList<String>(); WindowsDriveInfo info = new WindowsDriveInfo(); for (String drive : info.getLogicalDrives()) { if (info.isFixedDisk(drive)) drives.add(drive); } return drives; }
reallocate direct no cleaner|public static ByteBuffer (ByteBuffer buffer, int capacity) { assert USE_DIRECT_BUFFER_NO_CLEANER; int len = capacity - buffer.capacity(); incrementMemoryCounter(len); try { return PlatformDependent0.reallocateDirectNoCleaner(buffer, capacity); } catch (Throwable e) { decrementMemoryCounter(len); throwException(e); return null; } }
stream closed|public void (Status status) { if (closed.compareAndSet(false, true)) { for (StreamTracer tracer : tracers) { tracer.streamClosed(status); } } }
decode with state|public Result (BinaryBitmap image) throws NotFoundException { // Make sure to set up the default state so we don't crash if (readers == null) { setHints(null); } return decodeInternal(image); }
log result|private static <T> void (T result, int level, int indentSize, StringBuilder info) { if (result instanceof Map) { level += 1; int finalLevel = level; ((Map) result).forEach(( k, v) -> { info.append("\n"); info.append(getTabBasedOnLevel(finalLevel, indentSize)).append(k.toString()).append(":"); _logResult(v, finalLevel, indentSize, info); }); } else if (result instanceof List) { int finalLevel = level; ((List) result).forEach( element -> _logResult(element, finalLevel, indentSize, info)); } else if (result instanceof String) { info.append(" ").append(result); } else if (result != null) { try { logger.warn(getTabBasedOnLevel(level, indentSize) + "{}", result); } catch (Exception e) { logger.error("Cannot handle this type: {}", result.getClass().getTypeName()); } } }
item state changed|@Override public void (ItemEvent e) { if (e.getStateChange() == ItemEvent.SELECTED) { // Prepare the new session management method log.debug("Selected new Session Management type: " + e.getItem()); SessionManagementMethodType type = ((SessionManagementMethodType) e.getItem()); // different class, create it now if (selectedMethod == null || !type.isTypeForMethod(selectedMethod)) { // Create the new session management method selectedMethod = type.createSessionManagementMethod(getContextIndex()); } // Show the status panel and configuration button, if needed changeMethodConfigPanel(type); if (type.hasOptionsPanel()) shownConfigPanel.bindMethod(selectedMethod); } }
from flat select|static <IN, OUT> FlatSelectBuilder<IN, OUT> (final PatternFlatSelectFunction<IN, OUT> function) { return new FlatSelectBuilder<>(function); }
load props by suffix|public static void (final File jobPath, final Props props, final String... suffixes) { try { if (jobPath.isDirectory()) { final File[] files = jobPath.listFiles(); if (files != null) { for (final File file : files) { loadPropsBySuffix(file, props, suffixes); } } } else if (endsWith(jobPath, suffixes)) { props.putAll(new Props(null, jobPath.getAbsolutePath())); } } catch (final IOException e) { throw new RuntimeException("Error loading schedule properties.", e); } }
get connection|CompletableFuture<ClientConnection> () throws SegmentSealedException { if (state.isClosed()) { throw new IllegalStateException("SegmentOutputStream is already closed", state.getException()); } if (state.isAlreadySealed()) { throw new SegmentSealedException(this.segmentName); } if (state.getConnection() == null) { reconnect(); } CompletableFuture<ClientConnection> future = new CompletableFuture<>(); state.setupConnection.register(future); return future; }
self checking move|private void (String s3Bucket, String targetS3Bucket, String s3Path, String targetS3Path, String copyMsg) throws IOException, SegmentLoadingException { if (s3Bucket.equals(targetS3Bucket) && s3Path.equals(targetS3Path)) { log.info("No need to move file[s3://%s/%s] onto itself", s3Bucket, s3Path); return; } if (s3Client.doesObjectExist(s3Bucket, s3Path)) { final ListObjectsV2Result listResult = s3Client.listObjectsV2(new ListObjectsV2Request().withBucketName(s3Bucket).withPrefix(s3Path).withMaxKeys(1)); // keyCount is still zero. if (listResult.getObjectSummaries().size() == 0) { // should never happen throw new ISE("Unable to list object [s3://%s/%s]", s3Bucket, s3Path); } final S3ObjectSummary objectSummary = listResult.getObjectSummaries().get(0); if (objectSummary.getStorageClass() != null && StorageClass.fromValue(StringUtils.toUpperCase(objectSummary.getStorageClass())).equals(StorageClass.Glacier)) { throw new AmazonServiceException(StringUtils.format("Cannot move file[s3://%s/%s] of storage class glacier, skipping.", s3Bucket, s3Path)); } else { log.info("Moving file %s", copyMsg); final CopyObjectRequest copyRequest = new CopyObjectRequest(s3Bucket, s3Path, targetS3Bucket, targetS3Path); if (!config.getDisableAcl()) { copyRequest.setAccessControlList(S3Utils.grantFullControlToBucketOwner(s3Client, targetS3Bucket)); } s3Client.copyObject(copyRequest); if (!s3Client.doesObjectExist(targetS3Bucket, targetS3Path)) { throw new IOE("After copy was reported as successful the file doesn't exist in the target location [%s]", copyMsg); } deleteWithRetriesSilent(s3Bucket, s3Path); log.debug("Finished moving file %s", copyMsg); } } else { // ensure object exists in target location if (s3Client.doesObjectExist(targetS3Bucket, targetS3Path)) { log.info("Not moving file [s3://%s/%s], already present in target location [s3://%s/%s]", s3Bucket, s3Path, targetS3Bucket, targetS3Path); } else { throw new SegmentLoadingException("Unable to move file %s, not present in either source or target location", copyMsg); } } }
get snapshot|public AccumulatorSnapshot () { try { return new AccumulatorSnapshot(jobID, taskID, userAccumulators); } catch (Throwable e) { LOG.warn("Failed to serialize accumulators for task.", e); return null; } }
get socket|@Nullable public InternalInstrumented<SocketStats> (long id) { InternalInstrumented<SocketStats> clientSocket = otherSockets.get(id); if (clientSocket != null) { return clientSocket; } return getServerSocket(id); }
update|public void (float newData) { float data = history[0] * decay + newData * (1 - decay); float[] r = new float[Math.min(history.length + 1, historySize)]; System.arraycopy(history, 0, r, 1, Math.min(history.length, r.length - 1)); r[0] = data; history = r; }
get max concurrent runs for flow|public static int (String projectName, String flowName, int defaultMaxConcurrentRuns, Map<Pair<String, String>, Integer> maxConcurrentRunsFlowMap) { return maxConcurrentRunsFlowMap.getOrDefault(new Pair(projectName, flowName), defaultMaxConcurrentRuns); }
is annotated|public boolean (ExecutableElement method) { List<? extends AnnotationMirror> annotationMirrors = method.getAnnotationMirrors(); for (AnnotationMirror annotationMirror : annotationMirrors) { String typeName = annotationMirror.getAnnotationType().toString(); if (!AnnotationUtil.INTERNAL_ANNOTATION_NAMES.contains(typeName)) { return true; } } return false; }
get team|@Override public DataResponse<Team> (ObjectId componentId, String teamId) { Component component = componentRepository.findOne(componentId); CollectorItem item = component.getCollectorItems().get(CollectorType.AgileTool).get(0); // Get one scope by Id Team team = teamRepository.findByTeamId(teamId); Collector collector = collectorRepository.findOne(item.getCollectorId()); return new DataResponse<>(team, collector.getLastExecuted()); }
add field names|public Entity (String... fieldNames) { if (ArrayUtil.isNotEmpty(fieldNames)) { if (null == this.fieldNames) { return setFieldNames(fieldNames); } else { for (String fieldName : fieldNames) { this.fieldNames.add(fieldName); } } } return this; }
read as text|public String (boolean withSheetName) { final ExcelExtractor extractor = getExtractor(); extractor.setIncludeSheetNames(withSheetName); return extractor.getText(); }
configure drop box client|protected void (final Collection<BaseClient> properties) { val db = pac4jProperties.getDropbox(); if (StringUtils.isNotBlank(db.getId()) && StringUtils.isNotBlank(db.getSecret())) { val client = new DropBoxClient(db.getId(), db.getSecret()); configureClient(client, db); LOGGER.debug("Created client [{}] with identifier [{}]", client.getName(), client.getKey()); properties.add(client); } }
append condition|private static void (DbMapping dbMapping, StringBuilder sql, Map<String, Object> values, ResultSet rs) throws SQLException { // 拼接主键 for (Map.Entry<String, String> entry : dbMapping.getTargetPk().entrySet()) { String targetColumnName = entry.getKey(); String srcColumnName = entry.getValue(); if (srcColumnName == null) { srcColumnName = targetColumnName; } sql.append(targetColumnName).append("=? AND "); values.put(targetColumnName, rs.getObject(srcColumnName)); } int len = sql.length(); sql.delete(len - 4, len); }
binomial|public static int (RandomGenerator rng, int n, double p) { if ((p < 0) || (p > 1)) { return 0; } int c = 0; for (int i = 0; i < n; i++) { if (rng.nextDouble() < p) { c++; } } return c; }
generate empty events from aggregators|private Map<String, Object> (Map<String, AggregatorFactory> aggMap, Map<String, PostAggregator> postAggMap) { Map<String, Object> emptyEvents = new LinkedHashMap<>(); aggMap.values().forEach( agg -> { Aggregator aggFactorized = agg.factorize(getEmptyColumnSelectorFactory()); emptyEvents.put(agg.getName(), aggFactorized.get()); }); postAggMap.values().forEach( postAgg -> emptyEvents.put(postAgg.getName(), postAgg.compute(emptyEvents))); return emptyEvents; }
add new entries if absent|public CompletableFuture<Void> (String tableName, Map<String, byte[]> toAdd) { List<TableEntry<byte[], byte[]>> entries = toAdd.entrySet().stream().map( x -> new TableEntryImpl<>(new TableKeyImpl<>(x.getKey().getBytes(Charsets.UTF_8), KeyVersion.NOT_EXISTS), x.getValue())).collect(Collectors.toList()); Supplier<String> errorMessage = () -> String.format("addNewEntriesIfAbsent: table: %s", tableName); return expectingDataExists(withRetries(() -> segmentHelper.updateTableEntries(tableName, entries, authToken.get(), RequestTag.NON_EXISTENT_ID), errorMessage).handle(( r, e) -> { if (e != null) { Throwable unwrap = Exceptions.unwrap(e); if (unwrap instanceof StoreException.WriteConflictException) { throw StoreException.create(StoreException.Type.DATA_EXISTS, errorMessage.get()); } else { log.debug("add new entries to {} threw exception {} {}", tableName, unwrap.getClass(), unwrap.getMessage()); throw new CompletionException(e); } } else { log.trace("entries added to table {}", tableName); return null; } }), null); }
remove all cache entries|@VisibleForTesting void () { List<CacheEntry> entries; synchronized (this.cacheEntries) { entries = new ArrayList<>(this.cacheEntries.values()); this.cacheEntries.clear(); } removeFromCache(entries); if (entries.size() > 0) { log.debug("{}: Cleared all cache entries ({}).", this.traceObjectId, entries.size()); } }
join|public static CodeBlock (Iterable<CodeBlock> codeBlocks, String separator) { return StreamSupport.stream(codeBlocks.spliterator(), false).collect(joining(separator)); }
load|public static HandlerLibrary (Messager messager, Trees trees) { HandlerLibrary library = new HandlerLibrary(messager); try { loadAnnotationHandlers(library, trees); loadVisitorHandlers(library, trees); } catch (IOException e) { System.err.println("Lombok isn't running due to misconfigured SPI files: " + e); } library.calculatePriorities(); return library; }
get same mode top left padding|public static int (int outSize, int inSize, int kernel, int strides, int dilation) { int eKernel = effectiveKernelSize(kernel, dilation); //Note that padBottom is 1 bigger than this if bracketed term is not divisible by 2 int outPad = ((outSize - 1) * strides + eKernel - inSize) / 2; Preconditions.checkState(outPad >= 0, "Invalid padding values calculated: %s - " + "layer configuration is invalid? Input size %s, output size %s, kernel %s, " + "strides %s, dilation %s", outPad, inSize, outSize, kernel, strides, dilation); return outPad; }
get s int|public static String (int Value) { // String Result = ""; // Result = String.format("%d", Value); // return Result; }
configure clear text with prior knowledge|private void (SocketChannel ch) { ch.pipeline().addLast(connectionHandler, new PrefaceFrameWrittenEventHandler(), new UserEventLogger()); configureEndOfPipeline(ch.pipeline()); }
clear registry|@Override public void () { overriddenInstanceStatusMap.clear(); recentCanceledQueue.clear(); recentRegisteredQueue.clear(); recentlyChangedQueue.clear(); registry.clear(); }
wait for rebalance|private void () throws InterruptedException { log.info("Waiting for {} seconds before attempting to rebalance", minRebalanceInterval.getSeconds()); Thread.sleep(minRebalanceInterval.toMillis()); }
build inet socket address|public static InetSocketAddress (String host, int defaultPort) { if (StrUtil.isBlank(host)) { host = LOCAL_IP; } String destHost = null; int port = 0; int index = host.indexOf(":"); if (index != -1) { // host:port形式 destHost = host.substring(0, index); port = Integer.parseInt(host.substring(index + 1)); } else { destHost = host; port = defaultPort; } return new InetSocketAddress(destHost, port); }
can accept|public boolean (WorkChunk c) { if (this.size() < c.size()) // too small compared towork return false; if (c.assignedLabel != null && !c.assignedLabel.contains(node)) // label mismatch return false; if (!(Node.SKIP_BUILD_CHECK_ON_FLYWEIGHTS && item.task instanceof Queue.FlyweightTask) && !nodeAcl.hasPermission(item.authenticate(), Computer.BUILD)) // tasks don't have a permission to run on this node return false; return true; }
install logger|@edu.umd.cs.findbugs.annotations.SuppressFBWarnings("LG_LOST_LOGGER_DUE_TO_WEAK_REFERENCE") private void () { Jenkins.logRecords = handler.getView(); Logger.getLogger("").addHandler(handler); }
start checkpoint scheduler|public void () { synchronized (lock) { if (shutdown) { throw new IllegalArgumentException("Checkpoint coordinator is shut down"); } // make sure all prior timers are cancelled stopCheckpointScheduler(); periodicScheduling = true; long initialDelay = ThreadLocalRandom.current().nextLong(minPauseBetweenCheckpointsNanos / 1_000_000L, baseInterval + 1L); currentPeriodicTrigger = timer.scheduleAtFixedRate(new ScheduledTrigger(), initialDelay, baseInterval, TimeUnit.MILLISECONDS); } }
to byte|public static Byte (Object value, Byte defaultValue) { return convert(Byte.class, value, defaultValue); }
get|public static Binder (Environment environment) { return new Binder(ConfigurationPropertySources.get(environment), new PropertySourcesPlaceholdersResolver(environment)); }
set min frame|public void (final int minFrame) { if (composition == null) { lazyCompositionTasks.add(new LazyCompositionTask() { @Override public void run(LottieComposition composition) { setMinFrame(minFrame); } }); return; } animator.setMinFrame(minFrame); }
thread properties|public final ThreadProperties () { ThreadProperties threadProperties = this.threadProperties; if (threadProperties == null) { Thread thread = this.thread; if (thread == null) { assert !inEventLoop(); submit(NOOP_TASK).syncUninterruptibly(); thread = this.thread; assert thread != null; } threadProperties = new DefaultThreadProperties(thread); if (!PROPERTIES_UPDATER.compareAndSet(this, null, threadProperties)) { threadProperties = this.threadProperties; } } return threadProperties; }
delete job|public final void (String name) { DeleteJobRequest request = DeleteJobRequest.newBuilder().setName(name).build(); deleteJob(request); }
is button enabled for selected messages|protected boolean (List<HttpMessage> httpMessages) { for (HttpMessage httpMessage : httpMessages) { if (httpMessage != null && !isButtonEnabledForSelectedHttpMessage(httpMessage)) { return false; } } return true; }
contains string constant|public boolean (String value) { int index = findUtf8(value); if (index == NOT_FOUND) return false; for (int i = 1; i < maxPoolSize; i++) { if (types[i] == STRING && readValue(offsets[i]) == index) return true; } return false; }
validate non negative|public static int[] (int[] data, String paramName) { validateNonNegative(data, paramName); return validate3(data, paramName); }
dot product attention|public SDVariable (SDVariable queries, SDVariable keys, SDVariable values, SDVariable mask, boolean scaled) { return dotProductAttention(null, queries, keys, values, mask, scaled); }
lookup select hints|void (SqlSelect select, SqlParserPos pos, Collection<SqlMoniker> hintList) { IdInfo info = idPositions.get(pos.toString()); if ((info == null) || (info.scope == null)) { SqlNode fromNode = select.getFrom(); final SqlValidatorScope fromScope = getFromScope(select); lookupFromHints(fromNode, fromScope, pos, hintList); } else { lookupNameCompletionHints(info.scope, info.id.names, info.id.getParserPosition(), hintList); } }
write all impl|@Override protected AutoBuffer (AutoBuffer ab) { for (Key<Model> k : _models.values()) ab.putKey(k); return super.writeAll_impl(ab); }
get first|public static <T> T (Iterable<T> iterable) { if (null != iterable) { return getFirst(iterable.iterator()); } return null; }
get functions|@Override public OperationHandle (SessionHandle sessionHandle, String catalogName, String schemaName, String functionName) throws HiveSQLException { try { TGetFunctionsReq req = new TGetFunctionsReq(sessionHandle.toTSessionHandle(), functionName); req.setCatalogName(catalogName); req.setSchemaName(schemaName); TGetFunctionsResp resp = cliService.GetFunctions(req); checkStatus(resp.getStatus()); TProtocolVersion protocol = sessionHandle.getProtocolVersion(); return new OperationHandle(resp.getOperationHandle(), protocol); } catch (HiveSQLException e) { throw e; } catch (Exception e) { throw new HiveSQLException(e); } }
pointerize op|protected GridPointers (Op op, int... dimensions) { GridPointers pointers = new GridPointers(op, dimensions); AtomicAllocator allocator = AtomicAllocator.getInstance(); // CudaContext context = AtomicAllocator.getInstance().getFlowController().prepareAction(op.z(), op.x(), op.y()); // FIXME: do not leave it as is CudaContext context = (CudaContext) allocator.getDeviceContext().getContext(); pointers.setX(allocator.getPointer(op.x(), context)); pointers.setXShapeInfo(allocator.getPointer(op.x().shapeInfoDataBuffer(), context)); pointers.setZ(allocator.getPointer(op.z(), context)); pointers.setZShapeInfo(allocator.getPointer(op.z().shapeInfoDataBuffer(), context)); pointers.setZLength(op.z().length()); if (op.y() != null) { pointers.setY(allocator.getPointer(op.y(), context)); pointers.setYShapeInfo(allocator.getPointer(op.y().shapeInfoDataBuffer(), context)); } if (dimensions != null && dimensions.length > 0) { DataBuffer dimensionBuffer = Nd4j.getConstantHandler().getConstantBuffer(dimensions, DataType.INT); pointers.setDimensions(allocator.getPointer(dimensionBuffer, context)); pointers.setDimensionsLength(dimensions.length); } // we build TADs if (dimensions != null && dimensions.length > 0) { Pair<DataBuffer, DataBuffer> tadBuffers = tadManager.getTADOnlyShapeInfo(op.x(), dimensions); Pointer devTadShapeInfo = AtomicAllocator.getInstance().getPointer(tadBuffers.getFirst(), context); Pointer devTadOffsets = tadBuffers.getSecond() == null ? null : AtomicAllocator.getInstance().getPointer(tadBuffers.getSecond(), context); // we don't really care, if tadOffsets will be nulls pointers.setTadShape(devTadShapeInfo); pointers.setTadOffsets(devTadOffsets); } return pointers; }
wrap|public static <K, V> Map<K, V> (final Multimap<K, V> source) { if (source != null && !source.isEmpty()) { val inner = source.asMap(); val map = new HashMap<Object, Object>(); inner.forEach(( k, v) -> map.put(k, wrap(v))); return (Map) map; } return new HashMap<>(0); }
has create permission|public boolean (@Nonnull Authentication a, @Nonnull ViewGroup c, @Nonnull ViewDescriptor d) { return true; }
render exception|private static void (final Map model, final HttpServletResponse response) { response.setStatus(HttpServletResponse.SC_BAD_REQUEST); model.put("status", HttpServletResponse.SC_BAD_REQUEST); render(model, response); }
gen ranks|private static void (int noDocs, String path) { Random rand = new Random(Calendar.getInstance().getTimeInMillis()); try (BufferedWriter fw = new BufferedWriter(new FileWriter(path))) { for (int i = 0; i < noDocs; i++) { // Rank StringBuilder rank = new StringBuilder(rand.nextInt(100) + "|"); // URL rank.append("url_" + i + "|"); // Average duration rank.append(rand.nextInt(10) + rand.nextInt(50) + "|\n"); fw.write(rank.toString()); } } catch (IOException e) { e.printStackTrace(); } }
with int|public Postcard (@Nullable String key, int value) { mBundle.putInt(key, value); return this; }
remove no writer|@Nullable V (Object key) { Node<K, V> node = data.remove(nodeFactory.newLookupKey(key)); if (node == null) { return null; } V oldValue; synchronized (node) { oldValue = node.getValue(); if (node.isAlive()) { node.retire(); } } RemovalCause cause; if (oldValue == null) { cause = RemovalCause.COLLECTED; } else if (hasExpired(node, expirationTicker().read())) { cause = RemovalCause.EXPIRED; } else { cause = RemovalCause.EXPLICIT; } if (hasRemovalListener()) { @SuppressWarnings("unchecked") K castKey = (K) key; notifyRemoval(castKey, oldValue, cause); } afterWrite(new RemovalTask(node)); return (cause == RemovalCause.EXPLICIT) ? oldValue : null; }
show license|private void () { final LicenseFrame license = new LicenseFrame(); license.setPostTask(new Runnable() { @Override public void run() { license.dispose(); if (!license.isAccepted()) { return; } createAcceptedLicenseFile(); init(true); } }); license.setVisible(true); }
my dashboards filter count|@RequestMapping(value = "/dashboard/mydashboard/filter/count/{title}/{type}", method = GET, produces = APPLICATION_JSON_VALUE) public long (@PathVariable String title, @PathVariable String type) { return dashboardService.getMyDashboardsByTitleCount(title, type); }
get exit code|public int () { int exitCode = 0; for (ExitCodeGenerator generator : this.generators) { try { int value = generator.getExitCode(); if (value > 0 && value > exitCode || value < 0 && value < exitCode) { exitCode = value; } } catch (Exception ex) { exitCode = (exitCode != 0) ? exitCode : 1; ex.printStackTrace(); } } return exitCode; }
outcome|public static Tag (ServerWebExchange exchange) { HttpStatus status = exchange.getResponse().getStatusCode(); if (status != null) { if (status.is1xxInformational()) { return OUTCOME_INFORMATIONAL; } if (status.is2xxSuccessful()) { return OUTCOME_SUCCESS; } if (status.is3xxRedirection()) { return OUTCOME_REDIRECTION; } if (status.is4xxClientError()) { return OUTCOME_CLIENT_ERROR; } return OUTCOME_SERVER_ERROR; } return OUTCOME_UNKNOWN; }
is annotation present|public boolean (Class<? extends Annotation> annotationClass) { boolean result = false; if (field != null) { result = field.isAnnotationPresent(annotationClass); } if (!result && setter != null) { result = setter.isAnnotationPresent(annotationClass); } if (!result && getter != null) { result = getter.isAnnotationPresent(annotationClass); } return result; }
format between|public static String (long betweenMs, BetweenFormater.Level level) { return new BetweenFormater(betweenMs, level).format(); }
read meta data|private KeyedBackendSerializationProxy<K> (StreamStateHandle metaStateHandle) throws Exception { FSDataInputStream inputStream = null; try { inputStream = metaStateHandle.openInputStream(); cancelStreamRegistry.registerCloseable(inputStream); DataInputView in = new DataInputViewStreamWrapper(inputStream); return readMetaData(in); } finally { if (cancelStreamRegistry.unregisterCloseable(inputStream)) { inputStream.close(); } } }
put authentication|public static void (final Authentication authentication, final RequestContext ctx) { ctx.getConversationScope().put(PARAMETER_AUTHENTICATION, authentication); }
identify driver|public static String (String nameContainsProductInfo) { if (StrUtil.isBlank(nameContainsProductInfo)) { return null; } // 全部转为小写，忽略大小写 nameContainsProductInfo = StrUtil.cleanBlank(nameContainsProductInfo.toLowerCase()); String driver = null; if (nameContainsProductInfo.contains("mysql")) { driver = ClassLoaderUtil.isPresent(DRIVER_MYSQL_V6) ? DRIVER_MYSQL_V6 : DRIVER_MYSQL; } else if (nameContainsProductInfo.contains("oracle")) { driver = ClassLoaderUtil.isPresent(DRIVER_ORACLE) ? DRIVER_ORACLE : DRIVER_ORACLE_OLD; } else if (nameContainsProductInfo.contains("postgresql")) { driver = DRIVER_POSTGRESQL; } else if (nameContainsProductInfo.contains("sqlite")) { driver = DRIVER_SQLLITE3; } else if (nameContainsProductInfo.contains("sqlserver")) { driver = DRIVER_SQLSERVER; } else if (nameContainsProductInfo.contains("hive")) { driver = DRIVER_HIVE; } else if (nameContainsProductInfo.contains("h2")) { driver = DRIVER_H2; } else if (nameContainsProductInfo.startsWith("jdbc:derby://")) { // Derby数据库网络连接方式 driver = DRIVER_DERBY; } else if (nameContainsProductInfo.contains("derby")) { // 嵌入式Derby数据库 driver = DRIVER_DERBY_EMBEDDED; } else if (nameContainsProductInfo.contains("hsqldb")) { // HSQLDB driver = DRIVER_HSQLDB; } else if (nameContainsProductInfo.contains("dm")) { // 达梦7 driver = DRIVER_DM7; } return driver; }
get jar manifest|private Manifest (File container) throws IOException { if (container.isDirectory()) { return null; } JarFile jarFile = (JarFile) jarFiles.get(container); if (jarFile == null) { return null; } return jarFile.getManifest(); }
evaluate|public void (DataSetIterator iterator, Map<String, IEvaluation> variableEvals) { Map<String, Integer> map = new HashMap<>(); Map<String, List<IEvaluation>> variableEvalsList = new HashMap<>(); for (String s : variableEvals.keySet()) { //Only 1 possible output here with DataSetIterator map.put(s, 0); variableEvalsList.put(s, Collections.singletonList(variableEvals.get(s))); } evaluate(new MultiDataSetIteratorAdapter(iterator), variableEvalsList, map); }
get nature|public Nature () { Nature nature = Nature.nz; switch(nPOS) { case CharType.CT_CHINESE: break; case CharType.CT_NUM: case CharType.CT_INDEX: case CharType.CT_CNUM: nature = Nature.m; sWord = Predefine.TAG_NUMBER; break; case CharType.CT_DELIMITER: nature = Nature.w; break; case CharType.CT_LETTER: nature = Nature.nx; sWord = Predefine.TAG_CLUSTER; break; case //12021-2129-3121 CharType.CT_SINGLE: if (//匹配浮点数 Predefine.PATTERN_FLOAT_NUMBER.matcher(sWord).matches()) { nature = Nature.m; sWord = Predefine.TAG_NUMBER; } else { nature = Nature.nx; sWord = Predefine.TAG_CLUSTER; } break; default: break; } return nature; }
main|public static void (String[] args) throws IOException, ClassNotFoundException { String outputDirectory = args[0]; String rootDir = args[1]; for (OptionsClassLocation location : LOCATIONS) { createTable(rootDir, location.getModule(), location.getPackage(), outputDirectory, DEFAULT_PATH_PREFIX); } generateCommonSection(rootDir, outputDirectory, LOCATIONS, DEFAULT_PATH_PREFIX); }
clear|public static void (ObjectReader reader, final CacheScope scope) { withCache(reader, cache -> { if (scope == CacheScope.GLOBAL_SCOPE) { if (debugLogEnabled) logger.debug("clearing global-level cache with size {}", cache.globalCache.size()); cache.globalCache.clear(); } if (debugLogEnabled) logger.debug("clearing app-level serialization cache with size {}", cache.applicationCache.size()); cache.applicationCache.clear(); return null; }); }
new builder|public static void (String projectId, String topicId) throws Exception { ProjectTopicName topic = ProjectTopicName.of(projectId, topicId); Publisher publisher = Publisher.newBuilder(topic).build(); try { // ... } finally { // When finished with the publisher, make sure to shutdown to free up resources. publisher.shutdown(); publisher.awaitTermination(1, TimeUnit.MINUTES); } }
get load balancing configs from service config|@SuppressWarnings("unchecked") @VisibleForTesting public static List<Map<String, ?>> (Map<String, ?> serviceConfig) { /* schema as follows { "loadBalancingConfig": [ {"xds" : { "balancerName": "balancer1", "childPolicy": [...], "fallbackPolicy": [...], } }, {"round_robin": {}} ], "loadBalancingPolicy": "ROUND_ROBIN" // The deprecated policy key } */ List<Map<String, ?>> lbConfigs = new ArrayList<>(); if (serviceConfig.containsKey(SERVICE_CONFIG_LOAD_BALANCING_CONFIG_KEY)) { List<?> configs = getList(serviceConfig, SERVICE_CONFIG_LOAD_BALANCING_CONFIG_KEY); for (Map<String, ?> config : checkObjectList(configs)) { lbConfigs.add(config); } } if (lbConfigs.isEmpty()) { // No LoadBalancingConfig found. Fall back to the deprecated LoadBalancingPolicy if (serviceConfig.containsKey(SERVICE_CONFIG_LOAD_BALANCING_POLICY_KEY)) { // TODO(zhangkun83): check if this is null. String policy = getString(serviceConfig, SERVICE_CONFIG_LOAD_BALANCING_POLICY_KEY); // Convert the policy to a config, so that the caller can handle them in the same way. policy = policy.toLowerCase(Locale.ROOT); Map<String, ?> fakeConfig = Collections.singletonMap(policy, Collections.emptyMap()); lbConfigs.add(fakeConfig); } } return Collections.unmodifiableList(lbConfigs); }
send message to all neighbors|public void (Message m) { if (edgesUsed) { throw new IllegalStateException("Can use either 'getEdges()' or 'sendMessageToAllNeighbors()'" + "exactly once."); } edgesUsed = true; outValue.f1 = m; while (edges.hasNext()) { Tuple next = (Tuple) edges.next(); /* * When EdgeDirection is OUT, the edges iterator only has the out-edges * of the vertex, i.e. the ones where this vertex is src. * next.getField(1) gives the neighbor of the vertex running this ScatterFunction. */ if (getDirection().equals(EdgeDirection.OUT)) { outValue.f0 = next.getField(1); } else /* * When EdgeDirection is IN, the edges iterator only has the in-edges * of the vertex, i.e. the ones where this vertex is trg. * next.getField(10) gives the neighbor of the vertex running this ScatterFunction. */ if (getDirection().equals(EdgeDirection.IN)) { outValue.f0 = next.getField(0); } // When EdgeDirection is ALL, the edges iterator contains both in- and out- edges if (getDirection().equals(EdgeDirection.ALL)) { if (next.getField(0).equals(vertexId)) { // send msg to the trg outValue.f0 = next.getField(1); } else { // send msg to the src outValue.f0 = next.getField(0); } } out.collect(outValue); } }
depth|public int () { if (isLeaf()) { return 0; } int maxDepth = 0; List<Tree> kids = children(); for (Tree kid : kids) { int curDepth = kid.depth(); if (curDepth > maxDepth) { maxDepth = curDepth; } } return maxDepth + 1; }
get matrix center|private Point () { ResultPoint pointA; ResultPoint pointB; ResultPoint pointC; ResultPoint pointD; //Get a white rectangle that can be the border of the matrix in center bull's eye or try { ResultPoint[] cornerPoints = new WhiteRectangleDetector(image).detect(); pointA = cornerPoints[0]; pointB = cornerPoints[1]; pointC = cornerPoints[2]; pointD = cornerPoints[3]; } catch (NotFoundException e) { int cx = image.getWidth() / 2; int cy = image.getHeight() / 2; pointA = getFirstDifferent(new Point(cx + 7, cy - 7), false, 1, -1).toResultPoint(); pointB = getFirstDifferent(new Point(cx + 7, cy + 7), false, 1, 1).toResultPoint(); pointC = getFirstDifferent(new Point(cx - 7, cy + 7), false, -1, 1).toResultPoint(); pointD = getFirstDifferent(new Point(cx - 7, cy - 7), false, -1, -1).toResultPoint(); } //Compute the center of the rectangle int cx = MathUtils.round((pointA.getX() + pointD.getX() + pointB.getX() + pointC.getX()) / 4.0f); int cy = MathUtils.round((pointA.getY() + pointD.getY() + pointB.getY() + pointC.getY()) / 4.0f); // in order to compute a more accurate center. try { ResultPoint[] cornerPoints = new WhiteRectangleDetector(image, 15, cx, cy).detect(); pointA = cornerPoints[0]; pointB = cornerPoints[1]; pointC = cornerPoints[2]; pointD = cornerPoints[3]; } catch (NotFoundException e) { pointA = getFirstDifferent(new Point(cx + 7, cy - 7), false, 1, -1).toResultPoint(); pointB = getFirstDifferent(new Point(cx + 7, cy + 7), false, 1, 1).toResultPoint(); pointC = getFirstDifferent(new Point(cx - 7, cy + 7), false, -1, 1).toResultPoint(); pointD = getFirstDifferent(new Point(cx - 7, cy - 7), false, -1, -1).toResultPoint(); } // Recompute the center of the rectangle cx = MathUtils.round((pointA.getX() + pointD.getX() + pointB.getX() + pointC.getX()) / 4.0f); cy = MathUtils.round((pointA.getY() + pointD.getY() + pointB.getY() + pointC.getY()) / 4.0f); return new Point(cx, cy); }
type convert|public static Object (Object val, String esType) { if (val == null) { return null; } if (esType == null) { return val; } Object res = null; if ("integer".equals(esType)) { if (val instanceof Number) { res = ((Number) val).intValue(); } else { res = Integer.parseInt(val.toString()); } } else if ("long".equals(esType)) { if (val instanceof Number) { res = ((Number) val).longValue(); } else { res = Long.parseLong(val.toString()); } } else if ("short".equals(esType)) { if (val instanceof Number) { res = ((Number) val).shortValue(); } else { res = Short.parseShort(val.toString()); } } else if ("byte".equals(esType)) { if (val instanceof Number) { res = ((Number) val).byteValue(); } else { res = Byte.parseByte(val.toString()); } } else if ("double".equals(esType)) { if (val instanceof Number) { res = ((Number) val).doubleValue(); } else { res = Double.parseDouble(val.toString()); } } else if ("float".equals(esType) || "half_float".equals(esType) || "scaled_float".equals(esType)) { if (val instanceof Number) { res = ((Number) val).floatValue(); } else { res = Float.parseFloat(val.toString()); } } else if ("boolean".equals(esType)) { if (val instanceof Boolean) { res = val; } else if (val instanceof Number) { int v = ((Number) val).intValue(); res = v != 0; } else { res = Boolean.parseBoolean(val.toString()); } } else if ("date".equals(esType)) { if (val instanceof java.sql.Time) { DateTime dateTime = new DateTime(((java.sql.Time) val).getTime()); if (dateTime.getMillisOfSecond() != 0) { res = dateTime.toString("HH:mm:ss.SSS"); } else { res = dateTime.toString("HH:mm:ss"); } } else if (val instanceof java.sql.Timestamp) { DateTime dateTime = new DateTime(((java.sql.Timestamp) val).getTime()); if (dateTime.getMillisOfSecond() != 0) { res = dateTime.toString("yyyy-MM-dd'T'HH:mm:ss.SSS" + Util.timeZone); } else { res = dateTime.toString("yyyy-MM-dd'T'HH:mm:ss" + Util.timeZone); } } else if (val instanceof java.sql.Date || val instanceof Date) { DateTime dateTime; if (val instanceof java.sql.Date) { dateTime = new DateTime(((java.sql.Date) val).getTime()); } else { dateTime = new DateTime(((Date) val).getTime()); } if (dateTime.getHourOfDay() == 0 && dateTime.getMinuteOfHour() == 0 && dateTime.getSecondOfMinute() == 0 && dateTime.getMillisOfSecond() == 0) { res = dateTime.toString("yyyy-MM-dd"); } else { if (dateTime.getMillisOfSecond() != 0) { res = dateTime.toString("yyyy-MM-dd'T'HH:mm:ss.SSS" + Util.timeZone); } else { res = dateTime.toString("yyyy-MM-dd'T'HH:mm:ss" + Util.timeZone); } } } else if (val instanceof Long) { DateTime dateTime = new DateTime(((Long) val).longValue()); if (dateTime.getHourOfDay() == 0 && dateTime.getMinuteOfHour() == 0 && dateTime.getSecondOfMinute() == 0 && dateTime.getMillisOfSecond() == 0) { res = dateTime.toString("yyyy-MM-dd"); } else if (dateTime.getMillisOfSecond() != 0) { res = dateTime.toString("yyyy-MM-dd'T'HH:mm:ss.SSS" + Util.timeZone); } else { res = dateTime.toString("yyyy-MM-dd'T'HH:mm:ss" + Util.timeZone); } } else if (val instanceof String) { String v = ((String) val).trim(); if (v.length() > 18 && v.charAt(4) == '-' && v.charAt(7) == '-' && v.charAt(10) == ' ' && v.charAt(13) == ':' && v.charAt(16) == ':') { String dt = v.substring(0, 10) + "T" + v.substring(11); Date date = Util.parseDate(dt); if (date != null) { DateTime dateTime = new DateTime(date); if (dateTime.getMillisOfSecond() != 0) { res = dateTime.toString("yyyy-MM-dd'T'HH:mm:ss.SSS" + Util.timeZone); } else { res = dateTime.toString("yyyy-MM-dd'T'HH:mm:ss" + Util.timeZone); } } } else if (v.length() == 10 && v.charAt(4) == '-' && v.charAt(7) == '-') { Date date = Util.parseDate(v); if (date != null) { DateTime dateTime = new DateTime(date); res = dateTime.toString("yyyy-MM-dd"); } } } } else if ("binary".equals(esType)) { if (val instanceof byte[]) { Base64 base64 = new Base64(); res = base64.encodeAsString((byte[]) val); } else if (val instanceof Blob) { byte[] b = blobToBytes((Blob) val); Base64 base64 = new Base64(); res = base64.encodeAsString(b); } else if (val instanceof String) { // 对应canal中的单字节编码 byte[] b = ((String) val).getBytes(StandardCharsets.ISO_8859_1); Base64 base64 = new Base64(); res = base64.encodeAsString(b); } } else if ("geo_point".equals(esType)) { if (!(val instanceof String)) { logger.error("es type is geo_point, but source type is not String"); return val; } if (!((String) val).contains(",")) { logger.error("es type is geo_point, source value not contains ',' separator"); return val; } String[] point = ((String) val).split(","); Map<String, Double> location = new HashMap<>(); location.put("lat", Double.valueOf(point[0].trim())); location.put("lon", Double.valueOf(point[1].trim())); return location; } else if ("array".equals(esType)) { if ("".equals(val.toString().trim())) { res = new ArrayList<>(); } else { String value = val.toString(); String separator = ","; if (!value.contains(",")) { if (value.contains(";")) { separator = ";"; } else if (value.contains("|")) { separator = "\\|"; } else if (value.contains("-")) { separator = "-"; } } String[] values = value.split(separator); return Arrays.asList(values); } } else if ("object".equals(esType)) { if ("".equals(val.toString().trim())) { res = new HashMap<>(); } else { res = JSON.parseObject(val.toString(), Map.class); } } else { // 其他类全以字符串处理 res = val.toString(); } return res; }
get callback url|public String (String challenge) { return "http://" + Model.getSingleton().getOptionsParam().getProxyParam().getProxyIp() + ":" + Model.getSingleton().getOptionsParam().getProxyParam().getProxyPort() + "/" + getPrefix() + "/" + challenge; }
internal refresh stats|protected void () { checkState(Thread.holdsLock(root), "Must hold lock to refresh stats"); synchronized (root) { if (subGroups.isEmpty()) { cachedMemoryUsageBytes = 0; for (ManagedQueryExecution query : runningQueries) { cachedMemoryUsageBytes += query.getUserMemoryReservation().toBytes(); } } else { for (Iterator<InternalResourceGroup> iterator = dirtySubGroups.iterator(); iterator.hasNext(); ) { InternalResourceGroup subGroup = iterator.next(); long oldMemoryUsageBytes = subGroup.cachedMemoryUsageBytes; cachedMemoryUsageBytes -= oldMemoryUsageBytes; subGroup.internalRefreshStats(); cachedMemoryUsageBytes += subGroup.cachedMemoryUsageBytes; if (!subGroup.isDirty()) { iterator.remove(); } if (oldMemoryUsageBytes != subGroup.cachedMemoryUsageBytes) { subGroup.updateEligibility(); } } } } }
release read suspended|void (ChannelHandlerContext ctx) { Channel channel = ctx.channel(); channel.attr(READ_SUSPENDED).set(false); channel.config().setAutoRead(true); }
check buffer coherence|protected void () { if (values.length() < length) { throw new IllegalStateException("nnz is larger than capacity of buffers"); } if (values.length() * rank() != indices.length()) { throw new IllegalArgumentException("Sizes of values, indices and shape are incoherent."); } }
error sum|public double () { if (isLeaf()) { return 0.0; } else if (isPreTerminal()) { return error(); } else { double error = 0.0; for (Tree child : children()) { error += child.errorSum(); } return error() + error; } }
show|public void (Node node, PopupVPosition vAlign, PopupHPosition hAlign) { this.show(node, vAlign, hAlign, 0, 0); }
set|public Setting (String group, String key, String value) { this.put(group, key, value); return this; }
get bean introspection|@SuppressWarnings({ "WeakerAccess", "unchecked" }) @Nullable protected BeanIntrospection<Object> (@Nonnull Object object) { //noinspection ConstantConditions if (object == null) { return null; } if (object instanceof Class) { return BeanIntrospector.SHARED.findIntrospection((Class<Object>) object).orElse(null); } return BeanIntrospector.SHARED.findIntrospection((Class<Object>) object.getClass()).orElse(null); }
mul|public static BigDecimal (String v1, String v2) { return mul(new BigDecimal(v1), new BigDecimal(v2)); }
is end to end|static boolean (String fieldName) { return !"Connection".equalsIgnoreCase(fieldName) && !"Keep-Alive".equalsIgnoreCase(fieldName) && !"Proxy-Authenticate".equalsIgnoreCase(fieldName) && !"Proxy-Authorization".equalsIgnoreCase(fieldName) && !"TE".equalsIgnoreCase(fieldName) && !"Trailers".equalsIgnoreCase(fieldName) && !"Transfer-Encoding".equalsIgnoreCase(fieldName) && !"Upgrade".equalsIgnoreCase(fieldName); }
local date time of|public static LocalDateTime (final String value) { var result = (LocalDateTime) null; try { result = LocalDateTime.parse(value, DateTimeFormatter.ISO_LOCAL_DATE_TIME); } catch (final Exception e) { result = null; } if (result == null) { try { result = LocalDateTime.parse(value, DateTimeFormatter.ISO_ZONED_DATE_TIME); } catch (final Exception e) { result = null; } } if (result == null) { try { result = LocalDateTime.parse(value); } catch (final Exception e) { result = null; } } if (result == null) { try { result = LocalDateTime.parse(value.toUpperCase(), DateTimeFormatter.ofPattern("MM/dd/yyyy hh:mm a")); } catch (final Exception e) { result = null; } } if (result == null) { try { result = LocalDateTime.parse(value.toUpperCase(), DateTimeFormatter.ofPattern("MM/dd/yyyy h:mm a")); } catch (final Exception e) { result = null; } } if (result == null) { try { result = LocalDateTime.parse(value, DateTimeFormatter.ofPattern("MM/dd/yyyy HH:mm")); } catch (final Exception e) { result = null; } } if (result == null) { try { val ld = LocalDate.parse(value, DateTimeFormatter.ofPattern("MM/dd/yyyy")); result = LocalDateTime.of(ld, LocalTime.now()); } catch (final Exception e) { result = null; } } if (result == null) { try { val ld = LocalDate.parse(value); result = LocalDateTime.of(ld, LocalTime.now()); } catch (final Exception e) { result = null; } } return result; }
has next|public boolean () { return itr.hasNext(); }
set crumb salt|public void (String salt) { if (Util.fixEmptyAndTrim(salt) == null) { crumbSalt = "hudson.crumb"; } else { crumbSalt = salt; } }
next|public T () { return itr.next(); }
remove|public void () { itr.remove(); }
as|@Nonnull public static ACLContext (@Nonnull Authentication auth) { final ACLContext context = new ACLContext(SecurityContextHolder.getContext()); SecurityContextHolder.setContext(new NonSerializableSecurityContext(auth)); return context; }
get connections|@SuppressWarnings("SynchronizationOnLocalVariableOrMethodParameter") public List<Connection> (final ConnectionMode connectionMode, final String dataSourceName, final int connectionSize, final TransactionType transactionType) throws SQLException { DataSource dataSource = dataSources.get(dataSourceName); if (1 == connectionSize) { return Collections.singletonList(createConnection(transactionType, dataSourceName, dataSource)); } if (ConnectionMode.CONNECTION_STRICTLY == connectionMode) { return createConnections(transactionType, dataSourceName, dataSource, connectionSize); } synchronized (dataSource) { return createConnections(transactionType, dataSourceName, dataSource, connectionSize); } }
get output type|@Override public InputType (InputType... inputType) throws InvalidKerasConfigurationException { if (inputType.length > 1) throw new InvalidKerasConfigurationException("Keras ZeroPadding layer accepts only one input (received " + inputType.length + ")"); return this.getZeroPadding2DLayer().getOutputType(-1, inputType[0]); }
get beans of type|@Nonnull protected <T> Collection<T> (@Nullable BeanResolutionContext resolutionContext, @Nonnull Class<T> beanType) { return getBeansOfTypeInternal(resolutionContext, beanType, null); }
parse|private static Date (String dateStr, DateFormat dateFormat) { try { return dateFormat.parse(dateStr); } catch (Exception e) { String pattern; if (dateFormat instanceof SimpleDateFormat) { pattern = ((SimpleDateFormat) dateFormat).toPattern(); } else { pattern = dateFormat.toString(); } throw new DateException(StrUtil.format("Parse [{}] with format [{}] error!", dateStr, pattern), e); } }
put no copy or await|protected V (K key, V value, boolean publishToWriter, int[] puts) { requireNonNull(key); requireNonNull(value); @SuppressWarnings("unchecked") V[] replaced = (V[]) new Object[1]; cache.asMap().compute(copyOf(key), ( k, expirable) -> { V newValue = copyOf(value); if (publishToWriter && configuration.isWriteThrough()) { publishToCacheWriter(writer::write, () -> new EntryProxy<>(key, value)); } if ((expirable != null) && !expirable.isEternal() && expirable.hasExpired(currentTimeMillis())) { dispatcher.publishExpired(this, key, expirable.get()); statistics.recordEvictions(1L); expirable = null; } long expireTimeMS = getWriteExpireTimeMS((expirable == null)); if ((expirable != null) && (expireTimeMS == Long.MIN_VALUE)) { expireTimeMS = expirable.getExpireTimeMS(); } if (expireTimeMS == 0) { replaced[0] = (expirable == null) ? null : expirable.get(); return null; } else if (expirable == null) { dispatcher.publishCreated(this, key, newValue); } else { replaced[0] = expirable.get(); dispatcher.publishUpdated(this, key, expirable.get(), newValue); } puts[0]++; return new Expirable<>(newValue, expireTimeMS); }); return replaced[0]; }
build glue expression|public static String (List<Column> partitionKeys, List<String> partitionValues) { if (partitionValues == null || partitionValues.isEmpty()) { return null; } if (partitionKeys == null || partitionValues.size() != partitionKeys.size()) { throw new PrestoException(HIVE_METASTORE_ERROR, "Incorrect number of partition values: " + partitionValues); } List<String> predicates = new LinkedList<>(); for (int i = 0; i < partitionValues.size(); i++) { if (!Strings.isNullOrEmpty(partitionValues.get(i))) { predicates.add(buildPredicate(partitionKeys.get(i), partitionValues.get(i))); } } return JOINER.join(predicates); }
remove|public static KV<String, SplitWord> (String key) { MyStaticValue.ENV.remove(key); return CRF.remove(key); }
is permitted address|public boolean (String addr) { if (addr == null || addr.isEmpty()) { return false; } for (DomainMatcher permAddr : permittedAddressesEnabled) { if (permAddr.matches(addr)) { return true; } } return false; }
call constructor|public static Object (final Class<?> cls, final Object... args) { return callConstructor(cls, getTypes(args), args); }
set ssl protocol|public HttpRequest (String protocol) { if (null == this.ssf) { try { this.ssf = SSLSocketFactoryBuilder.create().setProtocol(protocol).build(); } catch (Exception e) { throw new HttpException(e); } } return this; }
is done|public boolean () { if (status() == Status.DONE) { return true; } ChangeRequest updated = reload(Dns.ChangeRequestOption.fields(Dns.ChangeRequestField.STATUS)); return updated == null || updated.status() == Status.DONE; }
get publisher for thread pool|HystrixMetricsPublisherThreadPool (HystrixThreadPoolKey threadPoolKey, HystrixThreadPoolMetrics metrics, HystrixThreadPoolProperties properties) { // attempt to retrieve from cache first HystrixMetricsPublisherThreadPool publisher = threadPoolPublishers.get(threadPoolKey.name()); if (publisher != null) { return publisher; } // it doesn't exist so we need to create it publisher = HystrixPlugins.getInstance().getMetricsPublisher().getMetricsPublisherForThreadPool(threadPoolKey, metrics, properties); // attempt to store it (race other threads) HystrixMetricsPublisherThreadPool existing = threadPoolPublishers.putIfAbsent(threadPoolKey.name(), publisher); if (existing == null) { // we won the thread-race to store the instance we created so initialize it publisher.initialize(); // done registering, return instance that got cached return publisher; } else { // without calling initialize() on it return existing; } }
sniff encoding|public String () throws IOException { class Eureka extends SAXException { final String encoding; public Eureka(String encoding) { this.encoding = encoding; } } try (InputStream in = Files.newInputStream(file.toPath())) { InputSource input = new InputSource(file.toURI().toASCIIString()); input.setByteStream(in); JAXP.newSAXParser().parse(input, new DefaultHandler() { private Locator loc; @Override public void setDocumentLocator(Locator locator) { this.loc = locator; } @Override public void startDocument() throws SAXException { attempt(); } @Override public void startElement(String uri, String localName, String qName, Attributes attributes) throws SAXException { attempt(); // if we still haven't found it at the first start element, then we are not going to find it. throw new Eureka(null); } private void attempt() throws Eureka { if (loc == null) return; if (loc instanceof Locator2) { Locator2 loc2 = (Locator2) loc; String e = loc2.getEncoding(); if (e != null) throw new Eureka(e); } } }); // can't reach here throw new AssertionError(); } catch (Eureka e) { if (e.encoding != null) return e.encoding; return "UTF-8"; } catch (SAXException e) { throw new IOException("Failed to detect encoding of " + file, e); } catch (InvalidPathException e) { throw new IOException(e); } catch (ParserConfigurationException e) { throw new AssertionError(e); } }
create queue|public final Queue (String parent, Queue queue) { CreateQueueRequest request = CreateQueueRequest.newBuilder().setParent(parent).setQueue(queue).build(); return createQueue(request); }
of|public static ListValue (Timestamp first, Timestamp... other) { return newBuilder().addValue(first, other).build(); }
sha base|public static String (String s) { try { MessageDigest messageDigest = MessageDigest.getInstance("SHA-1"); byte[] sha1Bytes = messageDigest.digest(s.getBytes("UTF-8")); return ByteString.of(sha1Bytes).base64(); } catch (NoSuchAlgorithmException e) { throw new AssertionError(e); } catch (UnsupportedEncodingException e) { throw new AssertionError(e); } }
complete|public static StreamTruncationRecord (StreamTruncationRecord toComplete) { Preconditions.checkState(toComplete.updating); ImmutableSet.Builder<Long> builder = ImmutableSet.builder(); builder.addAll(toComplete.deletedSegments); builder.addAll(toComplete.toDelete); return StreamTruncationRecord.builder().updating(false).span(toComplete.span).streamCut(toComplete.streamCut).deletedSegments(builder.build()).toDelete(ImmutableSet.of()).sizeTill(toComplete.sizeTill).build(); }
init|public void () { String rootPath = ManagePathUtils.getRoot(); String channelRootPath = ManagePathUtils.getChannelRoot(); String nodeRootPath = ManagePathUtils.getNodeRoot(); try { zookeeper.create(rootPath, new byte[0], CreateMode.PERSISTENT); zookeeper.create(channelRootPath, new byte[0], CreateMode.PERSISTENT); zookeeper.create(nodeRootPath, new byte[0], CreateMode.PERSISTENT); } catch (ZkNodeExistsException e) { } catch (ZkException e) { throw new ArbitrateException("system_init", e); } }
next proxy|private Proxy () throws IOException { if (!hasNextProxy()) { throw new SocketException("No route to " + address.url().host() + "; exhausted proxy configurations: " + proxies); } Proxy result = proxies.get(nextProxyIndex++); resetNextInetSocketAddress(result); return result; }
open|public void (File file, final long filePosition) throws FileNotFoundException, IOException { fin = new FileInputStream(file); ensureCapacity(BIN_LOG_HEADER_SIZE); if (BIN_LOG_HEADER_SIZE != fin.read(buffer, 0, BIN_LOG_HEADER_SIZE)) { throw new IOException("No binlog file header"); } if (buffer[0] != BINLOG_MAGIC[0] || buffer[1] != BINLOG_MAGIC[1] || buffer[2] != BINLOG_MAGIC[2] || buffer[3] != BINLOG_MAGIC[3]) { throw new IOException("Error binlog file header: " + Arrays.toString(Arrays.copyOf(buffer, BIN_LOG_HEADER_SIZE))); } limit = 0; origin = 0; position = 0; if (filePosition > BIN_LOG_HEADER_SIZE) { final int maxFormatDescriptionEventLen = FormatDescriptionLogEvent.LOG_EVENT_MINIMAL_HEADER_LEN + FormatDescriptionLogEvent.ST_COMMON_HEADER_LEN_OFFSET + LogEvent.ENUM_END_EVENT + LogEvent.BINLOG_CHECKSUM_ALG_DESC_LEN + LogEvent.CHECKSUM_CRC32_SIGNATURE_LEN; ensureCapacity(maxFormatDescriptionEventLen); limit = fin.read(buffer, 0, maxFormatDescriptionEventLen); limit = (int) getUint32(LogEvent.EVENT_LEN_OFFSET); fin.getChannel().position(filePosition); } }
set image asset delegate|public void (@SuppressWarnings("NullableProblems") ImageAssetDelegate assetDelegate) { this.imageAssetDelegate = assetDelegate; if (imageAssetManager != null) { imageAssetManager.setDelegate(assetDelegate); } }
refer|@Override public synchronized T () { if (proxyIns != null) { return proxyIns; } referenceConfig = new ReferenceConfig<T>(); covert(consumerConfig, referenceConfig); proxyIns = referenceConfig.get(); return proxyIns; }
set cookies|public void (List<HttpCookie> cookies) { if (cookies.isEmpty()) { setHeader(HttpHeader.COOKIE, null); } StringBuilder sbData = new StringBuilder(); for (HttpCookie c : cookies) { sbData.append(c.getName()); sbData.append('='); sbData.append(c.getValue()); sbData.append("; "); } if (sbData.length() <= 3) { setHeader(HttpHeader.COOKIE, null); return; } final String data = sbData.substring(0, sbData.length() - 2); setHeader(HttpHeader.COOKIE, data); }
main|public static void (String[] args) throws Exception { CustomHeaderClient client = new CustomHeaderClient("localhost", 50051); try { /* Access a service running on the local machine on port 50051 */ String user = "world"; if (args.length > 0) { user = args[0]; /* Use the arg as the name to greet if provided */ } client.greet(user); } finally { client.shutdown(); } }
create sql type|public static RelDataType (final RelDataTypeFactory typeFactory, final SqlTypeName typeName) { return createSqlTypeWithNullability(typeFactory, typeName, false); }
run|@Override public void () { license.dispose(); if (!license.isAccepted()) { return; } createAcceptedLicenseFile(); init(true); }
check free slot at|private boolean (final long sequence) { final long wrapPoint = sequence - bufferSize; if (wrapPoint > flushSequence.get()) { // 刚好追上一轮 return false; } else { return true; } }
acknowledge task|public final void (String name, Timestamp scheduleTime) { AcknowledgeTaskRequest request = AcknowledgeTaskRequest.newBuilder().setName(name).setScheduleTime(scheduleTime).build(); acknowledgeTask(request); }
delete all alerts|@Override public int () throws DatabaseException { SqlPreparedStatementWrapper psDeleteAllAlerts = null; try { psDeleteAllAlerts = DbSQL.getSingleton().getPreparedStatement("alert.ps.deleteall"); return psDeleteAllAlerts.getPs().executeUpdate(); } catch (SQLException e) { throw new DatabaseException(e); } finally { DbSQL.getSingleton().releasePreparedStatement(psDeleteAllAlerts); } }
visit|@Override public void (JunitXmlReport report) { int testsPassed = report.getTests() - report.getFailures() - report.getErrors(); Map<String, Pair<Integer, CodeQualityMetricStatus>> metricsMap = new HashMap<>(); metricsMap.put(TOTAL_NO_OF_TESTS, Pair.of(report.getTests(), CodeQualityMetricStatus.Ok)); metricsMap.put(TEST_FAILURES, Pair.of(report.getFailures(), report.getFailures() > 0 ? CodeQualityMetricStatus.Warning : CodeQualityMetricStatus.Ok)); metricsMap.put(TEST_ERRORS, Pair.of(report.getErrors(), report.getErrors() > 0 ? CodeQualityMetricStatus.Alert : CodeQualityMetricStatus.Ok)); metricsMap.put(TEST_SUCCESS_DENSITY, Pair.of(testsPassed, CodeQualityMetricStatus.Ok)); if (null != report.getTimestamp()) { long timestamp = Math.max(quality.getTimestamp(), report.getTimestamp().toGregorianCalendar().getTimeInMillis()); quality.setTimestamp(timestamp); } quality.setType(CodeQualityType.StaticAnalysis); // finally produce the result this.sumMetrics(metricsMap); }
unmigrate builds dir|private void (File builds) throws Exception { File mapFile = new File(builds, MAP_FILE); if (!mapFile.isFile()) { System.err.println(builds + " does not look to have been migrated yet; skipping"); return; } for (File build : builds.listFiles()) { int number; try { number = Integer.parseInt(build.getName()); } catch (NumberFormatException x) { continue; } File buildXml = new File(build, "build.xml"); if (!buildXml.isFile()) { System.err.println(buildXml + " did not exist"); continue; } String xml = FileUtils.readFileToString(buildXml, Charsets.UTF_8); Matcher m = TIMESTAMP_ELT.matcher(xml); if (!m.find()) { System.err.println(buildXml + " did not contain <timestamp> as expected"); continue; } long timestamp = Long.parseLong(m.group(1)); String nl = m.group(2); xml = m.replaceFirst(" <number>" + number + "</number>" + nl); m = ID_ELT.matcher(xml); String id; if (m.find()) { id = m.group(1); xml = m.replaceFirst(""); } else { // Post-migration build. We give it a new ID based on its timestamp. id = legacyIdFormatter.format(new Date(timestamp)); } FileUtils.write(buildXml, xml, Charsets.UTF_8); if (!build.renameTo(new File(builds, id))) { System.err.println(build + " could not be renamed"); } Util.createSymlink(builds, id, Integer.toString(number), StreamTaskListener.fromStderr()); } Util.deleteFile(mapFile); System.err.println(builds + " has been restored to its original format"); }
handle unrecognized property|@ExceptionHandler(UnrecognizedPropertyException.class) public ResponseEntity<?> (UnrecognizedPropertyException ex, HttpServletRequest request) { ErrorResponse response = new ErrorResponse(); response.addFieldError(ex.getPropertyName(), ex.getMessage()); return ResponseEntity.status(HttpStatus.BAD_REQUEST).body(response); }
compute score|@Override public double (double fullNetRegTerm, boolean training, LayerWorkspaceMgr workspaceMgr) { if (input == null) throw new IllegalStateException("Cannot calculate score without input and labels " + layerId()); INDArray preOut = preOutput2d(training, workspaceMgr); ILossFunction lossFunction = layerConf().getLossFn(); double score = lossFunction.computeScore(getLabels2d(workspaceMgr, ArrayType.FF_WORKING_MEM), preOut, layerConf().getActivationFn(), maskArray, false); if (conf().isMiniBatch()) score /= getInputMiniBatchSize(); score += fullNetRegTerm; this.score = score; return score; }
peek|public Long () throws InterruptedException { lock.lockInterruptibly(); try { while (queue.size() == 0) { notEmpty.await(); } return queue.peek(); } finally { lock.unlock(); } }
execute connect|private boolean () throws IOException, HttpException { this.connectMethod = new ConnectMethod(this.hostConfiguration); this.connectMethod.getParams().setDefaults(this.hostConfiguration.getParams()); String agent = (String) getParams().getParameter(PARAM_DEFAULT_USER_AGENT_CONNECT_REQUESTS); if (agent != null) { this.connectMethod.setRequestHeader("User-Agent", agent); } int code; for (; ; ) { if (!this.conn.isOpen()) { this.conn.open(); } if (this.params.isAuthenticationPreemptive() || this.state.isAuthenticationPreemptive()) { LOG.debug("Preemptively sending default basic credentials"); this.connectMethod.getProxyAuthState().setPreemptive(); this.connectMethod.getProxyAuthState().setAuthAttempted(true); } try { authenticateProxy(this.connectMethod); } catch (AuthenticationException e) { LOG.error(e.getMessage(), e); } applyConnectionParams(this.connectMethod); this.connectMethod.execute(state, this.conn); code = this.connectMethod.getStatusCode(); boolean retry = false; AuthState authstate = this.connectMethod.getProxyAuthState(); authstate.setAuthRequested(code == HttpStatus.SC_PROXY_AUTHENTICATION_REQUIRED); if (authstate.isAuthRequested()) { if (processAuthenticationResponse(this.connectMethod)) { retry = true; } } if (!retry) { break; } if (this.connectMethod.getResponseBodyAsStream() != null) { this.connectMethod.getResponseBodyAsStream().close(); } } if ((code >= 200) && (code < 300)) { this.conn.tunnelCreated(); // Drop the connect method, as it is no longer needed this.connectMethod = null; return true; } else { return false; } }
create annotation|public static <A extends Annotation> AnnotationValues<A> (Class<A> type, JCAnnotation anno, final JavacNode node) { Map<String, AnnotationValue> values = new HashMap<String, AnnotationValue>(); List<JCExpression> arguments = anno.getArguments(); for (JCExpression arg : arguments) { String mName; JCExpression rhs; java.util.List<String> raws = new ArrayList<String>(); java.util.List<Object> guesses = new ArrayList<Object>(); java.util.List<Object> expressions = new ArrayList<Object>(); final java.util.List<DiagnosticPosition> positions = new ArrayList<DiagnosticPosition>(); if (arg instanceof JCAssign) { JCAssign assign = (JCAssign) arg; mName = assign.lhs.toString(); rhs = assign.rhs; } else { rhs = arg; mName = "value"; } if (rhs instanceof JCNewArray) { List<JCExpression> elems = ((JCNewArray) rhs).elems; for (JCExpression inner : elems) { raws.add(inner.toString()); expressions.add(inner); if (inner instanceof JCAnnotation) { try { @SuppressWarnings("unchecked") Class<A> innerClass = (Class<A>) Class.forName(inner.type.toString()); guesses.add(createAnnotation(innerClass, (JCAnnotation) inner, node)); } catch (ClassNotFoundException ex) { guesses.add(calculateGuess(inner)); } } else { guesses.add(calculateGuess(inner)); } positions.add(inner.pos()); } } else { raws.add(rhs.toString()); expressions.add(rhs); if (rhs instanceof JCAnnotation) { try { @SuppressWarnings("unchecked") Class<A> innerClass = (Class<A>) Class.forName(rhs.type.toString()); guesses.add(createAnnotation(innerClass, (JCAnnotation) rhs, node)); } catch (ClassNotFoundException ex) { guesses.add(calculateGuess(rhs)); } } else { guesses.add(calculateGuess(rhs)); } positions.add(rhs.pos()); } values.put(mName, new AnnotationValue(node, raws, expressions, guesses, true) { @Override public void setError(String message, int valueIdx) { if (valueIdx < 0) node.addError(message); else node.addError(message, positions.get(valueIdx)); } @Override public void setWarning(String message, int valueIdx) { if (valueIdx < 0) node.addWarning(message); else node.addWarning(message, positions.get(valueIdx)); } }); } for (Method m : type.getDeclaredMethods()) { if (!Modifier.isPublic(m.getModifiers())) continue; String name = m.getName(); if (!values.containsKey(name)) { values.put(name, new AnnotationValue(node, new ArrayList<String>(), new ArrayList<Object>(), new ArrayList<Object>(), false) { @Override public void setError(String message, int valueIdx) { node.addError(message); } @Override public void setWarning(String message, int valueIdx) { node.addWarning(message); } }); } } return new AnnotationValues<A>(type, values, node); }
cannot found service|private SofaRpcException (String appName, String serviceName) { String errorMsg = LogCodes.getLog(LogCodes.ERROR_PROVIDER_SERVICE_CANNOT_FOUND, serviceName); LOGGER.errorWithApp(appName, errorMsg); return new SofaRpcException(RpcErrorType.SERVER_NOT_FOUND_INVOKER, errorMsg); }
create|public static Dependencies (long startTs, long endTs, List<DependencyLink> links) { return new Dependencies(startTs, endTs, links); }
create solution set update output collector|protected Collector<OT> (Collector<OT> delegate) { Broker<Object> solutionSetBroker = SolutionSetBroker.instance(); Object ss = solutionSetBroker.get(brokerKey()); if (ss instanceof CompactingHashTable) { @SuppressWarnings("unchecked") CompactingHashTable<OT> solutionSet = (CompactingHashTable<OT>) ss; return new SolutionSetUpdateOutputCollector<OT>(solutionSet, delegate); } else if (ss instanceof JoinHashMap) { @SuppressWarnings("unchecked") JoinHashMap<OT> map = (JoinHashMap<OT>) ss; return new SolutionSetObjectsUpdateOutputCollector<OT>(map, delegate); } else { throw new RuntimeException("Unrecognized solution set handle: " + ss); } }
resolve single type argument|private static Optional<Class> (Type genericType) { if (genericType instanceof ParameterizedType) { ParameterizedType pt = (ParameterizedType) genericType; Type[] actualTypeArguments = pt.getActualTypeArguments(); if (actualTypeArguments.length == 1) { Type actualTypeArgument = actualTypeArguments[0]; return resolveParameterizedTypeArgument(actualTypeArgument); } } return Optional.empty(); }
is assignable|public static void (Class<?> superType, Class<?> subType, String errorMsgTemplate, Object... params) throws IllegalArgumentException { notNull(superType, "Type to check against must not be null"); if (subType == null || !superType.isAssignableFrom(subType)) { throw new IllegalArgumentException(StrUtil.format(errorMsgTemplate, params)); } }
decorate logger|public OutputStream (AbstractBuild build, OutputStream logger) throws IOException, InterruptedException { if (Util.isOverridden(ConsoleLogFilter.class, getClass(), "decorateLogger", Run.class, OutputStream.class)) { // old client calling newer implementation. forward the call. return decorateLogger((Run) build, logger); } else { // happens only if the subtype fails to override neither decorateLogger method throw new AssertionError("The plugin '" + this.getClass().getName() + "' still uses " + "deprecated decorateLogger(AbstractBuild,OutputStream) method. " + "Update the plugin to use setUp(Run,OutputStream) instead."); } }
create generic indexed version two|private static <T> GenericIndexed<T> (ByteBuffer byteBuffer, ObjectStrategy<T> strategy, SmooshedFileMapper fileMapper) { if (fileMapper == null) { throw new IAE("SmooshedFileMapper can not be null for version 2."); } boolean allowReverseLookup = byteBuffer.get() == REVERSE_LOOKUP_ALLOWED; int logBaseTwoOfElementsPerValueFile = byteBuffer.getInt(); int numElements = byteBuffer.getInt(); try { String columnName = SERIALIZER_UTILS.readString(byteBuffer); int elementsPerValueFile = 1 << logBaseTwoOfElementsPerValueFile; int numberOfFilesRequired = getNumberOfFilesRequired(elementsPerValueFile, numElements); ByteBuffer[] valueBuffersToUse = new ByteBuffer[numberOfFilesRequired]; for (int i = 0; i < numberOfFilesRequired; i++) { // SmooshedFileMapper.mapFile() contract guarantees that the valueBuffer's limit equals to capacity. ByteBuffer valueBuffer = fileMapper.mapFile(GenericIndexedWriter.generateValueFileName(columnName, i)); valueBuffersToUse[i] = valueBuffer.asReadOnlyBuffer(); } ByteBuffer headerBuffer = fileMapper.mapFile(GenericIndexedWriter.generateHeaderFileName(columnName)); return new GenericIndexed<>(valueBuffersToUse, headerBuffer, strategy, allowReverseLookup, logBaseTwoOfElementsPerValueFile, numElements); } catch (IOException e) { throw new RuntimeException("File mapping failed.", e); } }
sort|@Nonnull @Restricted(NoExternalUse.class) // invoked from stapler view @SuppressWarnings("unused") public List<View> (@Nonnull List<? extends View> views) { List<View> result = new ArrayList<>(views); result.sort(new Comparator<View>() { @Override public int compare(View o1, View o2) { return o1.getDisplayName().compareTo(o2.getDisplayName()); } }); return result; }
append utf lines|public static <T> File (Collection<T> list, File file) throws IORuntimeException { return appendLines(list, file, CharsetUtil.CHARSET_UTF_8); }
prime connections async|public List<Future<Boolean>> (final List<Server> servers, final PrimeConnectionListener listener) { if (servers == null) { return Collections.emptyList(); } List<Server> allServers = new ArrayList<Server>(); allServers.addAll(servers); if (allServers.size() == 0) { logger.debug("RestClient:" + name + ". No nodes/servers to prime connections"); return Collections.emptyList(); } logger.info("Priming Connections for RestClient:" + name + ", numServers:" + allServers.size()); List<Future<Boolean>> ftList = new ArrayList<Future<Boolean>>(); for (Server s : allServers) { // prevent the server to be used by load balancer // will be set to true when priming is done s.setReadyToServe(false); if (aSync) { Future<Boolean> ftC = null; try { ftC = makeConnectionASync(s, listener); ftList.add(ftC); } catch (RejectedExecutionException ree) { logger.error("executor submit failed", ree); } catch (Exception e) { logger.error("general error", e); } } else { connectToServer(s, listener); } } return ftList; }
apply|@Deprecated public <R> SingleOutputStreamOperator<R> (R initialValue, FoldFunction<T, R> foldFunction, WindowFunction<R, R, K, W> function) { TypeInformation<R> resultType = TypeExtractor.getFoldReturnTypes(foldFunction, input.getType(), Utils.getCallLocationName(), true); return apply(initialValue, foldFunction, function, resultType); }
compare|@Override public int (View o1, View o2) { return o1.getDisplayName().compareTo(o2.getDisplayName()); }
decode|public String (String morse) { Assert.notNull(morse, "Morse should not be null."); final char dit = this.dit; final char dah = this.dah; final char split = this.split; if (false == StrUtil.containsOnly(morse, dit, dah, split)) { throw new IllegalArgumentException("Incorrect morse."); } final List<String> words = StrUtil.split(morse, split); final StringBuilder textBuilder = new StringBuilder(); Integer codePoint; for (String word : words) { if (StrUtil.isEmpty(word)) { continue; } word = word.replace(dit, '0').replace(dah, '1'); codePoint = dictionaries.get(word); if (codePoint == null) { codePoint = Integer.valueOf(word, 2); } textBuilder.appendCodePoint(codePoint); } return textBuilder.toString(); }
tasks|private void () { Runnable r; while ((r = sslEngine.getDelegatedTask()) != null) { r.run(); } hs = sslEngine.getHandshakeStatus(); }
to preds|public static double[] (double in[], float[] out, double[] preds, int nclasses, double[] priorClassDistrib, double defaultThreshold) { if (nclasses > 2) { for (int i = 0; i < out.length; ++i) preds[1 + i] = out[i]; preds[0] = GenModel.getPrediction(preds, priorClassDistrib, in, defaultThreshold); } else if (nclasses == 2) { preds[1] = 1f - out[0]; preds[2] = out[0]; preds[0] = GenModel.getPrediction(preds, priorClassDistrib, in, defaultThreshold); } else { preds[0] = out[0]; } return preds; }
fold|@PublicEvolving @Deprecated public <ACC, R> SingleOutputStreamOperator<R> (ACC initialValue, FoldFunction<T, ACC> foldFunction, AllWindowFunction<ACC, R, W> function, TypeInformation<ACC> foldAccumulatorType, TypeInformation<R> resultType) { if (foldFunction instanceof RichFunction) { throw new UnsupportedOperationException("FoldFunction of fold can not be a RichFunction."); } if (windowAssigner instanceof MergingWindowAssigner) { throw new UnsupportedOperationException("Fold cannot be used with a merging WindowAssigner."); } //clean the closures function = input.getExecutionEnvironment().clean(function); foldFunction = input.getExecutionEnvironment().clean(foldFunction); String callLocation = Utils.getCallLocationName(); String udfName = "AllWindowedStream." + callLocation; String opName; KeySelector<T, Byte> keySel = input.getKeySelector(); OneInputStreamOperator<T, R> operator; if (evictor != null) { @SuppressWarnings({ "unchecked", "rawtypes" }) TypeSerializer<StreamRecord<T>> streamRecordSerializer = (TypeSerializer<StreamRecord<T>>) new StreamElementSerializer(input.getType().createSerializer(getExecutionEnvironment().getConfig())); ListStateDescriptor<StreamRecord<T>> stateDesc = new ListStateDescriptor<>("window-contents", streamRecordSerializer); opName = "TriggerWindow(" + windowAssigner + ", " + stateDesc + ", " + trigger + ", " + evictor + ", " + udfName + ")"; operator = new EvictingWindowOperator<>(windowAssigner, windowAssigner.getWindowSerializer(getExecutionEnvironment().getConfig()), keySel, input.getKeyType().createSerializer(getExecutionEnvironment().getConfig()), stateDesc, new InternalIterableAllWindowFunction<>(new FoldApplyAllWindowFunction<>(initialValue, foldFunction, function, foldAccumulatorType)), trigger, evictor, allowedLateness, lateDataOutputTag); } else { FoldingStateDescriptor<T, ACC> stateDesc = new FoldingStateDescriptor<>("window-contents", initialValue, foldFunction, foldAccumulatorType.createSerializer(getExecutionEnvironment().getConfig())); opName = "TriggerWindow(" + windowAssigner + ", " + stateDesc + ", " + trigger + ", " + udfName + ")"; operator = new WindowOperator<>(windowAssigner, windowAssigner.getWindowSerializer(getExecutionEnvironment().getConfig()), keySel, input.getKeyType().createSerializer(getExecutionEnvironment().getConfig()), stateDesc, new InternalSingleValueAllWindowFunction<>(function), trigger, allowedLateness, lateDataOutputTag); } return input.transform(opName, resultType, operator).forceNonParallel(); }
spy|@Incubating @CheckReturnValue public static <T> T (Class<T> classToSpy) { return MOCKITO_CORE.mock(classToSpy, withSettings().useConstructor().defaultAnswer(CALLS_REAL_METHODS)); }
contains|public boolean (String group, Collection<String> values) { final LinkedHashSet<String> valueSet = getValues(group); if (CollectionUtil.isEmpty(values) || CollectionUtil.isEmpty(valueSet)) { return false; } return valueSet.containsAll(values); }
do evaluation|public IEvaluation[] (JavaRDD<String> data, int evalNumWorkers, int evalBatchSize, DataSetLoader loader, IEvaluation... emptyEvaluations) { return doEvaluation(data, evalNumWorkers, evalBatchSize, loader, null, emptyEvaluations); }
close async|@Override public CompletableFuture<Void> () { synchronized (lock) { if (running) { LOG.info("Shutting down Flink Mini Cluster"); try { final long shutdownTimeoutMillis = miniClusterConfiguration.getConfiguration().getLong(ClusterOptions.CLUSTER_SERVICES_SHUTDOWN_TIMEOUT); final int numComponents = 2 + miniClusterConfiguration.getNumTaskManagers(); final Collection<CompletableFuture<Void>> componentTerminationFutures = new ArrayList<>(numComponents); componentTerminationFutures.addAll(terminateTaskExecutors()); componentTerminationFutures.add(shutDownResourceManagerComponents()); final FutureUtils.ConjunctFuture<Void> componentsTerminationFuture = FutureUtils.completeAll(componentTerminationFutures); final CompletableFuture<Void> metricSystemTerminationFuture = FutureUtils.composeAfterwards(componentsTerminationFuture, this::closeMetricSystem); // shut down the RpcServices final CompletableFuture<Void> rpcServicesTerminationFuture = metricSystemTerminationFuture.thenCompose((Void ignored) -> terminateRpcServices()); final CompletableFuture<Void> remainingServicesTerminationFuture = FutureUtils.runAfterwards(rpcServicesTerminationFuture, this::terminateMiniClusterServices); final CompletableFuture<Void> executorsTerminationFuture = FutureUtils.runAfterwards(remainingServicesTerminationFuture, () -> terminateExecutors(shutdownTimeoutMillis)); executorsTerminationFuture.whenComplete((Void ignored, Throwable throwable) -> { if (throwable != null) { terminationFuture.completeExceptionally(ExceptionUtils.stripCompletionException(throwable)); } else { terminationFuture.complete(null); } }); } finally { running = false; } } return terminationFuture; } }
shutdown|public void () { /** * Probably we don't need this method in practice */ if (initLocker.get() && shutdownLocker.compareAndSet(false, true)) { // do shutdown log.info("Shutting down transport..."); // we just sending out ShutdownRequestMessage //transport.sendMessage(new ShutdownRequestMessage()); transport.shutdown(); executor.shutdown(); initFinished.set(false); initLocker.set(false); shutdownLocker.set(false); } }
fail allocation|@Override public Optional<ResourceID> (final AllocationID allocationID, final Exception cause) { componentMainThreadExecutor.assertRunningInMainThread(); final PendingRequest pendingRequest = pendingRequests.removeKeyB(allocationID); if (pendingRequest != null) { // request was still pending failPendingRequest(pendingRequest, cause); return Optional.empty(); } else { return tryFailingAllocatedSlot(allocationID, cause); } // TODO: add some unit tests when the previous two are ready, the allocation may failed at any phase }
resolve principal|protected Principal (final AuthenticationHandler handler, final PrincipalResolver resolver, final Credential credential, final Principal principal) { if (resolver.supports(credential)) { try { val p = resolver.resolve(credential, Optional.ofNullable(principal), Optional.ofNullable(handler)); LOGGER.debug("[{}] resolved [{}] from [{}]", resolver, p, credential); return p; } catch (final Exception e) { LOGGER.error("[{}] failed to resolve principal from [{}]", resolver, credential, e); } } else { LOGGER.warn("[{}] is configured to use [{}] but it does not support [{}], which suggests a configuration problem.", handler.getName(), resolver, credential); } return null; }
random color|public static Color (Random random) { if (null == random) { random = RandomUtil.getRandom(); } return new Color(random.nextInt(255), random.nextInt(255), random.nextInt(255)); }
is form data|public static boolean (HttpRequest<?> request) { Optional<MediaType> opt = request.getContentType(); if (opt.isPresent()) { MediaType contentType = opt.get(); return (contentType.equals(MediaType.APPLICATION_FORM_URLENCODED_TYPE) || contentType.equals(MediaType.MULTIPART_FORM_DATA_TYPE)); } return false; }
get uri reference|public String () throws URIException { char[] uriReference = getRawURIReference(); return (uriReference == null) ? null : decode(uriReference, getProtocolCharset()); }
add panel|private static void (TabbedPanel2 tabbedPanel, AbstractPanel panel, boolean visible) { if (visible) { tabbedPanel.addTab(panel); } else { tabbedPanel.addTabHidden(panel); } }
single|private void (long timestamp) throws InterruptedException { lastTimestamps.add(timestamp); if (timestamp < state()) { // 针对mysql事务中会出现时间跳跃 // 例子： // 2012-08-08 16:24:26 事务头 // 2012-08-08 16:24:24 变更记录 // 2012-08-08 16:24:25 变更记录 // 2012-08-08 16:24:26　事务尾 // 针对这种case，一旦发现timestamp有回退的情况，直接更新threshold，强制阻塞其他的操作，等待最小数据优先处理完成 // 更新为最小值 threshold = timestamp; } if (lastTimestamps.size() >= groupSize) { // 判断队列是否需要触发 // 触发下一个出队列的数据 Long minTimestamp = this.lastTimestamps.peek(); if (minTimestamp != null) { threshold = minTimestamp; notify(minTimestamp); } } else { // 如果不满足队列长度，需要阻塞等待 threshold = Long.MIN_VALUE; } }
run|@Override public void (LottieComposition composition) { setMinFrame(minFrame); }
time string as|public static long (String str, TimeUnit unit) { String lower = str.toLowerCase(Locale.ROOT).trim(); try { Matcher m = Pattern.compile("(-?[0-9]+)([a-z]+)?").matcher(lower); if (!m.matches()) { throw new NumberFormatException("Failed to parse time string: " + str); } long val = Long.parseLong(m.group(1)); String suffix = m.group(2); // Check for invalid suffixes if (suffix != null && !timeSuffixes.containsKey(suffix)) { throw new NumberFormatException("Invalid suffix: \"" + suffix + "\""); } // If suffix is valid use that, otherwise none was provided and use the default passed return unit.convert(val, suffix != null ? timeSuffixes.get(suffix) : unit); } catch (NumberFormatException e) { String timeError = "Time must be specified as seconds (s), " + "milliseconds (ms), microseconds (us), minutes (m or min), hour (h), or day (d). " + "E.g. 50s, 100ms, or 250us."; throw new NumberFormatException(timeError + "\n" + e.getMessage()); } }
rc|static byte[] (final byte[] value, final byte[] key) throws AuthenticationException { try { final Cipher rc4 = Cipher.getInstance("RC4"); rc4.init(Cipher.ENCRYPT_MODE, new SecretKeySpec(key, "RC4")); return rc4.doFinal(value); } catch (final Exception e) { throw new AuthenticationException(e.getMessage(), e); } }
list paths|public static JavaRDD<String> (JavaSparkContext sc, String path, boolean recursive) throws IOException { //NativeImageLoader.ALLOWED_FORMATS return listPaths(sc, path, recursive, (Set<String>) null); }
discover|public List<WordInfo> (String doc, int size) { try { return discover(new BufferedReader(new StringReader(doc)), size); } catch (IOException e) { throw new RuntimeException(e); } }
validate for current mode|private static void (HttpMessage request) throws ApiException { if (!isValidForCurrentMode(request.getRequestHeader().getURI())) { throw new ApiException(ApiException.Type.MODE_VIOLATION); } }
set all labels|@SuppressWarnings("WeakerAccess") public UpdateInstanceRequest (@Nonnull Map<String, String> labels) { Preconditions.checkNotNull(labels, "labels can't be null"); builder.getInstanceBuilder().clearLabels(); builder.getInstanceBuilder().putAllLabels(labels); updateFieldMask(Instance.LABELS_FIELD_NUMBER); return this; }
write word vectors|@Deprecated public static void (@NonNull ParagraphVectors vectors, @NonNull File path) { try (FileOutputStream fos = new FileOutputStream(path)) { writeWordVectors(vectors, fos); } catch (Exception e) { throw new RuntimeException(e); } }
authority to uri|public static URI (String authority) { Preconditions.checkNotNull(authority, "authority"); URI uri; try { uri = new URI(null, authority, null, null, null); } catch (URISyntaxException ex) { throw new IllegalArgumentException("Invalid authority: " + authority, ex); } return uri; }
prepare default conf|private static String[] () throws IOException { final File templateFolder = new File("test/local-conf-templates"); final File localConfFolder = new File("local/conf"); if (!localConfFolder.exists()) { FileUtils.copyDirectory(templateFolder, localConfFolder.getParentFile()); log.info("Copied local conf templates from " + templateFolder.getAbsolutePath()); } log.info("Using conf at " + localConfFolder.getAbsolutePath()); return new String[] { "-conf", "local/conf" }; }
update finding|static Finding (FindingName findingName) { try (SecurityCenterClient client = SecurityCenterClient.create()) { // FindingName findingName = FindingName.of(/*organization=*/"123234324", // /*source=*/"423432321", /*findingId=*/"samplefindingid2"); // Use the current time as the finding "event time". Instant eventTime = Instant.now(); // Define source properties values as protobuf "Value" objects. Value stringValue = Value.newBuilder().setStringValue("value").build(); FieldMask updateMask = FieldMask.newBuilder().addPaths("event_time").addPaths("source_properties.stringKey").build(); Finding finding = Finding.newBuilder().setName(findingName.toString()).setEventTime(Timestamp.newBuilder().setSeconds(eventTime.getEpochSecond()).setNanos(eventTime.getNano())).putSourceProperties("stringKey", stringValue).build(); UpdateFindingRequest.Builder request = UpdateFindingRequest.newBuilder().setFinding(finding).setUpdateMask(updateMask); // Call the API. Finding response = client.updateFinding(request.build()); System.out.println("Updated Finding: " + response); return response; } catch (IOException e) { throw new RuntimeException("Couldn't create client.", e); } }
huber loss|public SDVariable (String name, @NonNull SDVariable label, @NonNull SDVariable predictions, SDVariable weights, @NonNull LossReduce lossReduce, double delta) { validateFloatingPoint("huber loss", "predictions", predictions); validateNumerical("huber loss", "labels", label); if (weights == null) weights = sd.scalar(null, predictions.dataType(), 1.0); SDVariable result = f().lossHuber(label, predictions, weights, lossReduce, delta); result = updateVariableNameAndReference(result, name); result.markAsLoss(); return result; }
to params|public static String (Map<String, Object> paramMap, String charsetName) { return toParams(paramMap, CharsetUtil.charset(charsetName)); }
ltrim|@Override public String (final byte[] key, final long start, final long stop) { checkIsInMultiOrPipeline(); client.ltrim(key, start, stop); return client.getStatusCodeReply(); }
handle|protected void (HttpConnection httpConnection) throws IOException { try { getServerThread().handleIncomingHttp(httpConnection); httpConnection.waitForResponse(); } catch (ConnectException ex) { httpConnection.respond(HttpStatus.GONE); } }
ip address to url string|public static String (InetAddress address) { if (address == null) { throw new NullPointerException("address is null"); } else if (address instanceof Inet4Address) { return address.getHostAddress(); } else if (address instanceof Inet6Address) { return getIPv6UrlRepresentation((Inet6Address) address); } else { throw new IllegalArgumentException("Unrecognized type of InetAddress: " + address); } }
scalar|public static Protos.Resource (String name, String role, double value) { checkNotNull(name); checkNotNull(role); checkNotNull(value); return Protos.Resource.newBuilder().setName(name).setType(Protos.Value.Type.SCALAR).setScalar(Protos.Value.Scalar.newBuilder().setValue(value)).setRole(role).build(); }
set|public HttpHeaders (CharSequence name, Object value) { return set(name.toString(), value); }
message decode plain|@Benchmark @BenchmarkMode(Mode.SampleTime) @OutputTimeUnit(TimeUnit.NANOSECONDS) public String () { return Status.MESSAGE_KEY.parseBytes("Unexpected RST in stream".getBytes(Charset.forName("US-ASCII"))); }
create ingestion schema|@VisibleForTesting static List<IndexIngestionSpec> (final TaskToolbox toolbox, final SegmentProvider segmentProvider, final PartitionConfigurationManager partitionConfigurationManager, @Nullable final DimensionsSpec dimensionsSpec, @Nullable final AggregatorFactory[] metricsSpec, @Nullable final Boolean keepSegmentGranularity, @Nullable final Granularity segmentGranularity, final ObjectMapper jsonMapper, final CoordinatorClient coordinatorClient, final SegmentLoaderFactory segmentLoaderFactory, final RetryPolicyFactory retryPolicyFactory) throws IOException, SegmentLoadingException { Pair<Map<DataSegment, File>, List<TimelineObjectHolder<String, DataSegment>>> pair = prepareSegments(toolbox, segmentProvider); final Map<DataSegment, File> segmentFileMap = pair.lhs; final List<TimelineObjectHolder<String, DataSegment>> timelineSegments = pair.rhs; if (timelineSegments.size() == 0) { return Collections.emptyList(); } // find metadata for interval final List<Pair<QueryableIndex, DataSegment>> queryableIndexAndSegments = loadSegments(timelineSegments, segmentFileMap, toolbox.getIndexIO()); final IndexTuningConfig compactionTuningConfig = partitionConfigurationManager.computeTuningConfig(queryableIndexAndSegments); if (segmentGranularity == null) { if (keepSegmentGranularity != null && !keepSegmentGranularity) { // all granularity final DataSchema dataSchema = createDataSchema(segmentProvider.dataSource, segmentProvider.interval, queryableIndexAndSegments, dimensionsSpec, metricsSpec, Granularities.ALL, jsonMapper); return Collections.singletonList(new IndexIngestionSpec(dataSchema, createIoConfig(toolbox, dataSchema, segmentProvider.interval, coordinatorClient, segmentLoaderFactory, retryPolicyFactory), compactionTuningConfig)); } else { // original granularity final Map<Interval, List<Pair<QueryableIndex, DataSegment>>> intervalToSegments = new TreeMap<>(Comparators.intervalsByStartThenEnd()); //noinspection ConstantConditions queryableIndexAndSegments.forEach( p -> intervalToSegments.computeIfAbsent(p.rhs.getInterval(), k -> new ArrayList<>()).add(p)); final List<IndexIngestionSpec> specs = new ArrayList<>(intervalToSegments.size()); for (Entry<Interval, List<Pair<QueryableIndex, DataSegment>>> entry : intervalToSegments.entrySet()) { final Interval interval = entry.getKey(); final List<Pair<QueryableIndex, DataSegment>> segmentsToCompact = entry.getValue(); final DataSchema dataSchema = createDataSchema(segmentProvider.dataSource, interval, segmentsToCompact, dimensionsSpec, metricsSpec, GranularityType.fromPeriod(interval.toPeriod()).getDefaultGranularity(), jsonMapper); specs.add(new IndexIngestionSpec(dataSchema, createIoConfig(toolbox, dataSchema, interval, coordinatorClient, segmentLoaderFactory, retryPolicyFactory), compactionTuningConfig)); } return specs; } } else { if (keepSegmentGranularity != null && keepSegmentGranularity) { // error throw new ISE("segmentGranularity[%s] and keepSegmentGranularity can't be used together", segmentGranularity); } else { // given segment granularity final DataSchema dataSchema = createDataSchema(segmentProvider.dataSource, segmentProvider.interval, queryableIndexAndSegments, dimensionsSpec, metricsSpec, segmentGranularity, jsonMapper); return Collections.singletonList(new IndexIngestionSpec(dataSchema, createIoConfig(toolbox, dataSchema, segmentProvider.interval, coordinatorClient, segmentLoaderFactory, retryPolicyFactory), compactionTuningConfig)); } } }
create|public static StorageLevel (boolean useDisk, boolean useMemory, boolean useOffHeap, boolean deserialized, int replication) { return StorageLevel.apply(useDisk, useMemory, useOffHeap, deserialized, replication); }
sample oob rows|public static int[] (int nrows, float rate, Random sampler, int[] oob) { // Number of oob rows int oobcnt = 0; Arrays.fill(oob, 0); for (int row = 0; row < nrows; row++) { if (sampler.nextFloat() >= rate) { // it is out-of-bag row oob[1 + oobcnt++] = row; if (1 + oobcnt >= oob.length) oob = Arrays.copyOf(oob, Math.round(1.2f * nrows + 0.5f) + 2); } } oob[0] = oobcnt; return oob; }
split by regex|public static List<String> (String str, String separatorRegex, int limit, boolean isTrim, boolean ignoreEmpty) { final Pattern pattern = PatternPool.get(separatorRegex); return split(str, pattern, limit, isTrim, ignoreEmpty); }
epoll busy wait|public static int (FileDescriptor epollFd, EpollEventArray events) throws IOException { int ready = epollBusyWait0(epollFd.intValue(), events.memoryAddress(), events.length()); if (ready < 0) { throw newIOException("epoll_wait", ready); } return ready; }
create h o compatible gson|public static Gson () { return new GsonBuilder().registerTypeAdapter(KeyV3.class, new KeySerializer()).registerTypeAdapter(FrameV3.ColSpecifierV3.class, new ColSerializer()).create(); }
to bean|public static <T> T (String jsonString, Type beanType, boolean ignoreError) { return toBean(parseObj(jsonString), beanType, ignoreError); }
list log entries|public final ListLogEntriesPagedResponse (List<String> resourceNames, String filter, String orderBy) { ListLogEntriesRequest request = ListLogEntriesRequest.newBuilder().addAllResourceNames(resourceNames).setFilter(filter).setOrderBy(orderBy).build(); return listLogEntries(request); }
is registerable|public boolean (InstanceInfo instanceInfo) { DataCenterInfo datacenterInfo = instanceInfo.getDataCenterInfo(); String serverRegion = clientConfig.getRegion(); if (AmazonInfo.class.isInstance(datacenterInfo)) { AmazonInfo info = AmazonInfo.class.cast(instanceInfo.getDataCenterInfo()); String availabilityZone = info.get(MetaDataKey.availabilityZone); // Can be null for dev environments in non-AWS data center if (availabilityZone == null && US_EAST_1.equalsIgnoreCase(serverRegion)) { return true; } else if ((availabilityZone != null) && (availabilityZone.contains(serverRegion))) { // If in the same region as server, then consider it registerable return true; } } // Everything non-amazon is registrable. return true; }
sort by label|@Override public void () { Map<Integer, Queue<DataSet>> map = new HashMap<>(); List<DataSet> data = asList(); int numLabels = numOutcomes(); int examples = numExamples(); for (DataSet d : data) { int label = d.outcome(); Queue<DataSet> q = map.get(label); if (q == null) { q = new ArrayDeque<>(); map.put(label, q); } q.add(d); } for (Map.Entry<Integer, Queue<DataSet>> label : map.entrySet()) { log.info("Label " + label + " has " + label.getValue().size() + " elements"); } //ideal input splits: 1 of each label in each batch //after we run out of ideal batches: fall back to a new strategy boolean optimal = true; for (int i = 0; i < examples; i++) { if (optimal) { for (int j = 0; j < numLabels; j++) { Queue<DataSet> q = map.get(j); if (q == null) { optimal = false; break; } DataSet next = q.poll(); //add a row; go to next if (next != null) { addRow(next, i); i++; } else { optimal = false; break; } } } else { DataSet add = null; for (Queue<DataSet> q : map.values()) { if (!q.isEmpty()) { add = q.poll(); break; } } addRow(add, i); } } }
key equals|private boolean (ByteBuffer curKeyBuffer, ByteBuffer buffer, int bufferOffset) { // Since this method is frequently called per each input row, the compare performance matters. int i = 0; for (; i + Long.BYTES <= keySize; i += Long.BYTES) { if (curKeyBuffer.getLong(i) != buffer.getLong(bufferOffset + i)) { return false; } } if (i + Integer.BYTES <= keySize) { // This can be called at most once because we already compared using getLong() in the above. if (curKeyBuffer.getInt(i) != buffer.getInt(bufferOffset + i)) { return false; } i += Integer.BYTES; } for (; i < keySize; i++) { if (curKeyBuffer.get(i) != buffer.get(bufferOffset + i)) { return false; } } return true; }
parse string|public static BigDecimal (String numberStr, String pattern) throws ParseException { DecimalFormat df = null; if (StringUtils.isEmpty(pattern)) { df = PRETTY_FORMAT.get(); } else { df = (DecimalFormat) DecimalFormat.getInstance(); df.applyPattern(pattern); } return new BigDecimal(df.parse(numberStr).doubleValue()); }
remove|@SuppressWarnings("unchecked") public final void (InternalThreadLocalMap threadLocalMap) { if (threadLocalMap == null) { return; } Object v = threadLocalMap.removeIndexedVariable(index); removeFromVariablesToRemove(threadLocalMap, this); if (v != InternalThreadLocalMap.UNSET) { try { onRemoval((V) v); } catch (Exception e) { PlatformDependent.throwException(e); } } }
load|public boolean (String path) { String binPath = path + Predefine.BIN_EXT; if (load(ByteArrayStream.createByteArrayStream(binPath))) return true; if (!loadTxt(path)) return false; try { logger.info("正在缓存" + binPath); DataOutputStream out = new DataOutputStream(IOUtil.newOutputStream(binPath)); save(out); out.close(); } catch (Exception e) { logger.warning("缓存" + binPath + "失败：\n" + TextUtility.exceptionToString(e)); } return true; }
get db storage paths|public String[] () { if (localRocksDbDirectories == null) { return null; } else { String[] paths = new String[localRocksDbDirectories.length]; for (int i = 0; i < paths.length; i++) { paths[i] = localRocksDbDirectories[i].toString(); } return paths; } }
sort by specificity and quality|public static void (List<MediaType> mediaTypes) { Assert.notNull(mediaTypes, "'mediaTypes' must not be null"); if (mediaTypes.size() > 1) { Collections.sort(mediaTypes, new CompoundComparator<MediaType>(MediaType.SPECIFICITY_COMPARATOR, MediaType.QUALITY_VALUE_COMPARATOR)); } }
set error|@Override public void (String message, int valueIdx) { if (valueIdx < 0) node.addError(message); else node.addError(message, positions.get(valueIdx)); }
get field name|public Optional<String> (int fieldIndex) { if (fieldIndex < 0 || fieldIndex >= fieldNames.length) { return Optional.empty(); } return Optional.of(fieldNames[fieldIndex]); }
next|public static void (HttpServerExchange httpServerExchange, String execName, Boolean returnToOrigFlow) throws Exception { String currentChainId = httpServerExchange.getAttachment(CHAIN_ID); Integer currentNextIndex = httpServerExchange.getAttachment(CHAIN_SEQ); httpServerExchange.putAttachment(CHAIN_ID, execName); httpServerExchange.putAttachment(CHAIN_SEQ, 0); next(httpServerExchange); // return to current flow. if (returnToOrigFlow) { httpServerExchange.putAttachment(CHAIN_ID, currentChainId); httpServerExchange.putAttachment(CHAIN_SEQ, currentNextIndex); next(httpServerExchange); } }
in order analyse|private int (StructuralNode node) { int subtreeNodes = 0; if (isStop) { return 0; } if (node == null) { return 0; } // Leaf is not analysed because only folder entity is used to determine if path exist. try { if (!node.isRoot()) { if (!node.isLeaf() || node.isLeaf() && node.getParent().isRoot()) { analyse(node); } else { //ZAP: it's a Leaf then no children are available return 1; } } } catch (Exception e) { } Iterator<StructuralNode> iter = node.getChildIterator(); while (iter.hasNext()) { subtreeNodes += inOrderAnalyse(iter.next()); } return subtreeNodes + 1; }
get method by name|public static Method (Class<?> clazz, String methodName) throws SecurityException { return getMethodByName(clazz, false, methodName); }
release waiters|private void (int phase) { // first element of queue QNode q; // its thread Thread t; AtomicReference<QNode> head = (phase & 1) == 0 ? evenQ : oddQ; while ((q = head.get()) != null && q.phase != (int) (root.state >>> PHASE_SHIFT)) { if (head.compareAndSet(q, q.next) && (t = q.thread) != null) { q.thread = null; LockSupport.unpark(t); } } }
get word vector|public double[] (String word) { INDArray r = getWordVectorMatrix(word); if (r == null) return null; return r.dup().data().asDouble(); }
get number of labels used|public int () { if (labels != null && !labels.isEmpty()) return labels.size(); else return ((Long) (maxCount + 1)).intValue(); }
get first|public static <T> T (Collection<T> collection) { if (isEmpty(collection)) { return null; } if (collection instanceof List) { return ((List<T>) collection).get(0); } return collection.iterator().next(); }
invoke|@Override public void () throws Exception { // -------------------------------------------------------------------- if (LOG.isDebugEnabled()) { LOG.debug(formatLogString("Start registering input and output.")); } // obtain task configuration (including stub parameters) Configuration taskConf = getTaskConfiguration(); this.config = new TaskConfig(taskConf); // now get the operator class which drives the operation final Class<? extends Driver<S, OT>> driverClass = this.config.getDriver(); this.driver = InstantiationUtil.instantiate(driverClass, Driver.class); String headName = getEnvironment().getTaskInfo().getTaskName().split("->")[0].trim(); this.metrics = getEnvironment().getMetricGroup().getOrAddOperator(headName.startsWith("CHAIN") ? headName.substring(6) : headName); this.metrics.getIOMetricGroup().reuseInputMetricsForTask(); if (config.getNumberOfChainedStubs() == 0) { this.metrics.getIOMetricGroup().reuseOutputMetricsForTask(); } // initialize the readers. // this does not yet trigger any stream consuming or processing. initInputReaders(); initBroadcastInputReaders(); // initialize the writers. initOutputs(); if (LOG.isDebugEnabled()) { LOG.debug(formatLogString("Finished registering input and output.")); } // -------------------------------------------------------------------- if (LOG.isDebugEnabled()) { LOG.debug(formatLogString("Start task code.")); } this.runtimeUdfContext = createRuntimeContext(metrics); // this is especially important, since there may be asynchronous closes (such as through canceling). try { // the local processing includes building the dams / caches try { int numInputs = driver.getNumberOfInputs(); int numComparators = driver.getNumberOfDriverComparators(); int numBroadcastInputs = this.config.getNumBroadcastInputs(); initInputsSerializersAndComparators(numInputs, numComparators); initBroadcastInputsSerializers(numBroadcastInputs); // set the iterative status for inputs and broadcast inputs { List<Integer> iterativeInputs = new ArrayList<Integer>(); for (int i = 0; i < numInputs; i++) { final int numberOfEventsUntilInterrupt = getTaskConfig().getNumberOfEventsUntilInterruptInIterativeGate(i); if (numberOfEventsUntilInterrupt < 0) { throw new IllegalArgumentException(); } else if (numberOfEventsUntilInterrupt > 0) { this.inputReaders[i].setIterativeReader(); iterativeInputs.add(i); if (LOG.isDebugEnabled()) { LOG.debug(formatLogString("Input [" + i + "] reads in supersteps with [" + +numberOfEventsUntilInterrupt + "] event(s) till next superstep.")); } } } this.iterativeInputs = asArray(iterativeInputs); } { List<Integer> iterativeBcInputs = new ArrayList<Integer>(); for (int i = 0; i < numBroadcastInputs; i++) { final int numberOfEventsUntilInterrupt = getTaskConfig().getNumberOfEventsUntilInterruptInIterativeBroadcastGate(i); if (numberOfEventsUntilInterrupt < 0) { throw new IllegalArgumentException(); } else if (numberOfEventsUntilInterrupt > 0) { this.broadcastInputReaders[i].setIterativeReader(); iterativeBcInputs.add(i); if (LOG.isDebugEnabled()) { LOG.debug(formatLogString("Broadcast input [" + i + "] reads in supersteps with [" + +numberOfEventsUntilInterrupt + "] event(s) till next superstep.")); } } } this.iterativeBroadcastInputs = asArray(iterativeBcInputs); } initLocalStrategies(numInputs); } catch (Exception e) { throw new RuntimeException("Initializing the input processing failed" + (e.getMessage() == null ? "." : ": " + e.getMessage()), e); } if (!this.running) { if (LOG.isDebugEnabled()) { LOG.debug(formatLogString("Task cancelled before task code was started.")); } return; } // pre main-function initialization initialize(); // read the broadcast variables. they will be released in the finally clause for (int i = 0; i < this.config.getNumBroadcastInputs(); i++) { final String name = this.config.getBroadcastInputName(i); readAndSetBroadcastInput(i, name, this.runtimeUdfContext, 1); } // the work goes here run(); } finally { // clean up in any case! closeLocalStrategiesAndCaches(); clearReaders(inputReaders); clearWriters(eventualOutputs); } if (this.running) { if (LOG.isDebugEnabled()) { LOG.debug(formatLogString("Finished task code.")); } } else { if (LOG.isDebugEnabled()) { LOG.debug(formatLogString("Task code cancelled.")); } } }
set document locator|@Override public void (Locator locator) { this.loc = locator; }
process|public CAS (String text) { CAS cas = retrieve(); cas.setDocumentText(text); try { analysisEngine.process(cas); } catch (AnalysisEngineProcessException e) { if (text != null && !text.isEmpty()) return process(text); throw new RuntimeException(e); } return cas; }
get v int size|public static int (long i) { if (i >= -112 && i <= 127) { return 1; } if (i < 0) { // take one's complement' i ^= -1L; } // find the number of bytes with non-leading zeros int dataBits = Long.SIZE - Long.numberOfLeadingZeros(i); // find the number of data bytes + length byte return (dataBits + 7) / 8 + 1; }
get finding|public final Finding (FindingName name) { GetFindingRequest request = GetFindingRequest.newBuilder().setName(name == null ? null : name.toString()).build(); return getFinding(request); }
materialize type variable|private static Type (ArrayList<Type> typeHierarchy, TypeVariable<?> typeVar) { TypeVariable<?> inTypeTypeVar = typeVar; // iterate thru hierarchy from top to bottom until type variable gets a class assigned for (int i = typeHierarchy.size() - 1; i >= 0; i--) { Type curT = typeHierarchy.get(i); // parameterized type if (curT instanceof ParameterizedType) { Class<?> rawType = ((Class<?>) ((ParameterizedType) curT).getRawType()); for (int paramIndex = 0; paramIndex < rawType.getTypeParameters().length; paramIndex++) { TypeVariable<?> curVarOfCurT = rawType.getTypeParameters()[paramIndex]; // check if variable names match if (sameTypeVars(curVarOfCurT, inTypeTypeVar)) { Type curVarType = ((ParameterizedType) curT).getActualTypeArguments()[paramIndex]; // another type variable level if (curVarType instanceof TypeVariable<?>) { inTypeTypeVar = (TypeVariable<?>) curVarType; } else // class { return curVarType; } } } } } // return the type variable of the deepest level return inTypeTypeVar; }
read header|Header (@NonNull InputStream input) throws IOException { byte version = (byte) input.read(); int keyLength = BitConverter.readInt(input); int valueLength = BitConverter.readInt(input); long entryVersion = BitConverter.readLong(input); validateHeader(keyLength, valueLength); return new Header(version, keyLength, valueLength, entryVersion); }
new builder|public static void (String projectId, String topicId) throws Exception { ProjectTopicName topic = ProjectTopicName.of(projectId, topicId); Publisher publisher = Publisher.newBuilder(topic).build(); try { // ... } finally { // When finished with the publisher, make sure to shutdown to free up resources. publisher.shutdown(); publisher.awaitTermination(1, TimeUnit.MINUTES); } }
ping txn|public CompletableFuture<PingTxnStatus> (final String scope, final String stream, final UUID txId, final long lease, final OperationContext contextOpt) { final OperationContext context = getNonNullOperationContext(scope, stream, contextOpt); return pingTxnBody(scope, stream, txId, lease, context); }
send metrics|@Scheduled(fixedRateString = "${hystrix.stream.queue.sendRate:${hystrix.stream.queue.send-rate:500}}") public void () { ArrayList<String> metrics = new ArrayList<>(); this.jsonMetrics.drainTo(metrics); if (!metrics.isEmpty()) { if (log.isTraceEnabled()) { log.trace("sending stream metrics size: " + metrics.size()); } for (String json : metrics) { // TODO: batch all metrics to one message try { // TODO: remove the explicit content type when s-c-stream can handle // that for us this.outboundChannel.send(MessageBuilder.withPayload(json).setHeader(MessageHeaders.CONTENT_TYPE, this.properties.getContentType()).build()); } catch (Exception ex) { if (log.isTraceEnabled()) { log.trace("failed sending stream metrics: " + ex.getMessage()); } } } } }
tls|public static InternalProtocolNegotiator.ProtocolNegotiator (SslContext sslContext) { final io.grpc.netty.ProtocolNegotiator negotiator = ProtocolNegotiators.tls(sslContext); final class TlsNegotiator implements InternalProtocolNegotiator.ProtocolNegotiator { @Override public AsciiString scheme() { return negotiator.scheme(); } @Override public ChannelHandler newHandler(GrpcHttp2ConnectionHandler grpcHandler) { return negotiator.newHandler(grpcHandler); } @Override public void close() { negotiator.close(); } } return new TlsNegotiator(); }
require state|static void (boolean expression, String template, @Nullable Object... args) { if (!expression) { throw new IllegalStateException(String.format(template, args)); } }
render|@SneakyThrows public static void (final HttpServletResponse response) { val map = new HashMap<String, Object>(); response.setStatus(HttpServletResponse.SC_OK); render(map, response); }
restore savepoint|public boolean (String savepointPointer, boolean allowNonRestored, Map<JobVertexID, ExecutionJobVertex> tasks, ClassLoader userClassLoader) throws Exception { Preconditions.checkNotNull(savepointPointer, "The savepoint path cannot be null."); LOG.info("Starting job {} from savepoint {} ({})", job, savepointPointer, (allowNonRestored ? "allowing non restored state" : "")); final CompletedCheckpointStorageLocation checkpointLocation = checkpointStorage.resolveCheckpoint(savepointPointer); // Load the savepoint as a checkpoint into the system CompletedCheckpoint savepoint = Checkpoints.loadAndValidateCheckpoint(job, tasks, checkpointLocation, userClassLoader, allowNonRestored); completedCheckpointStore.addCheckpoint(savepoint); // Reset the checkpoint ID counter long nextCheckpointId = savepoint.getCheckpointID() + 1; checkpointIdCounter.setCount(nextCheckpointId); LOG.info("Reset the checkpoint ID of job {} to {}.", job, nextCheckpointId); return restoreLatestCheckpointedState(tasks, true, allowNonRestored); }
is valid inet address|public boolean (String inet6Address) { boolean containsCompressedZeroes = inet6Address.contains("::"); if (containsCompressedZeroes && (inet6Address.indexOf("::") != inet6Address.lastIndexOf("::"))) { return false; } if ((inet6Address.startsWith(":") && !inet6Address.startsWith("::")) || (inet6Address.endsWith(":") && !inet6Address.endsWith("::"))) { return false; } String[] octets = inet6Address.split(":"); if (containsCompressedZeroes) { List<String> octetList = new ArrayList<>(Arrays.asList(octets)); if (inet6Address.endsWith("::")) { // String.split() drops ending empty segments octetList.add(""); } else if (inet6Address.startsWith("::") && !octetList.isEmpty()) { octetList.remove(0); } octets = octetList.toArray(new String[0]); } if (octets.length > IPV6_MAX_HEX_GROUPS) { return false; } int validOctets = 0; // consecutive empty chunks int emptyOctets = 0; for (int index = 0; index < octets.length; index++) { String octet = octets[index]; if (octet.length() == 0) { emptyOctets++; if (emptyOctets > 1) { return false; } } else { emptyOctets = 0; // Is last chunk an IPv4 address? if (index == octets.length - 1 && octet.contains(".")) { if (!isValidInet4Address(octet)) { return false; } validOctets += 2; continue; } if (octet.length() > IPV6_MAX_HEX_DIGITS_PER_GROUP) { return false; } int octetInt = 0; try { octetInt = Integer.parseInt(octet, BASE_16); } catch (NumberFormatException e) { return false; } if (octetInt < 0 || octetInt > MAX_UNSIGNED_SHORT) { return false; } } validOctets++; } if (validOctets > IPV6_MAX_HEX_GROUPS || (validOctets < IPV6_MAX_HEX_GROUPS && !containsCompressedZeroes)) { return false; } return true; }
initialize state|@Override public void (FunctionInitializationContext context) throws Exception { final int subtaskIndex = getRuntimeContext().getIndexOfThisSubtask(); this.buckets = bucketsBuilder.createBuckets(subtaskIndex); final OperatorStateStore stateStore = context.getOperatorStateStore(); bucketStates = stateStore.getListState(BUCKET_STATE_DESC); maxPartCountersState = stateStore.getUnionListState(MAX_PART_COUNTER_STATE_DESC); if (context.isRestored()) { buckets.initializeState(bucketStates, maxPartCountersState); } }
await|public static <T> boolean (CompletableFuture<T> f, long timeout) { Exceptions.handleInterrupted(() -> { try { f.get(timeout, TimeUnit.MILLISECONDS); } catch (TimeoutException | ExecutionException e) { } }); return isSuccessful(f); }
from key value block|public static MapBlock (Optional<boolean[]> mapIsNull, int[] offsets, Block keyBlock, Block valueBlock, MapType mapType, MethodHandle keyBlockNativeEquals, MethodHandle keyNativeHashCode, MethodHandle keyBlockHashCode) { validateConstructorArguments(0, offsets.length - 1, mapIsNull.orElse(null), offsets, keyBlock, valueBlock, mapType.getKeyType(), keyBlockNativeEquals, keyNativeHashCode); int mapCount = offsets.length - 1; return createMapBlockInternal(0, mapCount, mapIsNull, offsets, keyBlock, valueBlock, new HashTables(Optional.empty(), keyBlock.getPositionCount() * HASH_MULTIPLIER), mapType.getKeyType(), keyBlockNativeEquals, keyNativeHashCode, keyBlockHashCode); }
add|public void (E element) throws IllegalStateException { requireNonNull(element); lock.lock(); try { if (open) { elements.addLast(element); if (elements.size() == 1) { nonEmpty.signalAll(); } } else { throw new IllegalStateException("queue is closed"); } } finally { lock.unlock(); } }
get partition by id|public IntermediateResultPartition (IntermediateResultPartitionID resultPartitionId) { // Looks ups the partition number via the helper map and returns the // partition. Currently, this happens infrequently enough that we could // consider removing the map and scanning the partitions on every lookup. // The lookup (currently) only happen when the producer of an intermediate // result cannot be found via its registered execution. Integer partitionNumber = partitionLookupHelper.get(checkNotNull(resultPartitionId, "IntermediateResultPartitionID")); if (partitionNumber != null) { return partitions[partitionNumber]; } else { throw new IllegalArgumentException("Unknown intermediate result partition ID " + resultPartitionId); } }
get module roots|public FilePath[] (FilePath workspace, AbstractBuild build) { if (Util.isOverridden(SCM.class, getClass(), "getModuleRoots", FilePath.class)) // if the subtype derives legacy getModuleRoots(FilePath), delegate to it return getModuleRoots(workspace); // otherwise the default implementation return new FilePath[] { getModuleRoot(workspace, build) }; }
schema|private static Schema (int version, String type) { Class<? extends Schema> clz = schemaClass(version, type); if (clz == null) clz = schemaClass(EXPERIMENTAL_VERSION, type); if (clz == null) throw new H2ONotFoundArgumentException("Failed to find schema for version: " + version + " and type: " + type, "Failed to find schema for version: " + version + " and type: " + type + "\n" + "Did you forget to add an entry into META-INF/services/water.api.Schema?"); return Schema.newInstance(clz); }
join|public static String (long[] array, CharSequence conjunction) { if (null == array) { return null; } final StringBuilder sb = new StringBuilder(); boolean isFirst = true; for (long item : array) { if (isFirst) { isFirst = false; } else { sb.append(conjunction); } sb.append(item); } return sb.toString(); }
atom segment|private static List<AtomNode> (String sSentence, int start, int end) { if (end < start) { throw new RuntimeException("start=" + start + " < end=" + end); } List<AtomNode> atomSegment = new ArrayList<AtomNode>(); int pCur = 0, nCurType, nNextType; StringBuilder sb = new StringBuilder(); char c; char[] charArray = sSentence.substring(start, end).toCharArray(); int[] charTypeArray = new int[charArray.length]; // 生成对应单个汉字的字符类型数组 for (int i = 0; i < charArray.length; ++i) { c = charArray[i]; charTypeArray[i] = CharType.get(c); if (c == '.' && i < (charArray.length - 1) && CharType.get(charArray[i + 1]) == CharType.CT_NUM) charTypeArray[i] = CharType.CT_NUM; else if (c == '.' && i < (charArray.length - 1) && charArray[i + 1] >= '0' && charArray[i + 1] <= '9') charTypeArray[i] = CharType.CT_SINGLE; else if (charTypeArray[i] == CharType.CT_LETTER) charTypeArray[i] = CharType.CT_SINGLE; } // 根据字符类型数组中的内容完成原子切割 while (pCur < charArray.length) { nCurType = charTypeArray[pCur]; if (nCurType == CharType.CT_CHINESE || nCurType == CharType.CT_INDEX || nCurType == CharType.CT_DELIMITER || nCurType == CharType.CT_OTHER) { String single = String.valueOf(charArray[pCur]); if (single.length() != 0) atomSegment.add(new AtomNode(single, nCurType)); pCur++; } else //如果是字符、数字或者后面跟随了数字的小数点“.”则一直取下去。 if (pCur < charArray.length - 1 && ((nCurType == CharType.CT_SINGLE) || nCurType == CharType.CT_NUM)) { sb.delete(0, sb.length()); sb.append(charArray[pCur]); boolean reachEnd = true; while (pCur < charArray.length - 1) { nNextType = charTypeArray[++pCur]; if (nNextType == nCurType) sb.append(charArray[pCur]); else { reachEnd = false; break; } } atomSegment.add(new AtomNode(sb.toString(), nCurType)); if (reachEnd) pCur++; } else // 对于所有其它情况 { atomSegment.add(new AtomNode(charArray[pCur], nCurType)); pCur++; } } // logger.trace("原子分词:" + atomSegment); return atomSegment; }
generate deterministic hash|private byte[] (StreamNode node, Hasher hasher, Map<Integer, byte[]> hashes, boolean isChainingEnabled, StreamGraph streamGraph) { generateNodeLocalHash(hasher, hashes.size()); for (StreamEdge outEdge : node.getOutEdges()) { if (isChainable(outEdge, isChainingEnabled, streamGraph)) { generateNodeLocalHash(hasher, hashes.size()); } } byte[] hash = hasher.hash().asBytes(); // this loop (calling this method). for (StreamEdge inEdge : node.getInEdges()) { byte[] otherHash = hashes.get(inEdge.getSourceId()); // Sanity check if (otherHash == null) { throw new IllegalStateException("Missing hash for input node " + streamGraph.getSourceVertex(inEdge) + ". Cannot generate hash for " + node + "."); } for (int j = 0; j < hash.length; j++) { hash[j] = (byte) (hash[j] * 37 ^ otherHash[j]); } } if (LOG.isDebugEnabled()) { String udfClassName = ""; if (node.getOperator() instanceof AbstractUdfStreamOperator) { udfClassName = ((AbstractUdfStreamOperator<?, ?>) node.getOperator()).getUserFunction().getClass().getName(); } LOG.debug("Generated hash '" + byteToHexString(hash) + "' for node " + "'" + node.toString() + "' {id: " + node.getId() + ", " + "parallelism: " + node.getParallelism() + ", " + "user function: " + udfClassName + "}"); } return hash; }
print available image|public static void (final Image image) { final Subscription subscription = image.subscription(); System.out.println(String.format("Available image on %s streamId=%d sessionId=%d from %s", subscription.channel(), subscription.streamId(), image.sessionId(), image.sourceIdentity())); }
execute|public static HttpResponse (final String url, final String method, final String basicAuthUsername, final String basicAuthPassword, final Map<String, Object> headers) { return execute(url, method, basicAuthUsername, basicAuthPassword, new HashMap<>(), headers); }
scheme|@Override public AsciiString () { return negotiator.scheme(); }
new handler|@Override public ChannelHandler (GrpcHttp2ConnectionHandler grpcHandler) { return negotiator.newHandler(grpcHandler); }
close|@Override public void () { negotiator.close(); }
set storage type|@SuppressWarnings("WeakerAccess") public CreateClusterRequest (@Nonnull StorageType storageType) { Preconditions.checkNotNull(storageType); Preconditions.checkArgument(storageType != StorageType.UNRECOGNIZED, "StorageType can't be UNRECOGNIZED"); proto.getClusterBuilder().setDefaultStorageType(storageType.toProto()); return this; }
internal new dialect|private static Dialect (String driverName) { if (StrUtil.isNotBlank(driverName)) { if (DRIVER_MYSQL.equalsIgnoreCase(driverName) || DRIVER_MYSQL_V6.equalsIgnoreCase(driverName)) { return new MysqlDialect(); } else if (DRIVER_ORACLE.equalsIgnoreCase(driverName) || DRIVER_ORACLE_OLD.equalsIgnoreCase(driverName)) { return new OracleDialect(); } else if (DRIVER_SQLLITE3.equalsIgnoreCase(driverName)) { return new Sqlite3Dialect(); } else if (DRIVER_POSTGRESQL.equalsIgnoreCase(driverName)) { return new PostgresqlDialect(); } else if (DRIVER_H2.equalsIgnoreCase(driverName)) { return new H2Dialect(); } else if (DRIVER_SQLSERVER.equalsIgnoreCase(driverName)) { return new SqlServer2012Dialect(); } } // 无法识别可支持的数据库类型默认使用ANSI方言，可兼容大部分SQL语句 return new AnsiSqlDialect(); }
get schemas|@Override public OperationHandle (SessionHandle sessionHandle, String catalogName, String schemaName) throws HiveSQLException { return cliService.getSchemas(sessionHandle, catalogName, schemaName); }
mark executed|public void (@NonNull String nodeName, boolean executed) { states.get(nodeName).setExecuted(executed); }
scalar|@Override public INDArray (float value) { if (Nd4j.dataType() == DataType.FLOAT || Nd4j.dataType() == DataType.HALF) return create(new float[] { value }, new int[0], new int[0], 0); else if (Nd4j.dataType() == DataType.DOUBLE) return scalar((double) value); else return scalar((int) value); }
append|public void (byte[] utf8, int start, int len) { setCapacity(length + len, true); System.arraycopy(utf8, start, bytes, length, len); length += len; }
make collector|public static HyperLogLogCollector (ByteBuffer buffer) { int remaining = buffer.remaining(); if (remaining % 3 == 0 || remaining == 1027) { return new VersionZeroHyperLogLogCollector(buffer); } else { return new VersionOneHyperLogLogCollector(buffer); } }
get java compiler version|public static int () { int cv = compilerVersion.get(); if (cv != -1) return cv; /* Main algorithm: Use JavaCompiler's intended method to do this */ { Matcher m = VERSION_PARSER.matcher(JavaCompiler.version()); if (m.matches()) { int major = Integer.parseInt(m.group(1)); if (major == 1) { int minor = Integer.parseInt(m.group(2)); return setVersion(minor); } if (major >= 9) return setVersion(major); } } /* Fallback algorithm one: Check Source's values. Lets hope oracle never releases a javac that recognizes future versions for -source */ { String name = Source.values()[Source.values().length - 1].name(); Matcher m = SOURCE_PARSER.matcher(name); if (m.matches()) { int major = Integer.parseInt(m.group(1)); if (major == 1) { int minor = Integer.parseInt(m.group(2)); return setVersion(minor); } if (major >= 9) return setVersion(major); } } return setVersion(6); }
write to stream|public static void (BitMatrix matrix, String format, OutputStream stream) throws IOException { writeToStream(matrix, format, stream, DEFAULT_CONFIG); }
start document|@Override public void () throws SAXException { attempt(); }
process orphan commits|private void (GitHubRepo repo) { long refTime = Math.min(System.currentTimeMillis() - gitHubSettings.getCommitPullSyncTime(), gitHubClient.getRepoOffsetTime(repo)); List<Commit> orphanCommits = commitRepository.findCommitsByCollectorItemIdAndTimestampAfterAndPullNumberIsNull(repo.getId(), refTime); List<GitRequest> pulls = gitRequestRepository.findByCollectorItemIdAndMergedAtIsBetween(repo.getId(), refTime, System.currentTimeMillis()); orphanCommits = CommitPullMatcher.matchCommitToPulls(orphanCommits, pulls); List<Commit> orphanSaveList = orphanCommits.stream().filter( c -> !StringUtils.isEmpty(c.getPullNumber())).collect(Collectors.toList()); orphanSaveList.forEach( c -> LOG.info("Updating orphan " + c.getScmRevisionNumber() + " " + new DateTime(c.getScmCommitTimestamp()).toString("yyyy-MM-dd hh:mm:ss.SSa") + " with pull " + c.getPullNumber())); commitRepository.save(orphanSaveList); }
get or create row|public static Row (Sheet sheet, int rowIndex) { Row row = sheet.getRow(rowIndex); if (null == row) { row = sheet.createRow(rowIndex); } return row; }
read lines|public <T extends Collection<String>> T (T collection) throws IORuntimeException { BufferedReader reader = null; try { reader = FileUtil.getReader(file, charset); String line; while (true) { line = reader.readLine(); if (line == null) { break; } collection.add(line); } return collection; } catch (IOException e) { throw new IORuntimeException(e); } finally { IoUtil.close(reader); } }
get integer|public static Integer (String propertyName, String envName, Integer defaultValue) { checkEnvName(envName); Integer propertyValue = NumberUtil.toIntObject(System.getProperty(propertyName), null); if (propertyValue != null) { return propertyValue; } else { propertyValue = NumberUtil.toIntObject(System.getenv(envName), null); return propertyValue != null ? propertyValue : defaultValue; } }
headers|static List<HpackHeader> (HpackHeadersSize size, boolean limitToAscii) { return headersMap.get(new HeadersKey(size, limitToAscii)); }
append|public ConfigurationPropertyName (String elementValue) { if (elementValue == null) { return this; } Elements additionalElements = probablySingleElementOf(elementValue); return new ConfigurationPropertyName(this.elements.append(additionalElements)); }
get param value|protected static Object (MetaObject paramsObject, String paramName, boolean required) { Object value = null; if (paramsObject.hasGetter(PARAMS.get(paramName))) { value = paramsObject.getValue(PARAMS.get(paramName)); } if (value != null && value.getClass().isArray()) { Object[] values = (Object[]) value; if (values.length == 0) { value = null; } else { value = values[0]; } } if (required && value == null) { throw new PageException("分页查询缺少必要的参数:" + PARAMS.get(paramName)); } return value; }
set warning|@Override public void (String message, int valueIdx) { if (valueIdx < 0) node.addWarning(message); else node.addWarning(message, positions.get(valueIdx)); }
map|public PythonDataStream<SingleOutputStreamOperator<PyObject>> (MapFunction<PyObject, PyObject> mapper) throws IOException { return new PythonSingleOutputStreamOperator(stream.map(new PythonMapFunction(mapper))); }
set error|@Override public void (String message, int valueIdx) { node.addError(message); }
set warning|@Override public void (String message, int valueIdx) { node.addWarning(message); }
execute|@Override public CompletableFuture<Void> (CommitEvent event) { String scope = event.getScope(); String stream = event.getStream(); OperationContext context = streamMetadataStore.createContext(scope, stream); log.debug("Attempting to commit available transactions on stream {}/{}", event.getScope(), event.getStream()); CompletableFuture<Void> future = new CompletableFuture<>(); // Note: we will ignore the epoch in the event. It has been deprecated. // The logic now finds the smallest epoch with transactions and commits them. tryCommitTransactions(scope, stream, context).whenComplete(( r, e) -> { if (e != null) { Throwable cause = Exceptions.unwrap(e); // for operation not allowed, we will report the event if (cause instanceof StoreException.OperationNotAllowedException) { log.debug("Cannot commit transaction on stream {}/{}. Postponing", scope, stream); } else { log.error("Exception while attempting to commit transaction on stream {}/{}", scope, stream, e); } future.completeExceptionally(cause); } else { if (r >= 0) { log.debug("Successfully committed transactions on epoch {} on stream {}/{}", r, scope, stream); } else { log.debug("No transactions found in committing state on stream {}/{}", r, scope, stream); } if (processedEvents != null) { try { processedEvents.offer(event); } catch (Exception ex) { } } future.complete(null); } }); return future; }
fprop mini batch|public static void (long seed, Neurons[] neurons, DeepLearningModelInfo minfo, DeepLearningModelInfo consensus_minfo, boolean training, double[] responses, double[] offset, int n) { // Forward propagation for (int i = 1; i < neurons.length; ++i) neurons[i].fprop(seed, training, n); // Add offset (in link space) if applicable for (int mb = 0; mb < n; ++mb) { if (offset != null && offset[mb] > 0) { assert (!minfo._classification); // Regression double[] m = minfo.data_info()._normRespMul; double[] s = minfo.data_info()._normRespSub; double mul = m == null ? 1 : m[0]; double sub = s == null ? 0 : s[0]; neurons[neurons.length - 1]._a[mb].add(0, ((offset[mb] - sub) * mul)); } if (training) { // Compute the gradient at the output layer // auto-encoder: pass a dummy "response" (ignored) // otherwise: class label or regression target neurons[neurons.length - 1].setOutputLayerGradient(responses[mb], mb, n); // Elastic Averaging - set up helpers needed during back-propagation if (consensus_minfo != null) { for (int i = 1; i < neurons.length; i++) { neurons[i]._wEA = consensus_minfo.get_weights(i - 1); neurons[i]._bEA = consensus_minfo.get_biases(i - 1); } } } } }
initial centers|double[][] (KMeansModel model, final Vec[] vecs, final double[] means, final double[] mults, final int[] modes, int k) { // Categoricals use a different distance metric than numeric columns. model._output._categorical_column_count = 0; _isCats = new String[vecs.length][]; for (int v = 0; v < vecs.length; v++) { _isCats[v] = vecs[v].isCategorical() ? new String[0] : null; if (_isCats[v] != null) model._output._categorical_column_count++; } Random rand = water.util.RandomUtils.getRNG(_parms._seed - 1); // Cluster centers double centers[][]; if (null != _parms._user_points) { Frame user_points = _parms._user_points.get(); int numCenters = (int) user_points.numRows(); int numCols = model._output.nfeatures(); centers = new double[numCenters][numCols]; Vec[] centersVecs = user_points.vecs(); // Get the centers and standardize them if requested for (int r = 0; r < numCenters; r++) { for (int c = 0; c < numCols; c++) { centers[r][c] = centersVecs[c].at(r); centers[r][c] = Kmeans_preprocessData(centers[r][c], c, means, mults, modes); } } } else { // Random, Furthest, or PlusPlus initialization if (_parms._init == Initialization.Random) { // Initialize all cluster centers to random rows centers = new double[k][model._output.nfeatures()]; for (double[] center : centers) randomRow(vecs, rand, center, means, mults, modes); } else { centers = new double[1][model._output.nfeatures()]; // Initialize first cluster center to random row randomRow(vecs, rand, centers[0], means, mults, modes); model._output._iterations = 0; while (model._output._iterations < 5) { // Sum squares distances to cluster center SumSqr sqr = new SumSqr(centers, means, mults, modes, _isCats).doAll(vecs); // Sample with probability inverse to square distance Sampler sampler = new Sampler(centers, means, mults, modes, _isCats, sqr._sqr, k * 3, _parms.getOrMakeRealSeed(), hasWeightCol()).doAll(vecs); centers = ArrayUtils.append(centers, sampler._sampled); // Fill in sample centers into the model model._output._centers_raw = destandardize(centers, _isCats, means, mults); model._output._tot_withinss = sqr._sqr / _train.numRows(); // One iteration done model._output._iterations++; // Make early version of model visible, but don't update progress using update(1) model.update(_job); if (stop_requested()) { if (timeout()) warn("_max_runtime_secs reached.", "KMeans exited before finishing all iterations."); // Stopped/cancelled break; } } // Recluster down to k cluster centers centers = recluster(centers, rand, k, _parms._init, _isCats); // Reset iteration count model._output._iterations = 0; } } assert (centers.length == k); return centers; }
element to be clickable|public static ExpectedCondition<WebElement> (final By locator) { return new ExpectedCondition<WebElement>() { @Override public WebElement apply(WebDriver driver) { WebElement element = visibilityOfElementLocated(locator).apply(driver); try { if (element != null && element.isEnabled()) { return element; } return null; } catch (StaleElementReferenceException e) { return null; } } @Override public String toString() { return "element to be clickable: " + locator; } }; }
start|void () throws IOException, InterruptedException { synchronized (startupShutdownLock) { LOG.info("Starting history server."); Files.createDirectories(webDir.toPath()); LOG.info("Using directory {} as local cache.", webDir); Router router = new Router(); router.addGet("/:*", new HistoryServerStaticFileServerHandler(webDir)); if (!webDir.exists() && !webDir.mkdirs()) { throw new IOException("Failed to create local directory " + webDir.getAbsoluteFile() + "."); } createDashboardConfigFile(); archiveFetcher.start(); netty = new WebFrontendBootstrap(router, LOG, webDir, serverSSLFactory, webAddress, webPort, config); } }
delete image|@BetaApi public final Operation (ProjectGlobalImageName image) { DeleteImageHttpRequest request = DeleteImageHttpRequest.newBuilder().setImage(image == null ? null : image.toString()).build(); return deleteImage(request); }
apply|@Override public WebElement (WebDriver driver) { WebElement element = visibilityOfElementLocated(locator).apply(driver); try { if (element != null && element.isEnabled()) { return element; } return null; } catch (StaleElementReferenceException e) { return null; } }
set choices|@DataBoundSetter // this is terrible enough without being used anywhere @Restricted(NoExternalUse.class) public void (Object choices) { if (choices instanceof String) { setChoicesText((String) choices); return; } if (choices instanceof List) { ArrayList<String> newChoices = new ArrayList<>(); for (Object o : (List) choices) { if (o != null) { newChoices.add(o.toString()); } } this.choices = newChoices; return; } throw new IllegalArgumentException("expected String or List, but got " + choices.getClass().getName()); }
ensure principal access is allowed for service|static void (final ServiceTicket serviceTicket, final AuthenticationResult context, final RegisteredService registeredService) throws UnauthorizedServiceException, PrincipalException { ensurePrincipalAccessIsAllowedForService(serviceTicket.getService(), registeredService, context.getAuthentication()); }
create shape information|@Override public Pair<DataBuffer, long[]> (long[] shape, DataType dataType) { char order = Nd4j.order(); return createShapeInformation(shape, order, dataType); }
to string|@Override public String () { return "element to be clickable: " + locator; }
remove listener|public boolean (ChannelName channelName, EventListener msgListener) { Queue<RedisPubSubListener<?>> listeners = channelListeners.get(channelName); for (RedisPubSubListener<?> listener : listeners) { if (listener instanceof PubSubMessageListener) { if (((PubSubMessageListener<?>) listener).getListener() == msgListener) { removeListener(channelName, listener); return true; } } if (listener instanceof PubSubPatternMessageListener) { if (((PubSubPatternMessageListener<?>) listener).getListener() == msgListener) { removeListener(channelName, listener); return true; } } } return false; }
pojo|public static <T> TypeInformation<T> (Class<T> pojoClass, Map<String, TypeInformation<?>> fields) { final List<PojoField> pojoFields = new ArrayList<>(fields.size()); for (Map.Entry<String, TypeInformation<?>> field : fields.entrySet()) { final Field f = TypeExtractor.getDeclaredField(pojoClass, field.getKey()); if (f == null) { throw new InvalidTypesException("Field '" + field.getKey() + "'could not be accessed."); } pojoFields.add(new PojoField(f, field.getValue())); } return new PojoTypeInfo<>(pojoClass, pojoFields); }
iter|@Benchmark public int (IterState state) { ImmutableBitmap bitmap = state.bitmap; return iter(bitmap); }
get fingerprint|@StaplerDispatchable public Object (String md5sum) throws IOException { Fingerprint r = fingerprintMap.get(md5sum); if (r == null) return new NoFingerprintMatch(md5sum); else return r; }
get security token from request|protected SecurityToken (final HttpServletRequest request) { val cookieValue = wsFederationRequestConfigurationContext.getTicketGrantingTicketCookieGenerator().retrieveCookieValue(request); if (StringUtils.isNotBlank(cookieValue)) { val tgt = wsFederationRequestConfigurationContext.getTicketRegistry().getTicket(cookieValue, TicketGrantingTicket.class); if (tgt != null) { val sts = tgt.getDescendantTickets().stream().filter( t -> t.startsWith(SecurityTokenTicket.PREFIX)).findFirst().orElse(null); if (StringUtils.isNotBlank(sts)) { val stt = wsFederationRequestConfigurationContext.getTicketRegistry().getTicket(sts, SecurityTokenTicket.class); if (stt == null || stt.isExpired()) { LOGGER.warn("Security token ticket [{}] is not found or has expired", sts); return null; } if (stt.getSecurityToken().isExpired()) { LOGGER.warn("Security token linked to ticket [{}] has expired", sts); return null; } return stt.getSecurityToken(); } } } return null; }
decode|public static String (String rot, int offset, boolean isDecodeNumber) { final int len = rot.length(); final char[] chars = new char[len]; for (int i = 0; i < len; i++) { chars[i] = decodeChar(rot.charAt(i), offset, isDecodeNumber); } return new String(chars); }
add token|@Override public boolean (T element) { boolean ret = false; T oldElement = vocabulary.putIfAbsent(element.getStorageId(), element); if (oldElement == null) { //putIfAbsent added our element if (element.getLabel() != null) { extendedVocabulary.put(element.getLabel(), element); } oldElement = element; ret = true; } else { oldElement.incrementSequencesCount(element.getSequencesCount()); oldElement.increaseElementFrequency((int) element.getElementFrequency()); } totalWordCount.addAndGet((long) oldElement.getElementFrequency()); return ret; }
clear|public void () { weightParams.clear(); biasParams.clear(); paramsList = null; weightParamsList = null; biasParamsList = null; }
is event stream|public boolean () { boolean isEventStream = false; if (!getResponseHeader().isEmpty()) { isEventStream = getResponseHeader().hasContentType("text/event-stream"); } else { // response not available // is request for event-stream? String acceptHeader = getRequestHeader().getHeader("Accept"); if (acceptHeader != null && acceptHeader.equals("text/event-stream")) { // request is for an SSE stream isEventStream = true; } } return isEventStream; }
start|void () { Runnable r; for (int i = 0; i < threadCount; i++) { r = null; switch(config.getPayloadConfig().getPayloadCase()) { case SIMPLE_PARAMS: { if (config.getClientType() == Control.ClientType.SYNC_CLIENT) { if (config.getRpcType() == Control.RpcType.UNARY) { r = new BlockingUnaryWorker(blockingStubs[i % blockingStubs.length]); } } else if (config.getClientType() == Control.ClientType.ASYNC_CLIENT) { if (config.getRpcType() == Control.RpcType.UNARY) { r = new AsyncUnaryWorker(asyncStubs[i % asyncStubs.length]); } else if (config.getRpcType() == Control.RpcType.STREAMING) { r = new AsyncPingPongWorker(asyncStubs[i % asyncStubs.length]); } } break; } case BYTEBUF_PARAMS: { if (config.getClientType() == Control.ClientType.SYNC_CLIENT) { if (config.getRpcType() == Control.RpcType.UNARY) { r = new GenericBlockingUnaryWorker(channels[i % channels.length]); } } else if (config.getClientType() == Control.ClientType.ASYNC_CLIENT) { if (config.getRpcType() == Control.RpcType.UNARY) { r = new GenericAsyncUnaryWorker(channels[i % channels.length]); } else if (config.getRpcType() == Control.RpcType.STREAMING) { r = new GenericAsyncPingPongWorker(channels[i % channels.length]); } } break; } default: { throw Status.UNIMPLEMENTED.withDescription("Unknown payload case " + config.getPayloadConfig().getPayloadCase().name()).asRuntimeException(); } } if (r == null) { throw new IllegalStateException(config.getRpcType().name() + " not supported for client type " + config.getClientType()); } fixedThreadPool.execute(r); } if (osBean != null) { lastMarkCpuTime = osBean.getProcessCpuTime(); } }
press text|public static void (Image srcImage, File destFile, String pressText, Color color, Font font, int x, int y, float alpha) throws IORuntimeException { write(pressText(srcImage, pressText, color, font, x, y, alpha), destFile); }
send response|public static CompletableFuture<Void> (@Nonnull ChannelHandlerContext channelHandlerContext, boolean keepAlive, @Nonnull String message, @Nonnull HttpResponseStatus statusCode, @Nonnull Map<String, String> headers) { HttpResponse response = new DefaultHttpResponse(HTTP_1_1, statusCode); response.headers().set(CONTENT_TYPE, RestConstants.REST_CONTENT_TYPE); for (Map.Entry<String, String> headerEntry : headers.entrySet()) { response.headers().set(headerEntry.getKey(), headerEntry.getValue()); } if (keepAlive) { response.headers().set(CONNECTION, HttpHeaders.Values.KEEP_ALIVE); } byte[] buf = message.getBytes(ConfigConstants.DEFAULT_CHARSET); ByteBuf b = Unpooled.copiedBuffer(buf); HttpHeaders.setContentLength(response, buf.length); // write the initial line and the header. channelHandlerContext.write(response); channelHandlerContext.write(b); ChannelFuture lastContentFuture = channelHandlerContext.writeAndFlush(LastHttpContent.EMPTY_LAST_CONTENT); // close the connection, if no keep-alive is needed if (!keepAlive) { lastContentFuture.addListener(ChannelFutureListener.CLOSE); } return toCompletableFuture(lastContentFuture); }
add broadcast set for scatter function|public void (String name, DataSet<?> data) { this.bcVarsScatter.add(new Tuple2<>(name, data)); }
list annotation processors before ours|@SuppressWarnings("unused") private String () { try { Object discoveredProcessors = javacProcessingEnvironment_discoveredProcs.get(this.javacProcessingEnv); ArrayList<?> states = (ArrayList<?>) discoveredProcessors_procStateList.get(discoveredProcessors); if (states == null || states.isEmpty()) return null; if (states.size() == 1) return processorState_processor.get(states.get(0)).getClass().getName(); int idx = 0; StringBuilder out = new StringBuilder(); for (Object processState : states) { idx++; String name = processorState_processor.get(processState).getClass().getName(); if (out.length() > 0) out.append(", "); out.append("[").append(idx).append("] ").append(name); } return out.toString(); } catch (Exception e) { return null; } }
truncate vocabulary|public void (int threshold) { logger.debug("Truncating vocabulary to minWordFrequency: [" + threshold + "]"); Set<String> keyset = vocabulary.keySet(); for (String word : keyset) { VocabularyWord vw = vocabulary.get(word); // please note: we're not applying threshold to SPECIAL words if (!vw.isSpecial() && vw.getCount() < threshold) { vocabulary.remove(word); if (vw.getHuffmanNode() != null) idxMap.remove(vw.getHuffmanNode().getIdx()); } } }
remove bottom separators|public static void (JPopupMenu popupMenu) { int indexLastComponent = popupMenu.getComponentCount() - 1; while (indexLastComponent >= 0 && isPopupMenuSeparator(popupMenu.getComponent(indexLastComponent))) { popupMenu.remove(indexLastComponent); indexLastComponent -= 1; } }
append string|public static File (String content, File file, String charset) throws IORuntimeException { return FileWriter.create(file, CharsetUtil.charset(charset)).append(content); }
skip one line|private boolean () { if (!undecodedChunk.isReadable()) { return false; } byte nextByte = undecodedChunk.readByte(); if (nextByte == HttpConstants.CR) { if (!undecodedChunk.isReadable()) { undecodedChunk.readerIndex(undecodedChunk.readerIndex() - 1); return false; } nextByte = undecodedChunk.readByte(); if (nextByte == HttpConstants.LF) { return true; } undecodedChunk.readerIndex(undecodedChunk.readerIndex() - 2); return false; } if (nextByte == HttpConstants.LF) { return true; } undecodedChunk.readerIndex(undecodedChunk.readerIndex() - 1); return false; }
add output data|public TaskResult (String key, Object value) { this.outputData.put(key, value); return this; }
close safety net and guarded resources for thread|@Internal public static void () { SafetyNetCloseableRegistry registry = REGISTRIES.get(); if (null != registry) { REGISTRIES.remove(); IOUtils.closeQuietly(registry); } }
is null|public boolean (int index) { Object value = opt(index); return value == null || value == JSONObject.NULL; }
transport trailers received|protected void (Metadata trailers) { Preconditions.checkNotNull(trailers, "trailers"); if (transportError == null && !headersReceived) { transportError = validateInitialMetadata(trailers); if (transportError != null) { transportErrorMetadata = trailers; } } if (transportError != null) { transportError = transportError.augmentDescription("trailers: " + trailers); http2ProcessingFailed(transportError, false, transportErrorMetadata); } else { Status status = statusFromTrailers(trailers); stripTransportDetails(trailers); inboundTrailersReceived(trailers, status); } }
notify persistent connection listener|private boolean (HttpMessage httpMessage, Socket inSocket, ZapGetMethod method) { boolean keepSocketOpen = false; PersistentConnectionListener listener = null; List<PersistentConnectionListener> listenerList = parentServer.getPersistentConnectionListenerList(); for (int i = 0; i < listenerList.size(); i++) { listener = listenerList.get(i); try { if (listener.onHandshakeResponse(httpMessage, inSocket, method)) { // inform as long as one listener wishes to overtake the connection keepSocketOpen = true; break; } } catch (Exception e) { log.error("An error occurred while notifying listener:", e); } } return keepSocketOpen; }
get jwt from authorization|public static String (String authorization) { String jwt = null; if (authorization != null) { String[] parts = authorization.split(" "); if (parts.length == 2) { String scheme = parts[0]; String credentials = parts[1]; Pattern pattern = Pattern.compile("^Bearer$", Pattern.CASE_INSENSITIVE); if (pattern.matcher(scheme).matches()) { jwt = credentials; } } } return jwt; }
read string|public static String (URL url, String charset) throws IORuntimeException { if (url == null) { throw new NullPointerException("Empty url provided!"); } InputStream in = null; try { in = url.openStream(); return IoUtil.read(in, charset); } catch (IOException e) { throw new IORuntimeException(e); } finally { IoUtil.close(in); } }
create flow variable|public FlowVariable (final Flow flow, final String id, final Class type) { val opt = Arrays.stream(flow.getVariables()).filter( v -> v.getName().equalsIgnoreCase(id)).findFirst(); if (opt.isPresent()) { return opt.get(); } val flowVar = new FlowVariable(id, new BeanFactoryVariableValueFactory(type, applicationContext.getAutowireCapableBeanFactory())); flow.addVariable(flowVar); return flowVar; }
bind|@NonNull @UiThread public static Uner bind(@NonNull Object target, @NonNull View source) { Class<?> targetClass = target.getClass(); if (debug) Log.d(TAG, "Looking up binding for " + targetClass.getName()); Constructor<? extends Unbinder> constructor = findBindingConstructorForClass(targetClass); if (constructor == null) { return Unbinder.EMPTY; } //noinspection TryWithIdenticalCatches Resolves to API 19+ only type. try { return constructor.newInstance(target, source); } catch (IllegalAccessException e) { throw new RuntimeException("Unable to invoke " + constructor, e); } catch (InstantiationException e) { throw new RuntimeException("Unable to invoke " + constructor, e); } catch (InvocationTargetException e) { Throwable cause = e.getCause(); if (cause instanceof RuntimeException) { throw (RuntimeException) cause; } if (cause instanceof Error) { throw (Error) cause; } throw new RuntimeException("Unable to create binding instance.", cause); } }
authenticate|public Principal (List<String> authHeader) throws AuthException { if (isAuthEnabled()) { String credentials = parseCredentials(authHeader); return pravegaAuthManager.authenticate(credentials); } return null; }
get int|public final int (final int pos) { final int position = origin + pos; if (pos + 3 >= limit || pos < 0) throw new IllegalArgumentException("limit excceed: " + (pos < 0 ? pos : (pos + 3))); byte[] buf = buffer; return (0xff & buf[position]) | ((0xff & buf[position + 1]) << 8) | ((0xff & buf[position + 2]) << 16) | ((buf[position + 3]) << 24); }
of|public static RegionOperationId (RegionId regionId, String operation) { return new RegionOperationId(regionId.getProject(), regionId.getRegion(), operation); }
expire after update|long (Node<K, V> node, @Nullable K key, @Nullable V value, Expiry<K, V> expiry, long now) { if (expiresVariable() && (key != null) && (value != null)) { long currentDuration = Math.max(1, node.getVariableTime() - now); long duration = expiry.expireAfterUpdate(key, value, now, currentDuration); return isAsync ? (now + duration) : (now + Math.min(duration, MAXIMUM_EXPIRY)); } return 0L; }
get servo monitors|private List<Monitor<?>> () { List<Monitor<?>> monitors = new ArrayList<Monitor<?>>(); monitors.add(new InformationalMetric<String>(MonitorConfig.builder("name").build()) { @Override public String getValue() { return key.name(); } }); // allow Servo and monitor to know exactly at what point in time these stats are for so they can be plotted accurately monitors.add(new GaugeMetric(MonitorConfig.builder("currentTime").withTag(DataSourceLevel.DEBUG).build()) { @Override public Number getValue() { return System.currentTimeMillis(); } }); //collapser event cumulative metrics monitors.add(safelyGetCumulativeMonitor("countRequestsBatched", new Func0<HystrixEventType.Collapser>() { @Override public HystrixEventType.Collapser call() { return HystrixEventType.Collapser.ADDED_TO_BATCH; } })); monitors.add(safelyGetCumulativeMonitor("countBatches", new Func0<HystrixEventType.Collapser>() { @Override public HystrixEventType.Collapser call() { return HystrixEventType.Collapser.BATCH_EXECUTED; } })); monitors.add(safelyGetCumulativeMonitor("countResponsesFromCache", new Func0<HystrixEventType.Collapser>() { @Override public HystrixEventType.Collapser call() { return HystrixEventType.Collapser.RESPONSE_FROM_CACHE; } })); //batch size distribution metrics monitors.add(getBatchSizeMeanMonitor("batchSize_mean")); monitors.add(getBatchSizePercentileMonitor("batchSize_percentile_25", 25)); monitors.add(getBatchSizePercentileMonitor("batchSize_percentile_50", 50)); monitors.add(getBatchSizePercentileMonitor("batchSize_percentile_75", 75)); monitors.add(getBatchSizePercentileMonitor("batchSize_percentile_95", 95)); monitors.add(getBatchSizePercentileMonitor("batchSize_percentile_99", 99)); monitors.add(getBatchSizePercentileMonitor("batchSize_percentile_99_5", 99.5)); monitors.add(getBatchSizePercentileMonitor("batchSize_percentile_100", 100)); //shard size distribution metrics monitors.add(getShardSizeMeanMonitor("shardSize_mean")); monitors.add(getShardSizePercentileMonitor("shardSize_percentile_25", 25)); monitors.add(getShardSizePercentileMonitor("shardSize_percentile_50", 50)); monitors.add(getShardSizePercentileMonitor("shardSize_percentile_75", 75)); monitors.add(getShardSizePercentileMonitor("shardSize_percentile_95", 95)); monitors.add(getShardSizePercentileMonitor("shardSize_percentile_99", 99)); monitors.add(getShardSizePercentileMonitor("shardSize_percentile_99_5", 99.5)); monitors.add(getShardSizePercentileMonitor("shardSize_percentile_100", 100)); // properties (so the values can be inspected and monitored) monitors.add(new InformationalMetric<Number>(MonitorConfig.builder("propertyValue_rollingStatisticalWindowInMilliseconds").build()) { @Override public Number getValue() { return properties.metricsRollingStatisticalWindowInMilliseconds().get(); } }); monitors.add(new InformationalMetric<Boolean>(MonitorConfig.builder("propertyValue_requestCacheEnabled").build()) { @Override public Boolean getValue() { return properties.requestCacheEnabled().get(); } }); monitors.add(new InformationalMetric<Number>(MonitorConfig.builder("propertyValue_maxRequestsInBatch").build()) { @Override public Number getValue() { return properties.maxRequestsInBatch().get(); } }); monitors.add(new InformationalMetric<Number>(MonitorConfig.builder("propertyValue_timerDelayInMilliseconds").build()) { @Override public Number getValue() { return properties.timerDelayInMilliseconds().get(); } }); return monitors; }
either|public static <L, R> TypeInformation<Either<L, R>> (TypeInformation<L> leftType, TypeInformation<R> rightType) { return new EitherTypeInfo<>(leftType, rightType); }
feed forward to layer|public List<INDArray> (int layerNum, INDArray input) { try { return ffToLayerActivationsDetached(false, FwdPassType.STANDARD, false, layerNum, input, mask, null, true); } catch (OutOfMemoryError e) { CrashReportingUtil.writeMemoryCrashDump(this, e); throw e; } }
get install state|@Nonnull public InstallState () { if (installState != null) { installStateName = installState.name(); installState = null; } InstallState is = installStateName != null ? InstallState.valueOf(installStateName) : InstallState.UNKNOWN; return is != null ? is : InstallState.UNKNOWN; }
get ulong|public final long (final int pos) { final int position = origin + pos; if (pos + 4 >= limit || pos < 0) throw new IllegalArgumentException("limit excceed: " + (pos < 0 ? pos : (pos + 4))); byte[] buf = buffer; return ((long) (0xff & buf[position])) | ((long) (0xff & buf[position + 1]) << 8) | ((long) (0xff & buf[position + 2]) << 16) | ((long) (0xff & buf[position + 3]) << 24) | ((long) (0xff & buf[position + 4]) << 32); }
get source dirs on windows with drive letters|private List<String> () { List<String> driveLetters = asList("C"); try { driveLetters = OsUtils.getDrivesOnWindows(); } catch (Throwable ignore) { ignore.printStackTrace(); } List<String> sourceDirs = new ArrayList<String>(); for (String letter : driveLetters) { for (String possibleSource : descriptor.getSourceDirsOnWindows()) { if (!isDriveSpecificOnWindows(possibleSource)) { sourceDirs.add(letter + ":" + possibleSource); } } } for (String possibleSource : descriptor.getSourceDirsOnWindows()) { if (isDriveSpecificOnWindows(possibleSource)) sourceDirs.add(possibleSource); } return sourceDirs; }
add header|public MockResponse (String name, Object value) { headers.add(name, String.valueOf(value)); return this; }
log authentication transaction failure event|@EventListener public void (final CasAuthenticationTransactionFailureEvent e) { LOGGER.debug(AUTHN_TX_FAIL_MSG, e.getCredential(), e.getFailures()); }
new ant class loader|public static AntClassLoader (ClassLoader parent, Project project, Path path, boolean parentFirst) { if (subClassToLoad != null) { return (AntClassLoader) ReflectUtil.newInstance(subClassToLoad, CONSTRUCTOR_ARGS, new Object[] { parent, project, path, parentFirst }); } return new AntClassLoader(parent, project, path, parentFirst); }
reserve internal|@Override protected void (int newCapacity) { int oldCapacity = (nulls == 0L) ? 0 : capacity; if (isArray() || type instanceof MapType) { this.lengthData = Platform.reallocateMemory(lengthData, oldCapacity * 4L, newCapacity * 4L); this.offsetData = Platform.reallocateMemory(offsetData, oldCapacity * 4L, newCapacity * 4L); } else if (type instanceof ByteType || type instanceof BooleanType) { this.data = Platform.reallocateMemory(data, oldCapacity, newCapacity); } else if (type instanceof ShortType) { this.data = Platform.reallocateMemory(data, oldCapacity * 2L, newCapacity * 2L); } else if (type instanceof IntegerType || type instanceof FloatType || type instanceof DateType || DecimalType.is32BitDecimalType(type)) { this.data = Platform.reallocateMemory(data, oldCapacity * 4L, newCapacity * 4L); } else if (type instanceof LongType || type instanceof DoubleType || DecimalType.is64BitDecimalType(type) || type instanceof TimestampType) { this.data = Platform.reallocateMemory(data, oldCapacity * 8L, newCapacity * 8L); } else if (childColumns != null) { // Nothing to store. } else { throw new RuntimeException("Unhandled " + type); } this.nulls = Platform.reallocateMemory(nulls, oldCapacity, newCapacity); Platform.setMemory(nulls + oldCapacity, (byte) 0, newCapacity - oldCapacity); capacity = newCapacity; }
decode name|public static String (ByteBuf in) { int position = -1; int checked = 0; final int end = in.writerIndex(); final int readable = in.readableBytes(); // - https://www.ietf.org/rfc/rfc1035.txt , Section 3.1 if (readable == 0) { return ROOT; } final StringBuilder name = new StringBuilder(readable << 1); while (in.isReadable()) { final int len = in.readUnsignedByte(); final boolean pointer = (len & 0xc0) == 0xc0; if (pointer) { if (position == -1) { position = in.readerIndex() + 1; } if (!in.isReadable()) { throw new CorruptedFrameException("truncated pointer in a name"); } final int next = (len & 0x3f) << 8 | in.readUnsignedByte(); if (next >= end) { throw new CorruptedFrameException("name has an out-of-range pointer"); } in.readerIndex(next); // check for loops checked += 2; if (checked >= end) { throw new CorruptedFrameException("name contains a loop."); } } else if (len != 0) { if (!in.isReadable(len)) { throw new CorruptedFrameException("truncated label in a name"); } name.append(in.toString(in.readerIndex(), len, CharsetUtil.UTF_8)).append('.'); in.skipBytes(len); } else { // len == 0 break; } } if (position != -1) { in.readerIndex(position); } if (name.length() == 0) { return ROOT; } if (name.charAt(name.length() - 1) != '.') { name.append('.'); } return name.toString(); }
deserialize|@SneakyThrows public static <T> T (final InputStream inputStream, final Class<T> clazz) { try (val in = new ObjectInputStream(inputStream)) { val obj = in.readObject(); if (!clazz.isAssignableFrom(obj.getClass())) { throw new ClassCastException("Result [" + obj + " is of type " + obj.getClass() + " when we were expecting " + clazz); } return (T) obj; } }
get enum if present|@Nullable public static <T extends Enum<T>> T (final Class<T> enumClass, final String value) { Preconditions.checkNotNull(enumClass, "enumClass"); Preconditions.checkNotNull(value, "value"); for (T enumValue : enumClass.getEnumConstants()) { if (enumValue.name().equals(value)) { return enumValue; } } return null; }
random string fix length|public static String (Random random, int length) { return RandomStringUtils.random(length, 0, 0, true, true, null, random); }
read only|public static <T> Iterator<T> (final Iterator<T> itr) { return new Iterator<T>() { public boolean hasNext() { return itr.hasNext(); } public T next() { return itr.next(); } public void remove() { throw new UnsupportedOperationException(); } }; }
data|void (boolean outFinished, int streamId, Buffer source, boolean flush) { Preconditions.checkNotNull(source, "source"); OkHttpClientStream stream = transport.getStream(streamId); if (stream == null) { // In such case, we just throw away the data. return; } OutboundFlowState state = state(stream); int window = state.writableWindow(); boolean framesAlreadyQueued = state.hasPendingData(); int size = (int) source.size(); if (!framesAlreadyQueued && window >= size) { // Window size is large enough to send entire data frame state.write(source, size, outFinished); } else { // send partial data if (!framesAlreadyQueued && window > 0) { state.write(source, window, false); } // Queue remaining data in the buffer state.enqueue(source, (int) source.size(), outFinished); } if (flush) { flush(); } }
has next|public boolean () { return itr.hasNext(); }
save|public void (OutputStream stream) throws IOException { DataOutputStream out = null; try { out = new DataOutputStream(new BufferedOutputStream(stream)); for (int i = 0; i < _array.length; ++i) { out.writeInt(_array[i]); } } finally { if (out != null) { out.close(); } } }
next|public T () { return itr.next(); }
remove|public void () { throw new UnsupportedOperationException(); }
transform|Action (int act) { if (act == 0) { return ActionFactory.make_shift(); } else if (act < 1 + L) { return ActionFactory.make_left_arc(act - 1); } else if (act < 1 + 2 * L) { return ActionFactory.make_right_arc(act - 1 - L); } else { System.err.printf("unknown transition in transform(int): %d", act); } return new Action(); }
from config|public static ScopeFormats (Configuration config) { String jmFormat = config.getString(MetricOptions.SCOPE_NAMING_JM); String jmJobFormat = config.getString(MetricOptions.SCOPE_NAMING_JM_JOB); String tmFormat = config.getString(MetricOptions.SCOPE_NAMING_TM); String tmJobFormat = config.getString(MetricOptions.SCOPE_NAMING_TM_JOB); String taskFormat = config.getString(MetricOptions.SCOPE_NAMING_TASK); String operatorFormat = config.getString(MetricOptions.SCOPE_NAMING_OPERATOR); return new ScopeFormats(jmFormat, jmJobFormat, tmFormat, tmJobFormat, taskFormat, operatorFormat); }
get oss audit|private static Audit (JSONArray jsonArray, JSONArray global) { LOGGER.info("NFRR Audit Collector auditing LIBRARY_POLICY"); Audit audit = new Audit(); audit.setType(AuditType.LIBRARY_POLICY); Audit basicAudit; if ((basicAudit = doBasicAuditCheck(jsonArray, global, AuditType.LIBRARY_POLICY)) != null) { return basicAudit; } audit.setAuditStatus(AuditStatus.OK); audit.setDataStatus(DataStatus.OK); for (Object o : jsonArray) { JSONArray auditJO = (JSONArray) ((JSONObject) o).get(STR_AUDITSTATUSES); Optional<Object> lpOptObj = Optional.ofNullable(((JSONObject) o).get(STR_LIBRARYPOLICYRESULT)); lpOptObj.ifPresent( lpObj -> audit.getUrl().add(((JSONObject) lpOptObj.get()).get(STR_REPORTURL).toString())); auditJO.stream().map( aj -> audit.getAuditStatusCodes().add((String) aj)); boolean ok = false; for (Object s : auditJO) { String status = (String) s; audit.getAuditStatusCodes().add(status); if (LibraryPolicyAuditStatus.LIBRARY_POLICY_AUDIT_OK.name().equalsIgnoreCase(status)) { ok = true; break; } if (LibraryPolicyAuditStatus.LIBRARY_POLICY_AUDIT_MISSING.name().equalsIgnoreCase(status)) { audit.setAuditStatus(AuditStatus.NA); audit.setDataStatus(DataStatus.NO_DATA); return audit; } } if (!ok) { audit.setAuditStatus(AuditStatus.FAIL); return audit; } } return audit; }
get read schema|private MessageType (MessageType fileSchema, Path filePath) { RowTypeInfo fileTypeInfo = (RowTypeInfo) ParquetSchemaConverter.fromParquetType(fileSchema); List<Type> types = new ArrayList<>(); for (int i = 0; i < fieldNames.length; ++i) { String readFieldName = fieldNames[i]; TypeInformation<?> readFieldType = fieldTypes[i]; if (fileTypeInfo.getFieldIndex(readFieldName) < 0) { if (!skipWrongSchemaFileSplit) { throw new IllegalArgumentException("Field " + readFieldName + " cannot be found in schema of " + " Parquet file: " + filePath + "."); } else { this.skipThisSplit = true; return fileSchema; } } if (!readFieldType.equals(fileTypeInfo.getTypeAt(readFieldName))) { if (!skipWrongSchemaFileSplit) { throw new IllegalArgumentException("Expecting type " + readFieldType + " for field " + readFieldName + " but found type " + fileTypeInfo.getTypeAt(readFieldName) + " in Parquet file: " + filePath + "."); } else { this.skipThisSplit = true; return fileSchema; } } types.add(fileSchema.getType(readFieldName)); } return new MessageType(fileSchema.getName(), types); }
read blob from id|public byte[] (String bucketName, String blobName, long blobGeneration) { BlobId blobId = BlobId.of(bucketName, blobName, blobGeneration); byte[] content = storage.readAllBytes(blobId); // [END readBlobFromId] return content; }
add|public void (List<BatchCSVRecord> record) { if (records == null) records = new ArrayList<>(); records.add(record); }
synchronize to host|@Override public void (AllocationPoint point) { if (!point.isActualOnHostSide()) { CudaContext context = (CudaContext) allocator.getDeviceContext().getContext(); if (!point.isConstant()) waitTillFinished(point); // if this piece of memory is device-dependant, we'll also issue copyback once if (point.getAllocationStatus() == AllocationStatus.DEVICE && !point.isActualOnHostSide()) { long perfD = PerformanceTracker.getInstance().helperStartTransaction(); if (nativeOps.memcpyAsync(point.getHostPointer(), point.getDevicePointer(), AllocationUtils.getRequiredMemory(point.getShape()), CudaConstants.cudaMemcpyDeviceToHost, context.getSpecialStream()) == 0) throw new IllegalStateException("MemcpyAsync failed: " + point.getShape()); commitTransfer(context.getSpecialStream()); PerformanceTracker.getInstance().helperRegisterTransaction(point.getDeviceId(), perfD, point.getNumberOfBytes(), MemcpyDirection.DEVICE_TO_HOST); } // else log.info("Not [DEVICE] memory, skipping..."); // updating host read timer point.tickHostRead(); //log.info("After sync... isActualOnHostSide: {}", point.isActualOnHostSide()); } // else log.info("Point is actual on host side! " + point.getShape()); }
get server tls finger print|private String () { String fingerPrint = null; Map<String, Object> serverConfig = Config.getInstance().getJsonMapConfigNoCache("server"); Map<String, Object> secretConfig = Config.getInstance().getJsonMapConfigNoCache("secret"); // load keystore here based on server config and secret config String keystoreName = (String) serverConfig.get("keystoreName"); String serverKeystorePass = (String) secretConfig.get("serverKeystorePass"); if (keystoreName != null) { try (InputStream stream = Config.getInstance().getInputStreamFromFile(keystoreName)) { KeyStore loadedKeystore = KeyStore.getInstance("JKS"); loadedKeystore.load(stream, serverKeystorePass.toCharArray()); X509Certificate cert = (X509Certificate) loadedKeystore.getCertificate("server"); if (cert != null) { fingerPrint = FingerPrintUtil.getCertFingerPrint(cert); } else { logger.error("Unable to find the certificate with alias name as server in the keystore"); } } catch (Exception e) { logger.error("Unable to load server keystore ", e); } } return fingerPrint; }
eviction policy|public LocalCachedMapOptions<K, V> (EvictionPolicy evictionPolicy) { if (evictionPolicy == null) { throw new NullPointerException("evictionPolicy can't be null"); } this.evictionPolicy = evictionPolicy; return this; }
initialize|protected void () { this.addActionListener(new java.awt.event.ActionListener() { @Override public void actionPerformed(java.awt.event.ActionEvent e) { log.debug("actionPerformed " + lastInvoker.name() + " " + e.getActionCommand()); try { if (multiSelect) { performActions(getSelectedHistoryReferences()); } else { HistoryReference ref = getSelectedHistoryReference(); if (ref != null) { try { performAction(ref); } catch (Exception e1) { log.error(e1.getMessage(), e1); } } else { log.error("PopupMenuHistoryReference invoker " + lastInvoker + " failed to get history ref"); } } } catch (Exception e2) { log.error(e2.getMessage(), e2); } } }); }
extract peer|@Override public TsiPeer () throws GeneralSecurityException { Preconditions.checkState(!isInProgress(), "Handshake is not complete."); List<TsiPeer.Property<?>> peerProperties = new ArrayList<>(); peerProperties.add(new TsiPeer.StringProperty(TSI_SERVICE_ACCOUNT_PEER_PROPERTY, handshaker.getResult().getPeerIdentity().getServiceAccount())); return new TsiPeer(peerProperties); }
info|public void (CharSequence info) { if (StringUtils.isNotEmpty(info)) { msg.printMessage(Diagnostic.Kind.NOTE, Consts.PREFIX_OF_LOGGER + info); } }
no null elements|public static void (Object[] array, String message) { if (array != null) { for (int i = 0; i < array.length; i++) { if (array[i] == null) { throw new IllegalArgumentException(message); } } } }
reslove dialect class|private Class (String className) throws Exception { if (dialectAliasMap.containsKey(className.toLowerCase())) { return dialectAliasMap.get(className.toLowerCase()); } else { return Class.forName(className); } }
set scan buttons and progress bar states|private void (boolean isStarted, boolean isPaused, boolean allowStartScan) { if (isStarted) { getStartScanButton().setEnabled(false); getPauseScanButton().setEnabled(true); getPauseScanButton().setSelected(isPaused); getStopScanButton().setEnabled(true); getProgressBar().setEnabled(true); } else { getStartScanButton().setEnabled(allowStartScan); getStopScanButton().setEnabled(false); getPauseScanButton().setEnabled(false); getPauseScanButton().setSelected(false); getProgressBar().setEnabled(false); } }
get ext instance|public T (Class[] argTypes, Object[] args) { if (clazz != null) { try { if (singleton) { // 如果是单例 if (instance == null) { synchronized (this) { if (instance == null) { instance = ClassUtils.newInstanceWithArgs(clazz, argTypes, args); } } } // 保留单例 return instance; } else { return ClassUtils.newInstanceWithArgs(clazz, argTypes, args); } } catch (Exception e) { throw new SofaRpcRuntimeException("create " + clazz.getCanonicalName() + " instance error", e); } } throw new SofaRpcRuntimeException("Class of ExtensionClass is null"); }
action performed|@Override public void (java.awt.event.ActionEvent e) { log.debug("actionPerformed " + lastInvoker.name() + " " + e.getActionCommand()); try { if (multiSelect) { performActions(getSelectedHistoryReferences()); } else { HistoryReference ref = getSelectedHistoryReference(); if (ref != null) { try { performAction(ref); } catch (Exception e1) { log.error(e1.getMessage(), e1); } } else { log.error("PopupMenuHistoryReference invoker " + lastInvoker + " failed to get history ref"); } } } catch (Exception e2) { log.error(e2.getMessage(), e2); } }
after|static Deadline (long duration, TimeUnit units, Ticker ticker) { checkNotNull(units, "units"); return new Deadline(ticker, units.toNanos(duration), true); }
or else create|public T (Class<? extends T> type) { Assert.notNull(type, "Type must not be null"); return (this.value != null) ? this.value : BeanUtils.instantiateClass(type); }
get precise free memory|public long (int deviceId) { // we refresh free memory on device val extFree = NativeOpsHolder.getInstance().getDeviceNativeOps().getDeviceFreeMemory(deviceId); return extFree; }
mode|public static int (File f) throws PosixException { if (Functions.isWindows()) return -1; try { if (Util.NATIVE_CHMOD_MODE) { return PosixAPI.jnr().stat(f.getPath()).mode(); } else { return Util.permissionsToMode(Files.getPosixFilePermissions(fileToPath(f))); } } catch (IOException cause) { PosixException e = new PosixException("Unable to get file permissions", null); e.initCause(cause); throw e; } }
read|public static StreamSegmentReadResult (SegmentProperties segmentInfo, long startOffset, int maxReadLength, int readBlockSize, ReadOnlyStorage storage) { Exceptions.checkArgument(startOffset >= 0, "startOffset", "startOffset must be a non-negative number."); Exceptions.checkArgument(maxReadLength >= 0, "maxReadLength", "maxReadLength must be a non-negative number."); Preconditions.checkNotNull(segmentInfo, "segmentInfo"); Preconditions.checkNotNull(storage, "storage"); String traceId = String.format("Read[%s]", segmentInfo.getName()); return new StreamSegmentReadResult(startOffset, maxReadLength, new SegmentReader(segmentInfo, null, readBlockSize, storage), traceId); }
calculate truncation offset|long (SegmentProperties info, long highestCopiedOffset) { long truncateOffset = -1; if (highestCopiedOffset > 0) { // Due to the nature of compaction (all entries are copied in order of their original versions), if we encounter // any copied Table Entries then the highest explicit version defined on any of them is where we can truncate. truncateOffset = highestCopiedOffset; } else if (this.indexReader.getLastIndexedOffset(info) >= info.getLength()) { // Did not encounter any copied entries. If we were able to index the whole segment, then we should be safe // to truncate at wherever the compaction last finished. truncateOffset = this.indexReader.getCompactionOffset(info); } if (truncateOffset <= info.getStartOffset()) { // The segment is already truncated at the compaction offset; no need for more. truncateOffset = -1; } return truncateOffset; }
main|public static void (final String[] args) throws Exception { // Redirect all std out and err messages into log4j StdOutErrRedirect.redirectOutAndErrToLog(); logger.info("Starting Jetty Azkaban Executor..."); if (System.getSecurityManager() == null) { Policy.setPolicy(new Policy() { @Override public boolean implies(final ProtectionDomain domain, final Permission permission) { // allow all return true; } }); System.setSecurityManager(new SecurityManager()); } final Props props = AzkabanServer.loadProps(args); if (props == null) { logger.error("Azkaban Properties not loaded."); logger.error("Exiting Azkaban Executor Server..."); return; } /* Initialize Guice Injector */ final Injector injector = Guice.createInjector(new AzkabanCommonModule(props), new AzkabanExecServerModule()); SERVICE_PROVIDER.setInjector(injector); launch(injector.getInstance(AzkabanExecutorServer.class)); }
stop|protected void (String[] args) throws Exception { LOG.info("Running 'stop-with-savepoint' command."); final Options commandOptions = CliFrontendParser.getStopCommandOptions(); final Options commandLineOptions = CliFrontendParser.mergeOptions(commandOptions, customCommandLineOptions); final CommandLine commandLine = CliFrontendParser.parse(commandLineOptions, args, false); final StopOptions stopOptions = new StopOptions(commandLine); if (stopOptions.isPrintHelp()) { CliFrontendParser.printHelpForStop(customCommandLines); return; } final String[] cleanedArgs = stopOptions.getArgs(); final String targetDirectory = stopOptions.hasSavepointFlag() && cleanedArgs.length > 0 ? stopOptions.getTargetDirectory() : // the default savepoint location is going to be used in this case. null; final JobID jobId = cleanedArgs.length != 0 ? parseJobId(cleanedArgs[0]) : parseJobId(stopOptions.getTargetDirectory()); final boolean advanceToEndOfEventTime = stopOptions.shouldAdvanceToEndOfEventTime(); logAndSysout((advanceToEndOfEventTime ? "Draining job " : "Suspending job ") + "\"" + jobId + "\" with a savepoint."); final CustomCommandLine<?> activeCommandLine = getActiveCustomCommandLine(commandLine); runClusterAction(activeCommandLine, commandLine, clusterClient -> { try { clusterClient.stopWithSavepoint(jobId, advanceToEndOfEventTime, targetDirectory); } catch (Exception e) { throw new FlinkException("Could not stop with a savepoint job \"" + jobId + "\".", e); } }); logAndSysout((advanceToEndOfEventTime ? "Drained job " : "Suspended job ") + "\"" + jobId + "\" with a savepoint."); }
insert stop natures|public static void (String key, String... filterNatures) { StopRecognition fr = get(key); fr.insertStopNatures(filterNatures); }
for segment|static SegmentChunk (String segmentName, long startOffset) { return new SegmentChunk(StreamSegmentNameUtils.getSegmentChunkName(segmentName, startOffset), startOffset); }
get chk proxy only|private JCheckBox () { if (proxyOnlyCheckbox == null) { proxyOnlyCheckbox = new JCheckBox(); proxyOnlyCheckbox.setText(Constant.messages.getString("httpsessions.options.label.proxyOnly")); } return proxyOnlyCheckbox; }
handle|static void (ChannelHandlerContext ctx, Http2Connection connection, Http2FrameListener listener, FullHttpMessage message) throws Http2Exception { try { int streamId = getStreamId(connection, message.headers()); Http2Stream stream = connection.stream(streamId); if (stream == null) { stream = connection.remote().createStream(streamId, false); } message.headers().set(HttpConversionUtil.ExtensionHeaderNames.SCHEME.text(), HttpScheme.HTTP.name()); Http2Headers messageHeaders = HttpConversionUtil.toHttp2Headers(message, true); boolean hasContent = message.content().isReadable(); boolean hasTrailers = !message.trailingHeaders().isEmpty(); listener.onHeadersRead(ctx, streamId, messageHeaders, 0, !(hasContent || hasTrailers)); if (hasContent) { listener.onDataRead(ctx, streamId, message.content(), 0, !hasTrailers); } if (hasTrailers) { Http2Headers headers = HttpConversionUtil.toHttp2Headers(message.trailingHeaders(), true); listener.onHeadersRead(ctx, streamId, headers, 0, true); } stream.closeRemoteSide(); } finally { message.release(); } }
get pairwise edge intersection|private DataSet<Edge<K, EV>> (DataSet<Edge<K, EV>> edges) { return this.getEdges().coGroup(edges).where(0, 1, 2).equalTo(0, 1, 2).with(new MatchingEdgeReducer<>()).name("Intersect edges"); }
start element|@Override public void (String uri, String localName, String qName, Attributes attributes) throws SAXException { attempt(); // if we still haven't found it at the first start element, then we are not going to find it. throw new Eureka(null); }
merge object|public static Object (Object config, Class clazz) { merge(config); return convertMapToObj((Map<String, Object>) config, clazz); }
attempt|private void () throws Eureka { if (loc == null) return; if (loc instanceof Locator2) { Locator2 loc2 = (Locator2) loc; String e = loc2.getEncoding(); if (e != null) throw new Eureka(e); } }
save features|private void (List<Feature> features, FeatureCollector collector) { features.forEach( f -> { f.setCollectorId(collector.getId()); Feature existing = featureRepository.findByCollectorIdAndSIdAndSTeamID(collector.getId(), f.getsId(), f.getsTeamID()); if (existing != null) { f.setId(existing.getId()); } featureRepository.save(f); }); }
remove|public ByteBuf (int bytes, ChannelPromise aggregatePromise) { return remove(channel.alloc(), bytes, aggregatePromise); }
remove all|public <T> Iterable<T> (Key<T> key) { if (isEmpty()) { return null; } int writeIdx = 0; int readIdx = 0; List<T> ret = null; for (; readIdx < size; readIdx++) { if (bytesEqual(key.asciiName(), name(readIdx))) { ret = ret != null ? ret : new ArrayList<T>(); ret.add(key.parseBytes(value(readIdx))); continue; } name(writeIdx, name(readIdx)); value(writeIdx, value(readIdx)); writeIdx++; } int newSize = writeIdx; // Multiply by two since namesAndValues is interleaved. Arrays.fill(namesAndValues, writeIdx * 2, len(), null); size = newSize; return ret; }
notify new message|public void (Plugin plugin, HttpMessage message) { parentScanner.notifyNewMessage(message); notifyNewMessage(plugin); }
create has service check decision state|protected void (final Flow flow) { createDecisionState(flow, CasWebflowConstants.STATE_ID_HAS_SERVICE_CHECK, "flowScope.service != null", CasWebflowConstants.STATE_ID_RENEW_REQUEST_CHECK, CasWebflowConstants.STATE_ID_VIEW_GENERIC_LOGIN_SUCCESS); }
set value|public void (CharSequence value, int offset, int len) { checkNotNull(value); if (offset < 0 || len < 0 || offset > value.length() - len) { throw new IndexOutOfBoundsException("offset: " + offset + " len: " + len + " value.len: " + len); } ensureSize(len); this.len = len; for (int i = 0; i < len; i++) { this.value[i] = value.charAt(offset + i); } this.hashCode = 0; }
preflight response headers|public HttpHeaders () { if (preflightHeaders.isEmpty()) { return EmptyHttpHeaders.INSTANCE; } final HttpHeaders preflightHeaders = new DefaultHttpHeaders(); for (Entry<CharSequence, Callable<?>> entry : this.preflightHeaders.entrySet()) { final Object value = getValue(entry.getValue()); if (value instanceof Iterable) { preflightHeaders.add(entry.getKey(), (Iterable<?>) value); } else { preflightHeaders.add(entry.getKey(), value); } } return preflightHeaders; }
write message embedded web server ip port|private void (Socket s) throws Exception { writeType(s, TYPE_EMBEDDED_WEB_SERVER_IP_PORT); writeString(s, _embeddedWebServerIp); writeInt(s, _embeddedWebServerPort); }
exec|public INDArray (INDArray a, INDArray b, INDArray result) { a = transposeIfReq(transposeA, a); b = transposeIfReq(transposeB, b); if (result == null) { INDArray ret = a.mmul(b); return transposeIfReq(transposeResult, ret); } else { if (!transposeResult) { return a.mmuli(b, result); } else { return a.mmuli(b, result).transpose(); } } }
allocate single slot|private CompletableFuture<LogicalSlot> (SlotRequestId slotRequestId, SlotProfile slotProfile, boolean allowQueuedScheduling, Time allocationTimeout) { Optional<SlotAndLocality> slotAndLocality = tryAllocateFromAvailable(slotRequestId, slotProfile); if (slotAndLocality.isPresent()) { // already successful from available try { return CompletableFuture.completedFuture(completeAllocationByAssigningPayload(slotRequestId, slotAndLocality.get())); } catch (FlinkException e) { return FutureUtils.completedExceptionally(e); } } else if (allowQueuedScheduling) { // we allocate by requesting a new slot return slotPool.requestNewAllocatedSlot(slotRequestId, slotProfile.getResourceProfile(), allocationTimeout).thenApply((PhysicalSlot allocatedSlot) -> { try { return completeAllocationByAssigningPayload(slotRequestId, new SlotAndLocality(allocatedSlot, Locality.UNKNOWN)); } catch (FlinkException e) { throw new CompletionException(e); } }); } else { // failed to allocate return FutureUtils.completedExceptionally(new NoResourceAvailableException("Could not allocate a simple slot for " + slotRequestId + '.')); } }
get redirect response|public static Response (final String url, final Map<String, String> parameters) { val builder = new StringBuilder(parameters.size() * RESPONSE_INITIAL_CAPACITY); val sanitizedUrl = sanitizeUrl(url); LOGGER.debug("Sanitized URL for redirect response is [{}]", sanitizedUrl); val fragmentSplit = Splitter.on("#").splitToList(sanitizedUrl); builder.append(fragmentSplit.get(0)); val params = parameters.entrySet().stream().filter( entry -> entry.getValue() != null).map( entry -> { try { return String.join("=", entry.getKey(), EncodingUtils.urlEncode(entry.getValue())); } catch (final Exception e) { return String.join("=", entry.getKey(), entry.getValue()); } }).collect(Collectors.joining("&")); if (!(params == null || params.isEmpty())) { builder.append(url.contains("?") ? "&" : "?"); builder.append(params); } if (fragmentSplit.size() > 1) { builder.append('#'); builder.append(fragmentSplit.get(1)); } val urlRedirect = builder.toString(); LOGGER.debug("Final redirect response is [{}]", urlRedirect); return new DefaultResponse(ResponseType.REDIRECT, urlRedirect, parameters); }
implies|@Override public boolean (final ProtectionDomain domain, final Permission permission) { // allow all return true; }
try restore execution graph from savepoint|private void (ExecutionGraph executionGraphToRestore, SavepointRestoreSettings savepointRestoreSettings) throws Exception { if (savepointRestoreSettings.restoreSavepoint()) { final CheckpointCoordinator checkpointCoordinator = executionGraphToRestore.getCheckpointCoordinator(); if (checkpointCoordinator != null) { checkpointCoordinator.restoreSavepoint(savepointRestoreSettings.getRestorePath(), savepointRestoreSettings.allowNonRestoredState(), executionGraphToRestore.getAllVertices(), userCodeLoader); } } }
call|public Object (Long nid, final Event event) { return delegate.call(convertToAddress(nid), event); }
split sentence|static List<String> (String document, String sentence_separator) { List<String> sentences = new ArrayList<String>(); for (String line : document.split("[\r\n]")) { line = line.trim(); if (line.length() == 0) continue; for (// [，,。:：“”？?！!；;] String sent : // [，,。:：“”？?！!；;] line.split(sentence_separator)) { sent = sent.trim(); if (sent.length() == 0) continue; sentences.add(sent); } } return sentences; }
find all|public static List<String> (String regex, CharSequence content, int group) { return findAll(regex, content, group, new ArrayList<String>()); }
get header segment name|public static String (String segmentName) { Preconditions.checkArgument(!segmentName.endsWith(HEADER_SUFFIX), "segmentName is already a segment header name"); return segmentName + HEADER_SUFFIX; }
to setters|static Map<Method, Setter> (SetterFactory setterFactory, Target<?> target, Set<Method> methods) { Map<Method, Setter> result = new LinkedHashMap<Method, Setter>(); for (Method method : methods) { method.setAccessible(true); result.put(method, setterFactory.create(target, method)); } return result; }
parse|public static Area (String s) { Matcher m = PATTERN.matcher(s); if (m.matches()) return new Area(Integer.parseInt(m.group(1)), Integer.parseInt(m.group(2))); return null; }
get class loader for extension|public static URLClassLoader (File extension, boolean useExtensionClassloaderFirst) { return loadersMap.computeIfAbsent(extension, theExtension -> makeClassLoaderForExtension(theExtension, useExtensionClassloaderFirst)); }
rnn set previous state|public void (String layerName, Map<String, INDArray> state) { Layer l = verticesMap.get(layerName).getLayer(); if (l instanceof org.deeplearning4j.nn.layers.wrapper.BaseWrapperLayer) { l = ((org.deeplearning4j.nn.layers.wrapper.BaseWrapperLayer) l).getUnderlying(); } if (l == null || !(l instanceof RecurrentLayer)) { throw new UnsupportedOperationException("Layer \"" + layerName + "\" is not a recurrent layer. Cannot set state"); } ((RecurrentLayer) l).rnnSetPreviousState(state); }
add event|public void (UserFeedbackEvent event) { UserFeedbackEvent[] oldEvents = feedbackEvents; feedbackEvents = new UserFeedbackEvent[feedbackEvents.length + 1]; System.arraycopy(oldEvents, 0, feedbackEvents, 0, oldEvents.length); feedbackEvents[oldEvents.length] = event; }
parse model config|public static Map<String, Object> (String modelJson, String modelYaml) throws IOException, InvalidKerasConfigurationException { Map<String, Object> modelConfig; if (modelJson != null) modelConfig = parseJsonString(modelJson); else if (modelYaml != null) modelConfig = parseYamlString(modelYaml); else throw new InvalidKerasConfigurationException("Requires model configuration as either JSON or YAML string."); return modelConfig; }
is sub|public static boolean (File parent, File sub) { Assert.notNull(parent); Assert.notNull(sub); return sub.toPath().startsWith(parent.toPath()); }
restore from training config zip|public static SameDiff (File file) throws IOException { ZipFile zipFile = new ZipFile(file); ZipEntry config = zipFile.getEntry(TRAINING_CONFIG_JSON_ZIP_ENTRY_NAME); TrainingConfig trainingConfig = null; try (InputStream stream = zipFile.getInputStream(config)) { byte[] read = IOUtils.toByteArray(stream); trainingConfig = ObjectMapperHolder.getJsonMapper().readValue(read, TrainingConfig.class); } SameDiff ret = null; ZipEntry sameDiffFile = zipFile.getEntry(SAMEDIFF_FILE_ENTRY_NAME); try (InputStream stream = zipFile.getInputStream(sameDiffFile)) { byte[] read = IOUtils.toByteArray(stream); ret = SameDiff.fromFlatBuffers(ByteBuffer.wrap(read)); } ret.setTrainingConfig(trainingConfig); ret.initializeTraining(); return ret; }
to bytes|public void (ByteBuffer buf) { if (canStoreCompact() && getCompactStorageSize() < getSparseStorageSize()) { // store compact toBytesCompact(buf); } else { // store sparse toBytesSparse(buf); } }
get dynamic fork join tasks and input|@VisibleForTesting Pair<List<WorkflowTask>, Map<String, Map<String, Object>>> (WorkflowTask taskToSchedule, Workflow workflowInstance) throws TerminateWorkflowException { String dynamicForkJoinTaskParam = taskToSchedule.getDynamicForkJoinTasksParam(); Map<String, Object> input = parametersUtils.getTaskInput(taskToSchedule.getInputParameters(), workflowInstance, null, null); Object paramValue = input.get(dynamicForkJoinTaskParam); DynamicForkJoinTaskList dynamicForkJoinTaskList = objectMapper.convertValue(paramValue, DynamicForkJoinTaskList.class); if (dynamicForkJoinTaskList == null) { String reason = String.format("Dynamic tasks could not be created. The value of %s from task's input %s has no dynamic tasks to be scheduled", dynamicForkJoinTaskParam, input); logger.error(reason); throw new TerminateWorkflowException(reason); } Map<String, Map<String, Object>> dynamicForkJoinTasksInput = new HashMap<>(); List<WorkflowTask> dynamicForkJoinWorkflowTasks = dynamicForkJoinTaskList.getDynamicTasks().stream().peek(//TODO create a custom pair collector dynamicForkJoinTask -> dynamicForkJoinTasksInput.put(dynamicForkJoinTask.getReferenceName(), dynamicForkJoinTask.getInput())).map( dynamicForkJoinTask -> { WorkflowTask dynamicForkJoinWorkflowTask = new WorkflowTask(); dynamicForkJoinWorkflowTask.setTaskReferenceName(dynamicForkJoinTask.getReferenceName()); dynamicForkJoinWorkflowTask.setName(dynamicForkJoinTask.getTaskName()); dynamicForkJoinWorkflowTask.setType(dynamicForkJoinTask.getType()); if (MetadataMapperService.shouldPopulateDefinition(dynamicForkJoinWorkflowTask)) { dynamicForkJoinWorkflowTask.setTaskDefinition(metadataDAO.getTaskDef(dynamicForkJoinTask.getTaskName())); } return dynamicForkJoinWorkflowTask; }).collect(Collectors.toCollection(LinkedList::new)); return new ImmutablePair<>(dynamicForkJoinWorkflowTasks, dynamicForkJoinTasksInput); }
write bean definitions to meta inf|private void () { try { classWriterOutputVisitor.finish(); } catch (Exception e) { String message = e.getMessage(); error("Error occurred writing META-INF files: %s", message != null ? message : e); } }
create hadoop writable type info|public static <T> TypeInformation<T> (Class<T> clazz) { checkNotNull(clazz); Class<?> typeInfoClass; try { typeInfoClass = Class.forName(HADOOP_WRITABLE_TYPEINFO_CLASS, false, TypeExtractor.class.getClassLoader()); } catch (ClassNotFoundException e) { throw new RuntimeException("Could not load the TypeInformation for the class '" + HADOOP_WRITABLE_CLASS + "'. You may be missing the 'flink-hadoop-compatibility' dependency."); } try { Constructor<?> constr = typeInfoClass.getConstructor(Class.class); @SuppressWarnings("unchecked") TypeInformation<T> typeInfo = (TypeInformation<T>) constr.newInstance(clazz); return typeInfo; } catch (NoSuchMethodException | IllegalAccessException | InstantiationException e) { throw new RuntimeException("Incompatible versions of the Hadoop Compatibility classes found."); } catch (InvocationTargetException e) { throw new RuntimeException("Cannot create Hadoop WritableTypeInfo.", e.getTargetException()); } }
get dynamic fork join tasks and input|@VisibleForTesting Pair<List<WorkflowTask>, Map<String, Map<String, Object>>> (WorkflowTask taskToSchedule, Workflow workflowInstance) throws TerminateWorkflowException { String dynamicForkJoinTaskParam = taskToSchedule.getDynamicForkJoinTasksParam(); Map<String, Object> input = parametersUtils.getTaskInput(taskToSchedule.getInputParameters(), workflowInstance, null, null); Object paramValue = input.get(dynamicForkJoinTaskParam); DynamicForkJoinTaskList dynamicForkJoinTaskList = objectMapper.convertValue(paramValue, DynamicForkJoinTaskList.class); if (dynamicForkJoinTaskList == null) { String reason = String.format("Dynamic tasks could not be created. The value of %s from task's input %s has no dynamic tasks to be scheduled", dynamicForkJoinTaskParam, input); logger.error(reason); throw new TerminateWorkflowException(reason); } Map<String, Map<String, Object>> dynamicForkJoinTasksInput = new HashMap<>(); List<WorkflowTask> dynamicForkJoinWorkflowTasks = dynamicForkJoinTaskList.getDynamicTasks().stream().peek(//TODO create a custom pair collector dynamicForkJoinTask -> dynamicForkJoinTasksInput.put(dynamicForkJoinTask.getReferenceName(), dynamicForkJoinTask.getInput())).map( dynamicForkJoinTask -> { WorkflowTask dynamicForkJoinWorkflowTask = new WorkflowTask(); dynamicForkJoinWorkflowTask.setTaskReferenceName(dynamicForkJoinTask.getReferenceName()); dynamicForkJoinWorkflowTask.setName(dynamicForkJoinTask.getTaskName()); dynamicForkJoinWorkflowTask.setType(dynamicForkJoinTask.getType()); if (MetadataMapperService.shouldPopulateDefinition(dynamicForkJoinWorkflowTask)) { dynamicForkJoinWorkflowTask.setTaskDefinition(metadataDAO.getTaskDef(dynamicForkJoinTask.getTaskName())); } return dynamicForkJoinWorkflowTask; }).collect(Collectors.toCollection(LinkedList::new)); return new ImmutablePair<>(dynamicForkJoinWorkflowTasks, dynamicForkJoinTasksInput); }
verify|public boolean (byte[] data, byte[] sign) { lock.lock(); try { signature.initVerify(this.publicKey); signature.update(data); return signature.verify(sign); } catch (Exception e) { throw new CryptoException(e); } finally { lock.unlock(); } }
build async|@NonNull public <K1 extends K, V1 extends V> AsyncLoadingCache<K1, V1> (@NonNull CacheLoader<? super K1, V1> loader) { return buildAsync((AsyncCacheLoader<? super K1, V1>) loader); }
adjust auto commit config|static void (Properties properties, OffsetCommitMode offsetCommitMode) { if (offsetCommitMode == OffsetCommitMode.ON_CHECKPOINTS || offsetCommitMode == OffsetCommitMode.DISABLED) { properties.setProperty(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, "false"); } }
replace where|public static void (@NonNull INDArray to, @NonNull Number set, @NonNull Condition condition) { if (!(condition instanceof BaseCondition)) throw new UnsupportedOperationException("Only static Conditions are supported"); Nd4j.getExecutioner().exec(new CompareAndSet(to, set.doubleValue(), condition)); }
reload status|public JobStatus.State () throws InterruptedException { // [START ] while (!JobStatus.State.DONE.equals(job.getStatus().getState())) { Thread.sleep(1000L); job = job.reload(BigQuery.JobOption.fields(BigQuery.JobField.STATUS)); } // [END ] return job.getStatus().getState(); }
check|public boolean () { String path = ZookeeperPathUtils.getDestinationServerRunning(destination); try { byte[] bytes = zkClient.readData(path); ServerRunningData eventData = JsonUtils.unmarshalFromByte(bytes, ServerRunningData.class); // 更新下为最新值 activeData = eventData; // 检查下nid是否为自己 boolean result = isMine(activeData.getAddress()); if (!result) { logger.warn("canal is running in node[{}] , but not in node[{}]", activeData.getCid(), serverData.getCid()); } return result; } catch (ZkNoNodeException e) { logger.warn("canal is not run any in node"); return false; } catch (ZkInterruptedException e) { logger.warn("canal check is interrupt"); Thread.interrupted(); return check(); } catch (ZkException e) { logger.warn("canal check is failed"); return false; } }
match commit to pulls|public static List<Commit> (List<Commit> commits, List<GitRequest> pullRequests) { List<Commit> newCommitList = new LinkedList<>(); if (CollectionUtils.isEmpty(commits) || CollectionUtils.isEmpty(pullRequests)) { return commits; } //TODO: Need to optimize this method for (Commit commit : commits) { Iterator<GitRequest> pIter = pullRequests.iterator(); boolean foundPull = false; while (!foundPull && pIter.hasNext()) { GitRequest pull = pIter.next(); if (Objects.equals(pull.getScmRevisionNumber(), commit.getScmRevisionNumber()) || Objects.equals(pull.getScmMergeEventRevisionNumber(), commit.getScmRevisionNumber())) { foundPull = true; commit.setPullNumber(pull.getNumber()); } else { List<Commit> prCommits = pull.getCommits(); boolean foundCommit = false; if (!CollectionUtils.isEmpty(prCommits)) { Iterator<Commit> cIter = prCommits.iterator(); while (!foundCommit && cIter.hasNext()) { Commit loopCommit = cIter.next(); if (Objects.equals(commit.getScmAuthor(), loopCommit.getScmAuthor()) && (commit.getScmCommitTimestamp() == loopCommit.getScmCommitTimestamp()) && Objects.equals(commit.getScmCommitLog(), loopCommit.getScmCommitLog())) { foundCommit = true; foundPull = true; commit.setPullNumber(pull.getNumber()); } } } } } newCommitList.add(commit); } return newCommitList; }
join|public static String (long[] array, CharSequence conjunction) { if (null == array) { return null; } final StringBuilder sb = new StringBuilder(); boolean isFirst = true; for (long item : array) { if (isFirst) { isFirst = false; } else { sb.append(conjunction); } sb.append(item); } return sb.toString(); }
remove impl|@Override protected Futures (final Futures fs) { for (Key<Model> k : _models.values()) k.remove(fs); _models.clear(); return fs; }
validate dep definition uniqueness|private void (final List<FlowTriggerDependency> dependencies) { final Set<String> seen = new HashSet<>(); for (final FlowTriggerDependency dep : dependencies) { final Map<String, String> props = dep.getProps(); // set.add() returns false when there exists duplicate Preconditions.checkArgument(seen.add(dep.getType() + ":" + props.toString()), String.format("duplicate dependency config %s found, dependency config should be unique", dep.getName())); } }
match condition|public SDVariable (SDVariable in, Condition condition) { return new MatchConditionTransform(sameDiff(), in, condition).outputVariable(); }
mark node offline|public void (@NonNull Node node) { synchronized (node) { node.status(NodeStatus.OFFLINE); for (val n : node.getDownstreamNodes()) remapNode(n); } }
contains column|public boolean (final String tableName, final String column) { return containsTable(tableName) && tables.get(tableName).getColumns().keySet().contains(column.toLowerCase()); }
compose into composite|protected final ByteBuf (ByteBufAllocator alloc, ByteBuf cumulation, ByteBuf next) { // Create a composite buffer to accumulate this pair and potentially all the buffers // in the queue. Using +2 as we have already dequeued current and next. CompositeByteBuf composite = alloc.compositeBuffer(size() + 2); try { composite.addComponent(true, cumulation); composite.addComponent(true, next); } catch (Throwable cause) { composite.release(); safeRelease(next); throwException(cause); } return composite; }
fast uuid|public static UUID () { ThreadLocalRandom random = ThreadLocalRandom.current(); return new UUID(random.nextLong(), random.nextLong()); }
remove any|@SuppressWarnings("unchecked") public static <T> Collection<T> (Collection<T> collection, T... elesRemoved) { collection.removeAll(newHashSet(elesRemoved)); return collection; }
can write in current working directory|private boolean (final String effectiveUser) throws IOException { final ExecuteAsUser executeAsUser = new ExecuteAsUser(this.getSysProps().getString(AZKABAN_SERVER_NATIVE_LIB_FOLDER)); final List<String> checkIfUserCanWriteCommand = Arrays.asList(CREATE_FILE, getWorkingDirectory() + "/" + TEMP_FILE_NAME); final int result = executeAsUser.execute(effectiveUser, checkIfUserCanWriteCommand); return result == SUCCESSFUL_EXECUTION; }
merge labels|public static Pair<INDArray, INDArray> (@NonNull INDArray[][] labelsToMerge, INDArray[][] labelMasksToMerge, int inOutIdx) { Pair<INDArray[], INDArray[]> p = selectColumnFromMDSData(labelsToMerge, labelMasksToMerge, inOutIdx); return mergeLabels(p.getFirst(), p.getSecond()); }
get all methods|public static List<Method> (Class clazz) { List<Method> all = new ArrayList<Method>(); for (Class<?> c = clazz; c != Object.class && c != null; c = c.getSuperclass()) { Method[] // 所有方法，不包含父类 methods = c.getDeclaredMethods(); for (Method method : methods) { int mod = method.getModifiers(); // native的不要 if (Modifier.isNative(mod)) { continue; } // 不管private还是protect都可以 method.setAccessible(true); all.add(method); } } return all; }
get running flows|@Override public List<ExecutableFlow> () { final ArrayList<ExecutableFlow> flows = new ArrayList<>(); try { getFlowsHelper(flows, this.executorLoader.fetchUnfinishedFlows().values()); } catch (final ExecutorManagerException e) { logger.error("Failed to get running flows.", e); } return flows; }
read string|public static String (DataInput in) throws IOException { // the length we read is offset by one, because a length of zero indicates a null value int len = in.readUnsignedByte(); if (len == 0) { return null; } if (len >= HIGH_BIT) { int shift = 7; int curr; len = len & 0x7f; while ((curr = in.readUnsignedByte()) >= HIGH_BIT) { len |= (curr & 0x7f) << shift; shift += 7; } len |= curr << shift; } // subtract one for the null length len -= 1; final char[] data = new char[len]; for (int i = 0; i < len; i++) { int c = in.readUnsignedByte(); if (c < HIGH_BIT) { data[i] = (char) c; } else { int shift = 7; int curr; c = c & 0x7f; while ((curr = in.readUnsignedByte()) >= HIGH_BIT) { c |= (curr & 0x7f) << shift; shift += 7; } c |= curr << shift; data[i] = (char) c; } } return new String(data, 0, len); }
is pre terminal|public boolean () { if (children == null && label != null && !label.equals("TOP")) children = new ArrayList<>(); if (children != null && children.size() == 1) { Tree child = children.get(0); return child != null && child.isLeaf(); } return false; }
create transaction failed|public void (String scope, String streamName) { DYNAMIC_LOGGER.incCounterValue(globalMetricName(CREATE_TRANSACTION_FAILED), 1); DYNAMIC_LOGGER.incCounterValue(CREATE_TRANSACTION_FAILED, 1, streamTags(scope, streamName)); }
encoder|public static CharsetEncoder (Charset charset) { checkNotNull(charset, "charset"); Map<Charset, CharsetEncoder> map = InternalThreadLocalMap.get().charsetEncoderCache(); CharsetEncoder e = map.get(charset); if (e != null) { e.reset().onMalformedInput(CodingErrorAction.REPLACE).onUnmappableCharacter(CodingErrorAction.REPLACE); return e; } e = encoder(charset, CodingErrorAction.REPLACE, CodingErrorAction.REPLACE); map.put(charset, e); return e; }
delete|public int (String key) { if (key == null) { return -1; } int curState = 1; int[] ids = this.charMap.toIdList(key); int[] path = new int[ids.length + 1]; int i = 0; for (; i < ids.length; i++) { int c = ids[i]; if ((getBase(curState) + c >= getBaseArraySize()) || (getCheck(getBase(curState) + c) != curState)) { break; } curState = getBase(curState) + c; path[i] = curState; } int ret = -1; if (i == ids.length) { if (getCheck(getBase(curState) + UNUSED_CHAR_VALUE) == curState) { --this.size; ret = getLeafValue(getBase(getBase(curState) + UNUSED_CHAR_VALUE)); path[(path.length - 1)] = (getBase(curState) + UNUSED_CHAR_VALUE); for (int j = path.length - 1; j >= 0; --j) { boolean isLeaf = true; int state = path[j]; for (int k = 0; k < this.charMap.getCharsetSize(); k++) { if (isLeafValue(getBase(state))) { break; } if ((getBase(state) + k < getBaseArraySize()) && (getCheck(getBase(state) + k) == state)) { isLeaf = false; break; } } if (!isLeaf) { break; } addFreeLink(state); } } } return ret; }
thread dump|public void (String reasonMsg) { logger.info("Thread dump by ThreadDumpper" + (reasonMsg != null ? (" for " + reasonMsg) : "")); Map<Thread, StackTraceElement[]> threads = Thread.getAllStackTraces(); // 两条日志间的时间间隔，是VM被thread dump堵塞的时间. logger.info("Finish the threads snapshot"); StringBuilder sb = new StringBuilder(8192 * 20).append('\n'); for (Entry<Thread, StackTraceElement[]> entry : threads.entrySet()) { dumpThreadInfo(entry.getKey(), entry.getValue(), sb); } logger.info(sb.toString()); }
connection timeout ms|public int () { long defaultNetworkTimeoutS = JavaUtils.timeStringAsSec(conf.get("spark.network.timeout", "120s")); long defaultTimeoutMs = JavaUtils.timeStringAsSec(conf.get(SPARK_NETWORK_IO_CONNECTIONTIMEOUT_KEY, defaultNetworkTimeoutS + "s")) * 1000; return (int) defaultTimeoutMs; }
get field names|public static List<String> (Class<? extends Enum<?>> clazz) { final List<String> names = new ArrayList<>(); final Field[] fields = ReflectUtil.getFields(clazz); String name; for (Field field : fields) { name = field.getName(); if (field.getType().isEnum() || name.contains("$VALUES") || "ordinal".equals(name)) { continue; } if (false == names.contains(name)) { names.add(name); } } return names; }
set description|@SuppressWarnings("WeakerAccess") public UpdateAppProfileRequest (@Nonnull String description) { Preconditions.checkNotNull(description); proto.getAppProfileBuilder().setDescription(description); updateFieldMask(com.google.bigtable.admin.v2.AppProfile.DESCRIPTION_FIELD_NUMBER); return this; }
write string lenenc|public void (final String value) { if (Strings.isNullOrEmpty(value)) { byteBuf.writeByte(0); return; } writeIntLenenc(value.getBytes().length); byteBuf.writeBytes(value.getBytes()); }
create default cell style|public static CellStyle (Workbook workbook) { final CellStyle cellStyle = workbook.createCellStyle(); setAlign(cellStyle, HorizontalAlignment.CENTER, VerticalAlignment.CENTER); setBorder(cellStyle, BorderStyle.THIN, IndexedColors.BLACK); return cellStyle; }
with float array|public Postcard (@Nullable String key, @Nullable float[] value) { mBundle.putFloatArray(key, value); return this; }
serialize|public static Slice (Geometry geometry) { requireNonNull(geometry, "input is null"); DynamicSliceOutput output = new DynamicSliceOutput(100); writeGeometry(geometry, output); return output.slice(); }
copy to file and close|public static long (InputStream is, File file) throws IOException { file.getParentFile().mkdirs(); try (OutputStream os = new BufferedOutputStream(new FileOutputStream(file))) { final long result = ByteStreams.copy(is, os); // Workarround for http://hg.openjdk.java.net/jdk8/jdk8/jdk/rev/759aa847dcaf os.flush(); return result; } finally { is.close(); } }
get|public V (String key) { int index = exactMatchSearch(key); if (index >= 0) { return getValueAt(index); } return null; }
get value|@Override public String () { return key.name(); }
get db batch|private DbBatch (HttpPipeKey key) { String dataUrl = key.getUrl(); Pipeline pipeline = configClientService.findPipeline(key.getIdentity().getPipelineId()); DataRetriever dataRetriever = dataRetrieverFactory.createRetriever(pipeline.getParameters().getRetriever(), dataUrl, downloadDir); File archiveFile = null; try { dataRetriever.connect(); dataRetriever.doRetrieve(); archiveFile = dataRetriever.getDataAsFile(); } catch (Exception e) { dataRetriever.abort(); throw new PipeException("download_error", e); } finally { dataRetriever.disconnect(); } // 处理下有加密的数据 if (StringUtils.isNotEmpty(key.getKey()) && StringUtils.isNotEmpty(key.getCrc())) { decodeFile(archiveFile, key.getKey(), key.getCrc()); } InputStream input = null; JSONReader reader = null; try { input = new BufferedInputStream(new FileInputStream(archiveFile)); DbBatch dbBatch = new DbBatch(); byte[] lengthBytes = new byte[4]; input.read(lengthBytes); int length = ByteUtils.bytes2int(lengthBytes); BatchProto.RowBatch rowbatchProto = BatchProto.RowBatch.parseFrom(new LimitedInputStream(input, length)); // 构造原始的model对象 RowBatch rowBatch = new RowBatch(); rowBatch.setIdentity(build(rowbatchProto.getIdentity())); for (BatchProto.RowData rowDataProto : rowbatchProto.getRowsList()) { EventData eventData = new EventData(); eventData.setPairId(rowDataProto.getPairId()); eventData.setTableId(rowDataProto.getTableId()); eventData.setTableName(rowDataProto.getTableName()); eventData.setSchemaName(rowDataProto.getSchemaName()); eventData.setEventType(EventType.valuesOf(rowDataProto.getEventType())); eventData.setExecuteTime(rowDataProto.getExecuteTime()); // add by ljh at 2012-10-31 if (StringUtils.isNotEmpty(rowDataProto.getSyncMode())) { eventData.setSyncMode(SyncMode.valuesOf(rowDataProto.getSyncMode())); } if (StringUtils.isNotEmpty(rowDataProto.getSyncConsistency())) { eventData.setSyncConsistency(SyncConsistency.valuesOf(rowDataProto.getSyncConsistency())); } // 处理主键 List<EventColumn> keys = new ArrayList<EventColumn>(); for (BatchProto.Column columnProto : rowDataProto.getKeysList()) { keys.add(buildColumn(columnProto)); } eventData.setKeys(keys); // 处理old主键 if (CollectionUtils.isEmpty(rowDataProto.getOldKeysList()) == false) { List<EventColumn> oldKeys = new ArrayList<EventColumn>(); for (BatchProto.Column columnProto : rowDataProto.getOldKeysList()) { oldKeys.add(buildColumn(columnProto)); } eventData.setOldKeys(oldKeys); } // 处理具体的column value List<EventColumn> columns = new ArrayList<EventColumn>(); for (BatchProto.Column columnProto : rowDataProto.getColumnsList()) { columns.add(buildColumn(columnProto)); } eventData.setColumns(columns); eventData.setRemedy(rowDataProto.getRemedy()); eventData.setSize(rowDataProto.getSize()); eventData.setSql(rowDataProto.getSql()); eventData.setDdlSchemaName(rowDataProto.getDdlSchemaName()); eventData.setHint(rowDataProto.getHint()); eventData.setWithoutSchema(rowDataProto.getWithoutSchema()); // 添加到总记录 rowBatch.merge(eventData); } dbBatch.setRowBatch(rowBatch); input.read(lengthBytes); length = ByteUtils.bytes2int(lengthBytes); BatchProto.FileBatch filebatchProto = BatchProto.FileBatch.parseFrom(new LimitedInputStream(input, length)); // 构造原始的model对象 FileBatch fileBatch = new FileBatch(); fileBatch.setIdentity(build(filebatchProto.getIdentity())); for (BatchProto.FileData fileDataProto : filebatchProto.getFilesList()) { FileData fileData = new FileData(); fileData.setPairId(fileDataProto.getPairId()); fileData.setTableId(fileDataProto.getTableId()); fileData.setEventType(EventType.valuesOf(fileDataProto.getEventType())); fileData.setLastModifiedTime(fileDataProto.getLastModifiedTime()); fileData.setNameSpace(fileDataProto.getNamespace()); fileData.setPath(fileDataProto.getPath()); fileData.setSize(fileDataProto.getSize()); // 添加到filebatch中 fileBatch.getFiles().add(fileData); } dbBatch.setFileBatch(fileBatch); return dbBatch; } catch (IOException e) { throw new PipeException("deserial_error", e); } finally { IOUtils.closeQuietly(reader); } }
register view|public void (final String type, final Pair<View, View> view) { registeredViews.put(type, view); }
record evaluation candidates|public void (List<String> evaluationCandidates) { Assert.notNull(evaluationCandidates, "evaluationCandidates must not be null"); this.unconditionalClasses.addAll(evaluationCandidates); }
snapshot state|public HashMap<StreamShardMetadata, SequenceNumber> () { // this method assumes that the checkpoint lock is held assert Thread.holdsLock(checkpointLock); HashMap<StreamShardMetadata, SequenceNumber> stateSnapshot = new HashMap<>(); for (KinesisStreamShardState shardWithState : subscribedShardsState) { stateSnapshot.put(shardWithState.getStreamShardMetadata(), shardWithState.getLastProcessedSequenceNum()); } return stateSnapshot; }
is absolute|public boolean () { final int start = hasWindowsDrive(uri.getPath(), true) ? 3 : 0; return uri.getPath().startsWith(SEPARATOR, start); }
get string value|public static String (String primaryKey) { String val = (String) CFG.get(primaryKey); if (val == null) { throw new SofaRpcRuntimeException("Not Found Key: " + primaryKey); } else { return val; } }
child|public SpringApplicationBuilder (Class<?>... sources) { SpringApplicationBuilder child = new SpringApplicationBuilder(); child.sources(sources); // Copy environment stuff from parent to child child.properties(this.defaultProperties).environment(this.environment).additionalProfiles(this.additionalProfiles); child.parent = this; // It's not possible if embedded web server are enabled to support web contexts as // parents because the servlets cannot be initialized at the right point in // lifecycle. web(WebApplicationType.NONE); // Probably not interested in multiple banners bannerMode(Banner.Mode.OFF); // Make sure sources get copied over this.application.addPrimarySources(this.sources); return child; }
with executor|public CallOptions (@Nullable Executor executor) { CallOptions newOptions = new CallOptions(this); newOptions.executor = executor; return newOptions; }
build class path|List<String> (String appClassPath) throws IOException { String sparkHome = getSparkHome(); Set<String> cp = new LinkedHashSet<>(); addToClassPath(cp, appClassPath); addToClassPath(cp, getConfDir()); boolean prependClasses = !isEmpty(getenv("SPARK_PREPEND_CLASSES")); boolean isTesting = "1".equals(getenv("SPARK_TESTING")); if (prependClasses || isTesting) { String scala = getScalaVersion(); List<String> projects = Arrays.asList("common/kvstore", "common/network-common", "common/network-shuffle", "common/network-yarn", "common/sketch", "common/tags", "common/unsafe", "core", "examples", "graphx", "launcher", "mllib", "repl", "resource-managers/mesos", "resource-managers/yarn", "sql/catalyst", "sql/core", "sql/hive", "sql/hive-thriftserver", "streaming"); if (prependClasses) { if (!isTesting) { System.err.println("NOTE: SPARK_PREPEND_CLASSES is set, placing locally compiled Spark classes ahead of " + "assembly."); } for (String project : projects) { addToClassPath(cp, String.format("%s/%s/target/scala-%s/classes", sparkHome, project, scala)); } } if (isTesting) { for (String project : projects) { addToClassPath(cp, String.format("%s/%s/target/scala-%s/test-classes", sparkHome, project, scala)); } } // Add this path to include jars that are shaded in the final deliverable created during // the maven build. These jars are copied to this directory during the build. addToClassPath(cp, String.format("%s/core/target/jars/*", sparkHome)); addToClassPath(cp, String.format("%s/mllib/target/jars/*", sparkHome)); } // Add Spark jars to the classpath. For the testing case, we rely on the test code to set and // propagate the test classpath appropriately. For normal invocation, look for the jars // directory under SPARK_HOME. boolean isTestingSql = "1".equals(getenv("SPARK_SQL_TESTING")); String jarsDir = findJarsDir(getSparkHome(), getScalaVersion(), !isTesting && !isTestingSql); if (jarsDir != null) { addToClassPath(cp, join(File.separator, jarsDir, "*")); } addToClassPath(cp, getenv("HADOOP_CONF_DIR")); addToClassPath(cp, getenv("YARN_CONF_DIR")); addToClassPath(cp, getenv("SPARK_DIST_CLASSPATH")); return new ArrayList<>(cp); }
find frame nodes|public static FrameNodes (Frame fr) { // Count on how many nodes the data resides boolean[] nodesHoldingFrame = new boolean[H2O.CLOUD.size()]; Vec vec = fr.anyVec(); for (int chunkNr = 0; chunkNr < vec.nChunks(); chunkNr++) { int home = vec.chunkKey(chunkNr).home_node().index(); if (!nodesHoldingFrame[home]) nodesHoldingFrame[home] = true; } return new FrameNodes(fr, nodesHoldingFrame); }
relu|public SDVariable (String name, SDVariable x, double cutoff) { validateFloatingPoint("ReLU", x); SDVariable result = f().relu(x, cutoff); return updateVariableNameAndReference(result, name); }
get float|public static Float (Map<?, ?> map, Object key) { return get(map, key, Float.class); }
get kind for scheme|static FileSystemKind (String scheme) { scheme = scheme.toLowerCase(Locale.US); if (scheme.startsWith("s3") || scheme.startsWith("emr") || scheme.startsWith("oss")) { // the Amazon S3 storage or Aliyun OSS storage return FileSystemKind.OBJECT_STORE; } else if (scheme.startsWith("http") || scheme.startsWith("ftp")) { // currently to rely on that return FileSystemKind.OBJECT_STORE; } else { // this also includes federated HDFS (viewfs). return FileSystemKind.FILE_SYSTEM; } }
start|public void (JobLeaderIdActions initialJobLeaderIdActions) throws Exception { if (isStarted()) { clear(); } this.jobLeaderIdActions = Preconditions.checkNotNull(initialJobLeaderIdActions); }
shutdown|public static void (int status) { if (status == 0) H2O.orderlyShutdown(); UDPRebooted.T.error.send(H2O.SELF); H2O.exit(status); }
axpy|@Override public void (long n, double alpha, INDArray x, INDArray y) { BaseSparseNDArray sparseX = (BaseSparseNDArray) x; DataBuffer pointers = sparseX.getVectorCoordinates(); switch(x.data().dataType()) { case DOUBLE: DefaultOpExecutioner.validateDataType(DataType.DOUBLE, x); DefaultOpExecutioner.validateDataType(DataType.DOUBLE, y); daxpyi(n, alpha, x, pointers, y); break; case FLOAT: DefaultOpExecutioner.validateDataType(DataType.FLOAT, x); DefaultOpExecutioner.validateDataType(DataType.FLOAT, y); saxpyi(n, alpha, x, pointers, y); break; case HALF: DefaultOpExecutioner.validateDataType(DataType.HALF, x); DefaultOpExecutioner.validateDataType(DataType.HALF, y); haxpyi(n, alpha, x, pointers, y); break; default: throw new UnsupportedOperationException(); } }
configure|@Override protected void (ServletRegistration.Dynamic registration) { super.configure(registration); String[] urlMapping = StringUtils.toStringArray(this.urlMappings); if (urlMapping.length == 0 && this.alwaysMapUrl) { urlMapping = DEFAULT_MAPPINGS; } if (!ObjectUtils.isEmpty(urlMapping)) { registration.addMapping(urlMapping); } registration.setLoadOnStartup(this.loadOnStartup); if (this.multipartConfig != null) { registration.setMultipartConfig(this.multipartConfig); } }
hash|@SuppressWarnings("unchecked") @Override public int (T value) { int i = 0; try { int code = this.comparators[0].hash(value.getFieldNotNull(keyPositions[0])); for (i = 1; i < this.keyPositions.length; i++) { // salt code with (i % HASH_SALT.length)-th salt component code *= HASH_SALT[i & 0x1F]; code += this.comparators[i].hash(value.getFieldNotNull(keyPositions[i])); } return code; } catch (NullFieldException nfex) { throw new NullKeyFieldException(nfex); } catch (IndexOutOfBoundsException iobex) { throw new KeyFieldOutOfBoundsException(keyPositions[i]); } }
delete|@Override public void (String id) throws SQLException { Connection c = dataSource.getConnection(); PreparedStatement p = c.prepareStatement(deleteStatement()); p.setString(1, id); p.execute(); }
score|public static double (MultiLayerNetwork model, DataSetIterator testSet, RegressionValue regressionValue) { RegressionEvaluation eval = model.evaluateRegression(testSet); return getScoreFromRegressionEval(eval, regressionValue); }
exception listener|public static <T> void (CompletableFuture<T> completableFuture, Consumer<Throwable> exceptionListener) { completableFuture.whenComplete(( r, ex) -> { if (ex != null) { Callbacks.invokeSafely(exceptionListener, ex, null); } }); }
act|public <T> T (final FileCallable<T> callable) throws IOException, InterruptedException { return act(callable, callable.getClass().getClassLoader()); }
write as csv|public DataSink<T> (String filePath, String rowDelimiter, String fieldDelimiter, WriteMode writeMode) { return internalWriteAsCsv(new Path(filePath), rowDelimiter, fieldDelimiter, writeMode); }
generate error view|private ModelAndView (final String code, final Object[] args) { val modelAndView = new ModelAndView(this.failureView); val convertedDescription = getMessageSourceAccessor().getMessage(code, args, code); modelAndView.addObject("code", StringEscapeUtils.escapeHtml4(code)); modelAndView.addObject("description", StringEscapeUtils.escapeHtml4(convertedDescription)); return modelAndView; }
set allow circular redirects|public void (boolean allow) { client.getParams().setBooleanParameter(HttpClientParams.ALLOW_CIRCULAR_REDIRECTS, allow); clientViaProxy.getParams().setBooleanParameter(HttpClientParams.ALLOW_CIRCULAR_REDIRECTS, allow); }
exceptionally compose expecting|public static <T> CompletableFuture<T> (CompletableFuture<T> future, Predicate<Throwable> isExpected, Supplier<CompletableFuture<T>> exceptionFutureSupplier) { return exceptionallyCompose(future, ex -> { if (isExpected.test(Exceptions.unwrap(ex))) { return exceptionFutureSupplier.get(); } else { return Futures.failedFuture(ex); } }); }
write json impl|public final AutoBuffer (AutoBuffer ab) { ab.putJSON("job", job); ab.put1(','); ab.putJSONStr("algo", algo); ab.put1(','); ab.putJSONStr("algo_full_name", algo_full_name); ab.put1(','); ab.putJSONAEnum("can_build", can_build); ab.put1(','); ab.putJSONEnum("visibility", visibility); ab.put1(','); ab.putJSONZ("supervised", supervised); ab.put1(','); ab.putJSONA("messages", messages); ab.put1(','); ab.putJSON4("error_count", error_count); ab.put1(','); // Builds ModelParameterSchemaV2 objects for each field, and then calls writeJSON on the array ModelParametersSchemaV3.writeParametersJSON(ab, parameters, createParametersSchema().fillFromImpl((Model.Parameters) parameters.createImpl())); return ab; }
new ldaptive connection config|public static ConnectionConfig (final AbstractLdapProperties l) { if (StringUtils.isBlank(l.getLdapUrl())) { throw new IllegalArgumentException("LDAP url cannot be empty/blank"); } LOGGER.debug("Creating LDAP connection configuration for [{}]", l.getLdapUrl()); val cc = new ConnectionConfig(); val urls = l.getLdapUrl().contains(" ") ? l.getLdapUrl() : String.join(" ", l.getLdapUrl().split(",")); LOGGER.debug("Transformed LDAP urls from [{}] to [{}]", l.getLdapUrl(), urls); cc.setLdapUrl(urls); cc.setUseSSL(l.isUseSsl()); cc.setUseStartTLS(l.isUseStartTls()); cc.setConnectTimeout(Beans.newDuration(l.getConnectTimeout())); cc.setResponseTimeout(Beans.newDuration(l.getResponseTimeout())); if (StringUtils.isNotBlank(l.getConnectionStrategy())) { val strategy = AbstractLdapProperties.LdapConnectionStrategy.valueOf(l.getConnectionStrategy()); switch(strategy) { case RANDOM: cc.setConnectionStrategy(new RandomConnectionStrategy()); break; case DNS_SRV: cc.setConnectionStrategy(new DnsSrvConnectionStrategy()); break; case ACTIVE_PASSIVE: cc.setConnectionStrategy(new ActivePassiveConnectionStrategy()); break; case ROUND_ROBIN: cc.setConnectionStrategy(new RoundRobinConnectionStrategy()); break; case DEFAULT: default: cc.setConnectionStrategy(new DefaultConnectionStrategy()); break; } } if (l.getTrustCertificates() != null) { LOGGER.debug("Creating LDAP SSL configuration via trust certificates [{}]", l.getTrustCertificates()); val cfg = new X509CredentialConfig(); cfg.setTrustCertificates(l.getTrustCertificates()); cc.setSslConfig(new SslConfig(cfg)); } else if (l.getTrustStore() != null || l.getKeystore() != null) { val cfg = new KeyStoreCredentialConfig(); if (l.getTrustStore() != null) { LOGGER.trace("Creating LDAP SSL configuration with truststore [{}]", l.getTrustStore()); cfg.setTrustStore(l.getTrustStore()); cfg.setTrustStoreType(l.getTrustStoreType()); cfg.setTrustStorePassword(l.getTrustStorePassword()); } if (l.getKeystore() != null) { LOGGER.trace("Creating LDAP SSL configuration via keystore [{}]", l.getKeystore()); cfg.setKeyStore(l.getKeystore()); cfg.setKeyStorePassword(l.getKeystorePassword()); cfg.setKeyStoreType(l.getKeystoreType()); } cc.setSslConfig(new SslConfig(cfg)); } else { LOGGER.debug("Creating LDAP SSL configuration via the native JVM truststore"); cc.setSslConfig(new SslConfig()); } val sslConfig = cc.getSslConfig(); if (sslConfig != null) { switch(l.getHostnameVerifier()) { case ANY: sslConfig.setHostnameVerifier(new AllowAnyHostnameVerifier()); break; case DEFAULT: default: sslConfig.setHostnameVerifier(new DefaultHostnameVerifier()); break; } } if (StringUtils.isNotBlank(l.getSaslMechanism())) { LOGGER.debug("Creating LDAP SASL mechanism via [{}]", l.getSaslMechanism()); val bc = new BindConnectionInitializer(); val sc = getSaslConfigFrom(l); if (StringUtils.isNotBlank(l.getSaslAuthorizationId())) { sc.setAuthorizationId(l.getSaslAuthorizationId()); } sc.setMutualAuthentication(l.getSaslMutualAuth()); if (StringUtils.isNotBlank(l.getSaslQualityOfProtection())) { sc.setQualityOfProtection(QualityOfProtection.valueOf(l.getSaslQualityOfProtection())); } if (StringUtils.isNotBlank(l.getSaslSecurityStrength())) { sc.setSecurityStrength(SecurityStrength.valueOf(l.getSaslSecurityStrength())); } bc.setBindSaslConfig(sc); cc.setConnectionInitializer(bc); } else if (StringUtils.equals(l.getBindCredential(), "*") && StringUtils.equals(l.getBindDn(), "*")) { LOGGER.debug("Creating LDAP fast-bind connection initializer"); cc.setConnectionInitializer(new FastBindOperation.FastBindConnectionInitializer()); } else if (StringUtils.isNotBlank(l.getBindDn()) && StringUtils.isNotBlank(l.getBindCredential())) { LOGGER.debug("Creating LDAP bind connection initializer via [{}]", l.getBindDn()); cc.setConnectionInitializer(new BindConnectionInitializer(l.getBindDn(), new Credential(l.getBindCredential()))); } return cc; }
destroy all buffer pools|public void () { synchronized (factoryLock) { // create a copy to avoid concurrent modification exceptions LocalBufferPool[] poolsCopy = allBufferPools.toArray(new LocalBufferPool[allBufferPools.size()]); for (LocalBufferPool pool : poolsCopy) { pool.lazyDestroy(); } // some sanity checks if (allBufferPools.size() > 0 || numTotalRequiredBuffers > 0) { throw new IllegalStateException("NetworkBufferPool is not empty after destroying all LocalBufferPools"); } } }
write schema|public static void (String outputPath, Schema schema, JavaSparkContext sc) throws IOException { writeStringToFile(outputPath, schema.toString(), sc); }
for server|public static SslContextBuilder (PrivateKey key, X509Certificate... keyCertChain) { return new SslContextBuilder(true).keyManager(key, keyCertChain); }
find main class|public static String (JarFile jarFile, String classesLocation) throws IOException { return doWithMainClasses(jarFile, classesLocation, MainClass::getName); }
add|@Override public boolean (T element) { if (size < elementData.length) { elementData[size++] = element; } else { // overflow-conscious code final int oldCapacity = elementData.length; final int newCapacity = oldCapacity << 1; @SuppressWarnings("unchecked") final T[] newElementData = (T[]) Array.newInstance(clazz, newCapacity); System.arraycopy(elementData, 0, newElementData, 0, oldCapacity); newElementData[size++] = element; elementData = newElementData; } return true; }
create result|public <T> DynamicResult<T> (Environment env, TableSchema schema, ExecutionConfig config) { final RowTypeInfo outputType = new RowTypeInfo(schema.getFieldTypes(), schema.getFieldNames()); if (env.getExecution().isStreamingExecution()) { // determine gateway address (and port if possible) final InetAddress gatewayAddress = getGatewayAddress(env.getDeployment()); final int gatewayPort = getGatewayPort(env.getDeployment()); if (env.getExecution().isChangelogMode()) { return new ChangelogCollectStreamResult<>(outputType, config, gatewayAddress, gatewayPort); } else { return new MaterializedCollectStreamResult<>(outputType, config, gatewayAddress, gatewayPort, env.getExecution().getMaxTableResultRows()); } } else { // Batch Execution if (!env.getExecution().isTableMode()) { throw new SqlExecutionException("Results of batch queries can only be served in table mode."); } return new MaterializedCollectBatchResult<>(outputType, config); } }
to date str|public String () { if (null != this.timeZone) { final SimpleDateFormat simpleDateFormat = new SimpleDateFormat(DatePattern.NORM_DATE_PATTERN); simpleDateFormat.setTimeZone(this.timeZone); return toString(simpleDateFormat); } return toString(DatePattern.NORM_DATE_FORMAT); }
makesure dir exists|public static void (Path dirPath) throws IOException { Validate.notNull(dirPath); Files.createDirectories(dirPath); }
next buffer for frame|private ByteBuf (int bytesToRead) { ByteBuf buf = buffers.getFirst(); ByteBuf frame; if (buf.readableBytes() > bytesToRead) { frame = buf.retain().readSlice(bytesToRead); totalSize -= bytesToRead; } else { frame = buf; buffers.removeFirst(); totalSize -= frame.readableBytes(); } return frame; }
add pkcs button action performed|private void (java.awt.event.ActionEvent evt) { //GEN-FIRST:event_addPkcs11ButtonActionPerformed String name = null; try { final int indexSelectedDriver = driverComboBox.getSelectedIndex(); name = driverConfig.getNames().get(indexSelectedDriver); if (name.equals("")) { return; } String library = driverConfig.getPaths().get(indexSelectedDriver); if (library.equals("")) { return; } int slot = driverConfig.getSlots().get(indexSelectedDriver); if (slot < 0) { return; } int slotListIndex = driverConfig.getSlotIndexes().get(indexSelectedDriver); if (slotListIndex < 0) { return; } String kspass = new String(pkcs11PasswordField.getPassword()); if (kspass.equals("")) { kspass = null; } PCKS11ConfigurationBuilder confBuilder = PKCS11Configuration.builder(); confBuilder.setName(name).setLibrary(library); if (usePkcs11ExperimentalSliSupportCheckBox.isSelected()) { confBuilder.setSlotListIndex(slotListIndex); } else { confBuilder.setSlotId(slot); } int ksIndex = contextManager.initPKCS11(confBuilder.build(), kspass); if (ksIndex == -1) { logger.error("The required PKCS#11 provider is not available (" + SSLContextManager.SUN_PKCS11_CANONICAL_CLASS_NAME + " or " + SSLContextManager.IBM_PKCS11_CONONICAL_CLASS_NAME + ")."); showErrorMessageSunPkcs11ProviderNotAvailable(); return; } // The PCKS11 driver/smartcard was initialized properly: reset login attempts login_attempts = 0; keyStoreListModel.insertElementAt(contextManager.getKeyStoreDescription(ksIndex), ksIndex); // Issue 182 retry = true; certificatejTabbedPane.setSelectedIndex(0); activateFirstOnlyAliasOfKeyStore(ksIndex); driverComboBox.setSelectedIndex(-1); pkcs11PasswordField.setText(""); } catch (InvocationTargetException e) { if (e.getCause() instanceof ProviderException) { if ("Error parsing configuration".equals(e.getCause().getMessage())) { logAndShowGenericErrorMessagePkcs11CouldNotBeAdded(false, name, e); } else if ("Initialization failed".equals(e.getCause().getMessage())) { if (retry) { retry = false; addPkcs11ButtonActionPerformed(evt); } else { JOptionPane.showMessageDialog(null, new String[] { Constant.messages.getString("options.cert.error"), Constant.messages.getString("options.cert.error.pkcs11") }, Constant.messages.getString("options.cert.label.client.cert"), JOptionPane.ERROR_MESSAGE); retry = true; logger.warn("Couldn't add key from " + name, e); } } else { logAndShowGenericErrorMessagePkcs11CouldNotBeAdded(false, name, e); } } else { logAndShowGenericErrorMessagePkcs11CouldNotBeAdded(false, name, e); } } catch (java.io.IOException e) { if (e.getMessage().equals("load failed") && e.getCause().getClass().getName().equals("javax.security.auth.login.FailedLoginException")) { login_attempts++; String attempts = " (" + login_attempts + "/" + MAX_LOGIN_ATTEMPTS + ") "; if (login_attempts == (MAX_LOGIN_ATTEMPTS - 1)) { JOptionPane.showMessageDialog(null, new String[] { Constant.messages.getString("options.cert.error"), Constant.messages.getString("options.cert.error.wrongpassword"), Constant.messages.getString("options.cert.error.wrongpasswordlast"), attempts }, Constant.messages.getString("options.cert.label.client.cert"), JOptionPane.ERROR_MESSAGE); logger.warn("PKCS#11: Incorrect PIN or password" + attempts + ": " + name + " *LAST TRY BEFORE BLOCKING*"); } else { JOptionPane.showMessageDialog(null, new String[] { Constant.messages.getString("options.cert.error"), Constant.messages.getString("options.cert.error.wrongpassword"), attempts }, Constant.messages.getString("options.cert.label.client.cert"), JOptionPane.ERROR_MESSAGE); logger.warn("PKCS#11: Incorrect PIN or password" + attempts + ": " + name); } } else { logAndShowGenericErrorMessagePkcs11CouldNotBeAdded(false, name, e); } } catch (KeyStoreException e) { logAndShowGenericErrorMessagePkcs11CouldNotBeAdded(false, name, e); } catch (Exception e) { logAndShowGenericErrorMessagePkcs11CouldNotBeAdded(true, name, e); } }
list interconnect locations|@BetaApi public final ListInterconnectLocationsPagedResponse (String project) { ListInterconnectLocationsHttpRequest request = ListInterconnectLocationsHttpRequest.newBuilder().setProject(project).build(); return listInterconnectLocations(request); }
parameterize|private void (Parameterized parameterized) { try { parameterized.configure(parameters); } catch (RuntimeException ex) { throw new ProgramParametrizationException(ex.getMessage()); } }
with input subgraph|public SubGraphPredicate (int inputNum, @NonNull OpPredicate opPredicate) { opInputSubgraphPredicates.put(inputNum, opPredicate); return this; }
add bias param|public void (@NonNull String paramKey, @NonNull long... paramShape) { Preconditions.checkArgument(paramShape.length > 0, "Provided mia- parameter shape is" + " invalid: length 0 provided for shape. Parameter: " + paramKey); biasParams.put(paramKey, paramShape); paramsList = null; weightParamsList = null; biasParamsList = null; }
get register config|private AbstractInterfaceConfig (ConsumerConfig config) { String url = ZookeeperRegistryHelper.convertConsumerToUrl(config); String addr = url.substring(0, url.indexOf("?")); for (Map.Entry<ConsumerConfig, String> consumerUrl : consumerUrls.entrySet()) { if (consumerUrl.getValue().contains(addr)) { return consumerUrl.getKey(); } } return null; }
insert global address|@BetaApi public final Operation (ProjectName project, Address addressResource) { InsertGlobalAddressHttpRequest request = InsertGlobalAddressHttpRequest.newBuilder().setProject(project == null ? null : project.toString()).setAddressResource(addressResource).build(); return insertGlobalAddress(request); }
find smallest|private V (Node node) { if (node == null) { return null; } while (node.left != null) { node = node.left; } return node.item; }
decode bulk string end of line|private boolean (ByteBuf in, List<Object> out) throws Exception { if (in.readableBytes() < RedisConstants.EOL_LENGTH) { return false; } readEndOfLine(in); out.add(FullBulkStringRedisMessage.EMPTY_INSTANCE); resetDecoder(); return true; }
execute query|@Override public ResultSet () throws SQLException { connection.markCommitStateDirty(); ResultSet resultSet = ((PreparedStatement) delegate).executeQuery(); return ProxyFactory.getProxyResultSet(connection, this, resultSet); }
save|public synchronized void () { if (BulkChange.contains(this)) return; try { getConfigFile().write(sites); SaveableListener.fireOnChange(this, getConfigFile()); } catch (IOException e) { LOGGER.log(Level.WARNING, "Failed to save " + getConfigFile(), e); } }
resize|private int (int newSize) { int[] base2 = new int[newSize]; int[] check2 = new int[newSize]; if (allocSize > 0) { System.arraycopy(base, 0, base2, 0, allocSize); System.arraycopy(check, 0, check2, 0, allocSize); } base = base2; check = check2; return allocSize = newSize; }
do log rss|@Deprecated public void (StaplerRequest req, StaplerResponse rsp) throws IOException, ServletException { String qs = req.getQueryString(); rsp.sendRedirect2("./log/rss" + (qs == null ? "" : '?' + qs)); }
assign|public SDVariable (SDVariable in, Number value) { return assign(null, in, value); }
get types|private static Class<?>[] (final Object... args) { final Class<?>[] argTypes = new Class<?>[args.length]; for (int i = 0; i < argTypes.length; i++) { argTypes[i] = args[i].getClass(); } return argTypes; }
compatible with reconfigured serializer|public static <T> TypeSerializerSchemaCompatibility<T> (TypeSerializer<T> reconfiguredSerializer) { return new TypeSerializerSchemaCompatibility<>(Type.COMPATIBLE_WITH_RECONFIGURED_SERIALIZER, Preconditions.checkNotNull(reconfiguredSerializer)); }
get named config|public static IClientConfig (String name, Class<? extends IClientConfig> clientConfigClass) { return getNamedConfig(name, factoryFromConfigType(clientConfigClass)); }
read header|public String () throws IOException { byte[] buffer = new byte[BUFFER_SIZE]; StringBuilder content = new StringBuilder(BUFFER_SIZE); while (content.indexOf(HEADER_END) == -1) { int amountRead = checkedRead(buffer, 0, BUFFER_SIZE); content.append(new String(buffer, 0, amountRead)); } return content.substring(0, content.indexOf(HEADER_END)); }
iamax|@Override public int (INDArray arr) { if (arr.isSparse()) { return Nd4j.getSparseBlasWrapper().level1().iamax(arr); } if (Nd4j.getExecutioner().getProfilingMode() == OpExecutioner.ProfilingMode.ALL) OpProfiler.getInstance().processBlasCall(false, arr); if (arr.data().dataType() == DataType.DOUBLE) { DefaultOpExecutioner.validateDataType(DataType.DOUBLE, arr); return idamax(arr.length(), arr, BlasBufferUtil.getBlasStride(arr)); } else { DefaultOpExecutioner.validateDataType(DataType.FLOAT, arr); return isamax(arr.length(), arr, BlasBufferUtil.getBlasStride(arr)); } }
calculate average auc|public double () { double ret = 0.0; for (int i = 0; i < numLabels(); i++) { ret += calculateAUC(i); } return ret / (double) numLabels(); }
reach end|public boolean () throws IOException { // check if we have a read row that was not returned yet if (readRecord != null && !readRecordReturned) { return false; } // check if there are more rows to be read if (numReadRecords >= numTotalRecords) { return true; } // try to read next row return !readNextRecord(); }
extract in memory metric emitter|private InMemoryMetricEmitter (final MetricReportManager metricManager) { InMemoryMetricEmitter memoryEmitter = null; for (final IMetricEmitter emitter : metricManager.getMetricEmitters()) { if (emitter instanceof InMemoryMetricEmitter) { memoryEmitter = (InMemoryMetricEmitter) emitter; break; } } return memoryEmitter; }
check and enable view button|private void () { boolean enabled = true; enabled &= Desktop.isDesktopSupported(); enabled &= txt_PubCert.getDocument().getLength() > MIN_CERT_LENGTH; bt_view.setEnabled(enabled); }
is button enabled for history reference|@Override protected boolean (HistoryReference historyReference) { final SiteNode siteNode = getSiteNode(historyReference); if (siteNode != null && !isButtonEnabledForSiteNode(siteNode)) { return false; } return true; }
deserialize response|public RESP (final ByteBuf buf) { Preconditions.checkNotNull(buf); return responseDeserializer.deserializeMessage(buf); }
open|public static void (File file) { final Desktop dsktop = getDsktop(); try { dsktop.open(file); } catch (IOException e) { throw new IORuntimeException(e); } }
get type|public TypeInformation<T> () { if (type instanceof MissingTypeInfo) { MissingTypeInfo typeInfo = (MissingTypeInfo) type; throw new InvalidTypesException("The return type of function '" + typeInfo.getFunctionName() + "' could not be determined automatically, due to type erasure. " + "You can give type information hints by using the returns(...) method on the result of " + "the transformation call, or by letting your function implement the 'ResultTypeQueryable' " + "interface.", typeInfo.getTypeException()); } typeUsed = true; return this.type; }
parse|public void (String s) { Preconditions.checkNotNull(s, "s"); if (s.length() == 0) { // Nothing to do. return; } val pairs = s.split(pairDelimiter); for (String pair : pairs) { int delimiterPos = pair.indexOf(keyValueDelimiter); if (delimiterPos < 0) { throw new IllegalArgumentException(String.format("Invalid pair '%s' (missing key-value delimiter).", pair)); } String key = pair.substring(0, delimiterPos); String value; if (delimiterPos == pair.length() - 1) { value = ""; } else { value = pair.substring(delimiterPos + 1); } Extractor<?> e = this.extractors.get(key); Preconditions.checkArgument(e != null, String.format("No extractor provided for key '%s'.", key)); e.process(value); } }
set props in node bean|public static NodeBean (final String path, final File flowFile, final Props prop) { final NodeBeanLoader loader = new NodeBeanLoader(); try { final NodeBean nodeBean = loader.load(flowFile); final String[] pathList = path.split(Constants.PATH_DELIMITER); if (overridePropsInNodeBean(nodeBean, pathList, 0, prop)) { return nodeBean; } else { logger.error("Error setting props for " + path); } } catch (final Exception e) { logger.error("Failed to set props, error loading flow YAML file " + flowFile); } return null; }
max|public static INDArray (INDArray first, INDArray second) { return max(first, second, true); }
finish|public void (Promise<Void> aggregatePromise) { ObjectUtil.checkNotNull(aggregatePromise, "aggregatePromise"); checkInEventLoop(); if (this.aggregatePromise != null) { throw new IllegalStateException("Already finished"); } this.aggregatePromise = aggregatePromise; if (doneCount == expectedCount) { tryPromise(); } }
close class loader|public boolean (final ClassLoader cl) throws ValidatorManagerException { boolean res = false; if (cl == null) { return res; } final Class classURLClassLoader = URLClassLoader.class; Field f = null; try { f = classURLClassLoader.getDeclaredField("ucp"); } catch (final NoSuchFieldException e) { throw new ValidatorManagerException(e); } if (f != null) { f.setAccessible(true); Object obj = null; try { obj = f.get(cl); } catch (final IllegalAccessException e) { throw new ValidatorManagerException(e); } if (obj != null) { final Object ucp = obj; f = null; try { f = ucp.getClass().getDeclaredField("loaders"); } catch (final NoSuchFieldException e) { throw new ValidatorManagerException(e); } if (f != null) { f.setAccessible(true); ArrayList loaders = null; try { loaders = (ArrayList) f.get(ucp); res = true; } catch (final IllegalAccessException e) { throw new ValidatorManagerException(e); } for (int i = 0; loaders != null && i < loaders.size(); i++) { obj = loaders.get(i); f = null; try { f = obj.getClass().getDeclaredField("jar"); } catch (final NoSuchFieldException e) { throw new ValidatorManagerException(e); } if (f != null) { f.setAccessible(true); try { obj = f.get(obj); } catch (final IllegalAccessException e) { throw new ValidatorManagerException(e); } if (obj instanceof JarFile) { final JarFile jarFile = (JarFile) obj; this.setJarFileNames2Close.add(jarFile.getName()); try { jarFile.close(); } catch (final IOException e) { throw new ValidatorManagerException(e); } } } } } } } return res; }
row signature for|public static Map<String, ValueType> (final GroupByQuery query) { final ImmutableMap.Builder<String, ValueType> types = ImmutableMap.builder(); for (DimensionSpec dimensionSpec : query.getDimensions()) { types.put(dimensionSpec.getOutputName(), dimensionSpec.getOutputType()); } for (AggregatorFactory aggregatorFactory : query.getAggregatorSpecs()) { final String typeName = aggregatorFactory.getTypeName(); final ValueType valueType; if (typeName != null) { valueType = GuavaUtils.getEnumIfPresent(ValueType.class, StringUtils.toUpperCase(typeName)); } else { valueType = null; } if (valueType != null) { types.put(aggregatorFactory.getName(), valueType); } } // Don't include post-aggregators since we don't know what types they are. return types.build(); }
parse trace key|public static void (Map<String, String> tracerMap, String key, String value) { String lowKey = key.substring(PREFIX.length()); String realKey = TRACER_KEY_MAP.get(lowKey); tracerMap.put(realKey == null ? lowKey : realKey, value); }
generate access token|public final GenerateAccessTokenResponse (ServiceAccountName name, List<String> delegates, List<String> scope, Duration lifetime) { GenerateAccessTokenRequest request = GenerateAccessTokenRequest.newBuilder().setName(name == null ? null : name.toString()).addAllDelegates(delegates).addAllScope(scope).setLifetime(lifetime).build(); return generateAccessToken(request); }
permute|public SDVariable (String name, SDVariable x, int... dimensions) { SDVariable result = f().permute(x, dimensions); return updateVariableNameAndReference(result, name); }
annotate video async|@BetaApi("The surface for long-running operations is not stable yet and may change in the future.") public final OperationFuture<AnnotateVideoResponse, AnnotateVideoProgress> (String inputUri, List<Feature> features) { AnnotateVideoRequest request = AnnotateVideoRequest.newBuilder().setInputUri(inputUri).addAllFeatures(features).build(); return annotateVideoAsync(request); }
pageable convention|@Bean public AlternateTypeRuleConvention (final TypeResolver resolver, final RepositoryRestConfiguration restConfiguration) { return new AlternateTypeRuleConvention() { @Override public int getOrder() { return Ordered.HIGHEST_PRECEDENCE; } @Override public List<AlternateTypeRule> rules() { return singletonList(newRule(resolver.resolve(Pageable.class), resolver.resolve(pageableMixin(restConfiguration)))); } }; }
get date|@Nullable public Date (@Nonnull String field) { return ((Timestamp) get(field)).toDate(); }
get relative name from|static String (String itemFullName, String groupFullName) { String[] itemFullNameA = itemFullName.isEmpty() ? MemoryReductionUtil.EMPTY_STRING_ARRAY : itemFullName.split("/"); String[] groupFullNameA = groupFullName.isEmpty() ? MemoryReductionUtil.EMPTY_STRING_ARRAY : groupFullName.split("/"); for (int i = 0; ; i++) { if (i == itemFullNameA.length) { if (i == groupFullNameA.length) { // itemFullName and groupFullName are identical return "."; } else { // itemFullName is an ancestor of groupFullName; insert ../ for rest of groupFullName StringBuilder b = new StringBuilder(); for (int j = 0; j < groupFullNameA.length - itemFullNameA.length; j++) { if (j > 0) { b.append('/'); } b.append(".."); } return b.toString(); } } else if (i == groupFullNameA.length) { // groupFullName is an ancestor of itemFullName; insert rest of itemFullName StringBuilder b = new StringBuilder(); for (int j = i; j < itemFullNameA.length; j++) { if (j > i) { b.append('/'); } b.append(itemFullNameA[j]); } return b.toString(); } else if (itemFullNameA[i].equals(groupFullNameA[i])) { // identical up to this point continue; } else { // first mismatch; insert ../ for rest of groupFullName, then rest of itemFullName StringBuilder b = new StringBuilder(); for (int j = i; j < groupFullNameA.length; j++) { if (j > i) { b.append('/'); } b.append(".."); } for (int j = i; j < itemFullNameA.length; j++) { b.append('/').append(itemFullNameA[j]); } return b.toString(); } } }
all|public static DescriptorExtensionList<BuildWrapper, Descriptor<BuildWrapper>> () { // use getDescriptorList and not getExtensionList to pick up legacy instances return Jenkins.getInstance().<BuildWrapper, Descriptor<BuildWrapper>>getDescriptorList(BuildWrapper.class); }
init opposite main stem status|private void () { String path = null; try { path = StagePathUtils.getOppositeMainStem(getPipelineId()); byte[] bytes = zookeeper.readData(path); initOppositeMainStemStatus(bytes); } catch (ZkNoNodeException e) { oppositeMainStemStatus = MainStemEventData.Status.TAKEING; permitSem(); } catch (ZkException e) { logger.error(path, e); } }
between|public String (Date date, DateUnit unit, BetweenFormater.Level formatLevel) { return new DateBetween(this, date).toString(formatLevel); }
get property diff|public static String (Props oldProps, Props newProps) { final StringBuilder builder = new StringBuilder(""); // oldProps can not be null during the below comparison process. if (oldProps == null) { oldProps = new Props(); } if (newProps == null) { newProps = new Props(); } final MapDifference<String, String> md = Maps.difference(toStringMap(oldProps, false), toStringMap(newProps, false)); final Map<String, String> newlyCreatedProperty = md.entriesOnlyOnRight(); if (newlyCreatedProperty != null && newlyCreatedProperty.size() > 0) { builder.append("Newly created Properties: "); newlyCreatedProperty.forEach(( k, v) -> { builder.append("[ " + k + ", " + v + "], "); }); builder.append("\n"); } final Map<String, String> deletedProperty = md.entriesOnlyOnLeft(); if (deletedProperty != null && deletedProperty.size() > 0) { builder.append("Deleted Properties: "); deletedProperty.forEach(( k, v) -> { builder.append("[ " + k + ", " + v + "], "); }); builder.append("\n"); } final Map<String, MapDifference.ValueDifference<String>> diffProperties = md.entriesDiffering(); if (diffProperties != null && diffProperties.size() > 0) { builder.append("Modified Properties: "); diffProperties.forEach(( k, v) -> { builder.append("[ " + k + ", " + v.leftValue() + "-->" + v.rightValue() + "], "); }); } return builder.toString(); }
min|public SDVariable (SDVariable first, SDVariable second) { return min(null, first, second); }
load file|@Nonnull public static String (@Nonnull File logfile, @Nonnull Charset charset) throws IOException { // See: https://issues.jenkins-ci.org/browse/JENKINS-49060?focusedCommentId=325989&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-325989 try { return FileUtils.readFileToString(logfile, charset); } catch (FileNotFoundException e) { return ""; } catch (Exception e) { throw new IOException("Failed to fully read " + logfile, e); } }
viterbi compute|public static List<NS> (List<EnumItem<NS>> roleTagList) { return Viterbi.computeEnum(roleTagList, PlaceDictionary.transformMatrixDictionary); }
ping|public boolean (final String anode, final long timeout) { if (anode.equals(node)) { return true; } else if (anode.indexOf('@', 0) < 0 && anode.equals(node.substring(0, node.indexOf('@', 0)))) { return true; } // other node OtpMbox mbox = null; try { mbox = createMbox(); mbox.send("net_kernel", anode, getPingTuple(mbox)); final OtpErlangObject reply = mbox.receive(timeout); final OtpErlangTuple t = (OtpErlangTuple) reply; final OtpErlangAtom a = (OtpErlangAtom) t.elementAt(1); return "yes".equals(a.atomValue()); } catch (final Exception e) { } finally { closeMbox(mbox); } return false; }
delete instance|public final void (InstanceName name) { DeleteInstanceRequest request = DeleteInstanceRequest.newBuilder().setName(name == null ? null : name.toString()).build(); deleteInstance(request); }
with sparse parcelable array|public Postcard (@Nullable String key, @Nullable SparseArray<? extends Parcelable> value) { mBundle.putSparseParcelableArray(key, value); return this; }
stat file|private static void (String fname) { try { Path path = Paths.get(new URI(fname)); long size = Files.size(path); System.out.println(fname + ": " + size + " bytes."); } catch (Exception ex) { System.out.println(fname + ": " + ex.toString()); } }
filter out|public static <T> Collection<T> (Collection<T> collection, Collection<T> toExclude) { return collection.stream().filter( o -> !toExclude.contains(o)).collect(Collectors.toList()); }
split ignore case|public static List<String> (String str, String separator, int limit, boolean isTrim, boolean ignoreEmpty) { return split(str, separator, limit, isTrim, ignoreEmpty, true); }
set input local strategy|public void (int inputNum, LocalStrategy strategy) { this.config.setInteger(INPUT_LOCAL_STRATEGY_PREFIX + inputNum, strategy.ordinal()); }
store initial hash table|void () throws IOException { if (spilled) { // we create the initialHashTable only once. Later calls are caused by deeper recursion lvls return; } spilled = true; for (int partIdx = 0; partIdx < initialPartitions.size(); partIdx++) { final ReOpenableHashPartition<BT, PT> p = (ReOpenableHashPartition<BT, PT>) initialPartitions.get(partIdx); if (p.isInMemory()) { // write memory resident partitions to disk this.writeBehindBuffersAvailable += p.spillInMemoryPartition(spilledInMemoryPartitions.next(), ioManager, writeBehindBuffers); } } }
load from meta data|public DataSet (List<RecordMetaData> list) throws IOException { if (underlying == null) { SequenceRecord r = recordReader.loadSequenceFromMetaData(list.get(0)); initializeUnderlying(r); } //Two cases: single vs. multiple reader... List<RecordMetaData> l = new ArrayList<>(list.size()); if (singleSequenceReaderMode) { for (RecordMetaData m : list) { l.add(new RecordMetaDataComposableMap(Collections.singletonMap(READER_KEY, m))); } } else { for (RecordMetaData m : list) { RecordMetaDataComposable rmdc = (RecordMetaDataComposable) m; Map<String, RecordMetaData> map = new HashMap<>(2); map.put(READER_KEY, rmdc.getMeta()[0]); map.put(READER_KEY_LABEL, rmdc.getMeta()[1]); l.add(new RecordMetaDataComposableMap(map)); } } return mdsToDataSet(underlying.loadFromMetaData(l)); }
configure|@Override public FsStateBackend (Configuration config, ClassLoader classLoader) { return new FsStateBackend(this, config, classLoader); }
append strings|public CacheKeyBuilder (Collection<String> input) { appendItem(STRING_LIST_KEY, stringCollectionToByteArray(input, true)); return this; }
parse time unit|static TimeUnit (String key, @Nullable String value) { requireArgument((value != null) && !value.isEmpty(), "value of key %s omitted", key); @SuppressWarnings("NullAway") char lastChar = Character.toLowerCase(value.charAt(value.length() - 1)); switch(lastChar) { case 'd': return TimeUnit.DAYS; case 'h': return TimeUnit.HOURS; case 'm': return TimeUnit.MINUTES; case 's': return TimeUnit.SECONDS; default: throw new IllegalArgumentException(String.format("key %s invalid format; was %s, must end with one of [dDhHmMsS]", key, value)); } }
get name id|public NameID (final String nameIdFormat, final String nameIdValue) { val nameId = newSamlObject(NameID.class); nameId.setFormat(nameIdFormat); nameId.setValue(nameIdValue); return nameId; }
option|@SuppressWarnings("unchecked") public <T> Http2StreamChannelBootstrap (ChannelOption<T> option, T value) { if (option == null) { throw new NullPointerException("option"); } if (value == null) { synchronized (options) { options.remove(option); } } else { synchronized (options) { options.put(option, value); } } return this; }
get|@Override public LookupExtractor () { final Lock readLock = startStopSync.readLock(); try { readLock.lockInterruptibly(); } catch (InterruptedException e) { throw new RuntimeException(e); } try { if (entry == null) { throw new ISE("Factory [%s] not started", extractorID); } final CacheScheduler.CacheState cacheState = entry.getCacheState(); if (cacheState instanceof CacheScheduler.NoCache) { final String noCacheReason = ((CacheScheduler.NoCache) cacheState).name(); throw new ISE("%s: %s, extractorID = %s", entry, noCacheReason, extractorID); } CacheScheduler.VersionedCache versionedCache = (CacheScheduler.VersionedCache) cacheState; Map<String, String> map = versionedCache.getCache(); final byte[] v = StringUtils.toUtf8(versionedCache.getVersion()); final byte[] id = StringUtils.toUtf8(extractorID); return new MapLookupExtractor(map, isInjective()) { @Override public byte[] getCacheKey() { return ByteBuffer.allocate(CLASS_CACHE_KEY.length + id.length + 1 + v.length + 1 + 1).put(CLASS_CACHE_KEY).put(id).put((byte) 0xFF).put(v).put((byte) 0xFF).put(isOneToOne() ? (byte) 1 : (byte) 0).array(); } }; } finally { readLock.unlock(); } }
is cipher suite available|public static boolean (String cipherSuite) { String converted = CipherSuiteConverter.toOpenSsl(cipherSuite, IS_BORINGSSL); if (converted != null) { cipherSuite = converted; } return AVAILABLE_OPENSSL_CIPHER_SUITES.contains(cipherSuite); }
get output type|@Override public InputType (InputType... inputType) throws InvalidKerasConfigurationException { if (inputType.length > 1) throw new InvalidKerasConfigurationException("Keras LRN layer accepts only one input (received " + inputType.length + ")"); return this.getLocalResponseNormalization().getOutputType(-1, inputType[0]); }
init file system|private void () throws IOException { if (fs == null) { Path path = new Path(basePath); fs = createHadoopFileSystem(path, fsConfig); } }
insert ssl policy|@BetaApi public final Operation (ProjectName project, SslPolicy sslPolicyResource) { InsertSslPolicyHttpRequest request = InsertSslPolicyHttpRequest.newBuilder().setProject(project == null ? null : project.toString()).setSslPolicyResource(sslPolicyResource).build(); return insertSslPolicy(request); }
encode|@Override public void (final OtpOutputStream buf) { final int arity = arity(); buf.write_map_head(arity); for (final Map.Entry<OtpErlangObject, OtpErlangObject> e : entrySet()) { buf.write_any(e.getKey()); buf.write_any(e.getValue()); } }
do stop|@RequirePOST public HttpResponse () { // need write lock as interrupt will change the field lock.writeLock().lock(); try { if (executable != null) { getParentOf(executable).getOwnerTask().checkAbortPermission(); interrupt(); } } finally { lock.writeLock().unlock(); } return HttpResponses.forwardToPreviousPage(); }
close|public synchronized void (final boolean forceClose) throws SQLException { Collection<SQLException> exceptions = new LinkedList<>(); MasterVisitedManager.clear(); exceptions.addAll(closeResultSets()); exceptions.addAll(closeStatements()); if (!stateHandler.isInTransaction() || forceClose) { exceptions.addAll(releaseConnections(forceClose)); } stateHandler.doNotifyIfNecessary(); throwSQLExceptionIfNecessary(exceptions); }
add remedy index|public void (RemedyIndexEventData data) { String path = StagePathUtils.getRemedyRoot(data.getPipelineId()); try { zookeeper.create(path + "/" + RemedyIndexEventData.formatNodeName(data), new byte[] {}, CreateMode.PERSISTENT); } catch (ZkNodeExistsException e) { } catch (ZkException e) { throw new ArbitrateException("addRemedyIndex", data.getPipelineId().toString(), e); } }
get value|@Override public Number () { return System.currentTimeMillis(); }
print|public final void () { System.out.println("========="); print_impl(-99, NO_KEY, _val_1); _chm.print(); System.out.println("========="); }
read utf lines|public static List<String> (File file) throws IORuntimeException { return readLines(file, CharsetUtil.CHARSET_UTF_8); }
trust manager for certificates|private X509TrustManager (InputStream in) throws GeneralSecurityException { CertificateFactory certificateFactory = CertificateFactory.getInstance("X.509"); Collection<? extends Certificate> certificates = certificateFactory.generateCertificates(in); if (certificates.isEmpty()) { throw new IllegalArgumentException("expected non-empty set of trusted certificates"); } // Put the certificates a key store. char[] password = "password".toCharArray(); KeyStore keyStore = newEmptyKeyStore(password); int index = 0; for (Certificate certificate : certificates) { String certificateAlias = Integer.toString(index++); keyStore.setCertificateEntry(certificateAlias, certificate); } KeyManagerFactory keyManagerFactory = KeyManagerFactory.getInstance(KeyManagerFactory.getDefaultAlgorithm()); keyManagerFactory.init(keyStore, password); TrustManagerFactory trustManagerFactory = TrustManagerFactory.getInstance(TrustManagerFactory.getDefaultAlgorithm()); trustManagerFactory.init(keyStore); TrustManager[] trustManagers = trustManagerFactory.getTrustManagers(); if (trustManagers.length != 1 || !(trustManagers[0] instanceof X509TrustManager)) { throw new IllegalStateException("Unexpected default trust managers:" + Arrays.toString(trustManagers)); } return (X509TrustManager) trustManagers[0]; }
rowwise|private ValFrame (Env env, Frame fr, final AstPrimitive fun) { final String[] names = fr._names; final AstFunction scope = env._scope; // do a single row of the frame to determine the size of the output. double[] ds = new double[fr.numCols()]; for (int col = 0; col < fr.numCols(); ++col) ds[col] = fr.vec(col).at(0); int noutputs = fun.apply(env, env.stk(), new AstRoot[] { fun, new AstRow(ds, fr.names()) }).getRow().length; Frame res = new MRTask() { @Override public void map(Chunk chks[], NewChunk[] nc) { double ds[] = new double[chks.length]; // Working row AstRoot[] // Arguments to be called; they are reused endlessly asts = new AstRoot[] { fun, new AstRow(ds, names) }; // Session, again reused endlessly Session ses = new Session(); Env env = new Env(ses); // For proper namespace lookup env._scope = scope; for (int row = 0; row < chks[0]._len; row++) { for (// Fill the row int col = 0; // Fill the row col < chks.length; // Fill the row col++) ds[col] = chks[col].atd(row); try (Env.StackHelp stk_inner = env.stk()) { double[] // Make the call per-row valRow = fun.apply(env, stk_inner, asts).getRow(); for (int newCol = 0; newCol < nc.length; ++newCol) nc[newCol].addNum(valRow[newCol]); } } // Mostly for the sanity checks ses.end(null); } }.doAll(noutputs, Vec.T_NUM, fr).outputFrame(); return new ValFrame(res); }
get cache key|@Override public byte[] () { return ByteBuffer.allocate(CLASS_CACHE_KEY.length + id.length + 1 + v.length + 1 + 1).put(CLASS_CACHE_KEY).put(id).put((byte) 0xFF).put(v).put((byte) 0xFF).put(isOneToOne() ? (byte) 1 : (byte) 0).array(); }
make|public static DecryptionTool (DecryptionSetup ds) { if (ds._decrypt_tool_id == null) ds._decrypt_tool_id = Key.make(); try { Class<?> dtClass = DecryptionTool.class.getClassLoader().loadClass(ds._decrypt_impl); if (!DecryptionTool.class.isAssignableFrom(dtClass)) { throw new IllegalArgumentException("Class " + ds._decrypt_impl + " doesn't implement a Decryption Tool."); } Constructor<?> constructor = dtClass.getConstructor(DecryptionSetup.class); DecryptionTool dt = (DecryptionTool) constructor.newInstance(ds); DKV.put(dt); return dt; } catch (ClassNotFoundException e) { throw new RuntimeException("Unknown decrypt tool: " + ds._decrypt_impl, e); } catch (NoSuchMethodException e) { throw new RuntimeException("Invalid implementation of Decryption Tool (missing constructor).", e); } catch (Exception e) { throw new RuntimeException(e); } }
compute right entropy|public double (PairFrequency pair) { Set<Map.Entry<String, TriaFrequency>> entrySet = trieTria.prefixSearch(pair.getKey() + RIGHT); return computeEntropy(entrySet); }
decorate for|@Nonnull public final Launcher (@Nonnull Node node) { Launcher l = this; for (LauncherDecorator d : LauncherDecorator.all()) l = d.decorate(l, node); return l; }
split multipart header|private static String[] (String sb) { ArrayList<String> headers = new ArrayList<String>(1); int nameStart; int nameEnd; int colonEnd; int valueStart; int valueEnd; nameStart = HttpPostBodyUtil.findNonWhitespace(sb, 0); for (nameEnd = nameStart; nameEnd < sb.length(); nameEnd++) { char ch = sb.charAt(nameEnd); if (ch == ':' || Character.isWhitespace(ch)) { break; } } for (colonEnd = nameEnd; colonEnd < sb.length(); colonEnd++) { if (sb.charAt(colonEnd) == ':') { colonEnd++; break; } } valueStart = HttpPostBodyUtil.findNonWhitespace(sb, colonEnd); valueEnd = HttpPostBodyUtil.findEndOfString(sb); headers.add(sb.substring(nameStart, nameEnd)); String svalue = (valueStart >= valueEnd) ? StringUtil.EMPTY_STRING : sb.substring(valueStart, valueEnd); String[] values; if (svalue.indexOf(';') >= 0) { values = splitMultipartHeaderValues(svalue); } else { values = svalue.split(","); } for (String value : values) { headers.add(value.trim()); } String[] array = new String[headers.size()]; for (int i = 0; i < headers.size(); i++) { array[i] = headers.get(i); } return array; }
add uuid|public void (long lo, long hi) { if (C16Chunk.isNA(lo, hi)) { addNA(); return; } if (_ms == null || _ds == null || _sparseLen >= _ms.len()) append2slowUUID(); _ms.set(_sparseLen, lo); _ds[_sparseLen] = Double.longBitsToDouble(hi); if (_id != null) _id[_sparseLen] = _len; _sparseLen++; _len++; assert _sparseLen <= _len; }
sign|public byte[] (byte[] headerBytes, byte[] payloadBytes) throws SignatureGenerationException { // default implementation; keep around until sign(byte[]) method is removed byte[] contentBytes = new byte[headerBytes.length + 1 + payloadBytes.length]; System.arraycopy(headerBytes, 0, contentBytes, 0, headerBytes.length); contentBytes[headerBytes.length] = (byte) '.'; System.arraycopy(payloadBytes, 0, contentBytes, headerBytes.length + 1, payloadBytes.length); return sign(contentBytes); }
set destination provider|public WebServiceTemplateBuilder (DestinationProvider destinationProvider) { Assert.notNull(destinationProvider, "DestinationProvider must not be null"); return new WebServiceTemplateBuilder(this.detectHttpMessageSender, this.interceptors, this.internalCustomizers, this.customizers, this.messageSenders, this.marshaller, this.unmarshaller, destinationProvider, this.transformerFactoryClass, this.messageFactory); }
set class path|public void (Path classpath) { pathComponents.removeAllElements(); if (classpath != null) { Path actualClasspath = classpath.concatSystemClasspath("ignore"); String[] pathElements = actualClasspath.list(); for (int i = 0; i < pathElements.length; ++i) { try { addPathElement(pathElements[i]); } catch (BuildException e) { } } } }
rename to|protected void (final String newName) throws IOException { // always synchronize from bigger objects first final ItemGroup parent = getParent(); String oldName = this.name; String oldFullName = getFullName(); synchronized (parent) { synchronized (this) { // sanity check if (newName == null) throw new IllegalArgumentException("New name is not given"); // noop? if (this.name.equals(newName)) return; // the lookup is case insensitive, so we should not fail if this item was the “existing” one // to allow people to rename "Foo" to "foo", for example. // see http://www.nabble.com/error-on-renaming-project-tt18061629.html Items.verifyItemDoesNotAlreadyExist(parent, newName, this); File oldRoot = this.getRootDir(); doSetName(newName); File newRoot = this.getRootDir(); boolean success = false; try { // rename data files boolean interrupted = false; boolean renamed = false; // so retry few times before we fall back to copy. for (int retry = 0; retry < 5; retry++) { if (oldRoot.renameTo(newRoot)) { renamed = true; // succeeded break; } try { Thread.sleep(500); } catch (InterruptedException e) { interrupted = true; } } if (interrupted) Thread.currentThread().interrupt(); if (!renamed) { // failed to rename. it must be that some lengthy // process is going on // to prevent a rename operation. So do a copy. Ideally // we'd like to // later delete the old copy, but we can't reliably do // so, as before the VM // shuts down there might be a new job created under the // old name. Copy cp = new Copy(); cp.setProject(new org.apache.tools.ant.Project()); cp.setTodir(newRoot); FileSet src = new FileSet(); src.setDir(oldRoot); cp.addFileset(src); cp.setOverwrite(true); cp.setPreserveLastModified(true); // keep going even if cp.setFailOnError(false); // there's an error cp.execute(); // try to delete as much as possible try { Util.deleteRecursive(oldRoot); } catch (IOException e) { e.printStackTrace(); } } success = true; } finally { // if failed, back out the rename. if (!success) doSetName(oldName); } parent.onRenamed(this, oldName, newName); } } ItemListener.fireLocationChange(this, oldFullName); }
create sql type with nullability|public static RelDataType (final RelDataTypeFactory typeFactory, final SqlTypeName typeName, final boolean nullable) { final RelDataType dataType; switch(typeName) { case TIMESTAMP: // Our timestamps are down to the millisecond (precision = 3). dataType = typeFactory.createSqlType(typeName, 3); break; case CHAR: case VARCHAR: dataType = typeFactory.createTypeWithCharsetAndCollation(typeFactory.createSqlType(typeName), Calcites.defaultCharset(), SqlCollation.IMPLICIT); break; default: dataType = typeFactory.createSqlType(typeName); } return typeFactory.createTypeWithNullability(dataType, nullable); }
verify|@Override protected void (FullHttpResponse response) { final HttpResponseStatus status = HttpResponseStatus.SWITCHING_PROTOCOLS; final HttpHeaders headers = response.headers(); if (!response.status().equals(status)) { throw new WebSocketHandshakeException("Invalid handshake response getStatus: " + response.status()); } CharSequence upgrade = headers.get(HttpHeaderNames.UPGRADE); if (!HttpHeaderValues.WEBSOCKET.contentEqualsIgnoreCase(upgrade)) { throw new WebSocketHandshakeException("Invalid handshake response upgrade: " + upgrade); } if (!headers.containsValue(HttpHeaderNames.CONNECTION, HttpHeaderValues.UPGRADE, true)) { throw new WebSocketHandshakeException("Invalid handshake response connection: " + headers.get(HttpHeaderNames.CONNECTION)); } CharSequence accept = headers.get(HttpHeaderNames.SEC_WEBSOCKET_ACCEPT); if (accept == null || !accept.equals(expectedChallengeResponseString)) { throw new WebSocketHandshakeException(String.format("Invalid challenge. Actual: %s. Expected: %s", accept, expectedChallengeResponseString)); } }
initial capacity|void (String key, @Nullable String value) { requireArgument(initialCapacity == UNSET_INT, "initial capacity was already set to %,d", initialCapacity); initialCapacity = parseInt(key, value); }
add commands|public void (Iterable<Command> commands) { Assert.notNull(commands, "Commands must not be null"); for (Command command : commands) { addCommand(command); } }
to array|@Override @SuppressWarnings({ "unchecked" }) public <T> T[] (final T[] destination) { requireNonNull(destination, ILLEGAL_DESTINATION_ARRAY); Supplier<T[]> copyRingBuffer = () -> { if (size == 0) { return destination; } T[] result = destination; if (destination.length < size) { result = (T[]) newInstance(result.getClass().getComponentType(), size); } if (headIndex <= tailIndex) { System.arraycopy(ringBuffer, headIndex, result, 0, size); } else { int toTheEnd = ringBuffer.length - headIndex; System.arraycopy(ringBuffer, headIndex, result, 0, toTheEnd); System.arraycopy(ringBuffer, 0, result, toTheEnd, tailIndex + 1); } return result; }; return readConcurrentlyWithoutSpin(copyRingBuffer); }
perform recovery|public int () throws Exception { long traceId = LoggerHelpers.traceEnterWithContext(log, this.traceObjectId, "performRecovery"); Timer timer = new Timer(); log.info("{} Recovery started.", this.traceObjectId); // Put metadata (and entire container) into 'Recovery Mode'. this.metadata.enterRecoveryMode(); // Reset metadata. this.metadata.reset(); OperationMetadataUpdater metadataUpdater = new OperationMetadataUpdater(this.metadata); this.stateUpdater.enterRecoveryMode(metadataUpdater); boolean successfulRecovery = false; int recoveredItemCount; try { recoveredItemCount = recoverAllOperations(metadataUpdater); this.metadata.setContainerEpoch(this.durableDataLog.getEpoch()); log.info("{} Recovery completed. Epoch = {}, Items Recovered = {}, Time = {}ms.", this.traceObjectId, this.metadata.getContainerEpoch(), recoveredItemCount, timer.getElapsedMillis()); successfulRecovery = true; } finally { // We must exit recovery mode when done, regardless of outcome. this.metadata.exitRecoveryMode(); this.stateUpdater.exitRecoveryMode(successfulRecovery); } LoggerHelpers.traceLeave(log, this.traceObjectId, "performRecovery", traceId); return recoveredItemCount; }
delete invalid multi data sets|public static ValidationResult (JavaSparkContext sc, String path, List<int[]> featuresShape, List<int[]> labelsShape) { return validateMultiDataSets(sc, path, true, true, (featuresShape == null ? -1 : featuresShape.size()), (labelsShape == null ? -1 : labelsShape.size()), featuresShape, labelsShape); }
register client from properties|public static synchronized IClient<?, ?> (String restClientName, IClientConfig clientConfig) throws ClientException { IClient<?, ?> client = null; ILoadBalancer loadBalancer = null; if (simpleClientMap.get(restClientName) != null) { throw new ClientException(ClientException.ErrorType.GENERAL, "A Rest Client with this name is already registered. Please use a different name"); } try { String clientClassName = clientConfig.getOrDefault(CommonClientConfigKey.ClientClassName); client = (IClient<?, ?>) instantiateInstanceWithClientConfig(clientClassName, clientConfig); boolean initializeNFLoadBalancer = clientConfig.getOrDefault(CommonClientConfigKey.InitializeNFLoadBalancer); if (initializeNFLoadBalancer) { loadBalancer = registerNamedLoadBalancerFromclientConfig(restClientName, clientConfig); } if (client instanceof AbstractLoadBalancerAwareClient) { ((AbstractLoadBalancerAwareClient) client).setLoadBalancer(loadBalancer); } } catch (Throwable e) { String message = "Unable to InitializeAndAssociateNFLoadBalancer set for RestClient:" + restClientName; logger.warn(message, e); throw new ClientException(ClientException.ErrorType.CONFIGURATION, message, e); } simpleClientMap.put(restClientName, client); Monitors.registerObject("Client_" + restClientName, client); logger.info("Client Registered:" + client.toString()); return client; }
call|@Override public HystrixEventType.Collapser () { return HystrixEventType.Collapser.ADDED_TO_BATCH; }
map|@Override public void (Chunk chks[], NewChunk[] nc) { double ds[] = new double[chks.length]; // Working row AstRoot[] // Arguments to be called; they are reused endlessly asts = new AstRoot[] { fun, new AstRow(ds, names) }; // Session, again reused endlessly Session ses = new Session(); Env env = new Env(ses); // For proper namespace lookup env._scope = scope; for (int row = 0; row < chks[0]._len; row++) { for (// Fill the row int col = 0; // Fill the row col < chks.length; // Fill the row col++) ds[col] = chks[col].atd(row); try (Env.StackHelp stk_inner = env.stk()) { double[] // Make the call per-row valRow = fun.apply(env, stk_inner, asts).getRow(); for (int newCol = 0; newCol < nc.length; ++newCol) nc[newCol].addNum(valRow[newCol]); } } // Mostly for the sanity checks ses.end(null); }
peek|public int () throws OtpErlangDecodeException { int i; try { i = super.buf[super.pos]; if (i < 0) { i += 256; } return i; } catch (final Exception e) { throw new OtpErlangDecodeException("Cannot read from input stream"); } }
no null else get|public static <T> T (@NonNull Supplier<T> s1, @NonNull Supplier<T> s2) { T t1 = s1.get(); if (t1 != null) { return t1; } return s2.get(); }
initialize|public void (IGraph<V, E> graph) { int nVertices = graph.numVertices(); int[] degrees = new int[nVertices]; for (int i = 0; i < nVertices; i++) degrees[i] = graph.getVertexDegree(i); initialize(degrees); }
get timeout|private static int (final Ticket ticket) { val ttl = ticket.getExpirationPolicy().getTimeToLive().intValue(); if (ttl == 0) { return 1; } return ttl; }
un publish port|public void () { // unregister with epmd OtpEpmd.unPublishPort(this); // close the local descriptor (if we have one) try { if (super.epmd != null) { super.epmd.close(); } } catch (final IOException e) { } super.epmd = null; }
get closest fixed bits|public static int (int width) { if (width == 0) { return 1; } if (width >= 1 && width <= 24) { return width; } else if (width > 24 && width <= 26) { return 26; } else if (width > 26 && width <= 28) { return 28; } else if (width > 28 && width <= 30) { return 30; } else if (width > 30 && width <= 32) { return 32; } else if (width > 32 && width <= 40) { return 40; } else if (width > 40 && width <= 48) { return 48; } else if (width > 48 && width <= 56) { return 56; } else { return 64; } }
get boolean|public static boolean (String key, boolean def) { String value = get(key); if (value == null) { return def; } value = value.trim().toLowerCase(); if (value.isEmpty()) { return def; } if ("true".equals(value) || "yes".equals(value) || "1".equals(value)) { return true; } if ("false".equals(value) || "no".equals(value) || "0".equals(value)) { return false; } logger.warn("Unable to parse the boolean system property '{}':{} - using the default value: {}", key, value, def); return def; }
common prefix search|public static LinkedList<Map.Entry<String, CoreDictionary.Attribute>> (char[] chars, int begin) { return trie.commonPrefixSearchWithValue(chars, begin); }
create document db template|public DocumentDbTemplate (final DocumentDbFactory documentDbFactory, final BaseCosmosDbProperties properties) { val documentDbMappingContext = createDocumentDbMappingContext(); val mappingDocumentDbConverter = createMappingDocumentDbConverter(documentDbMappingContext); return new DocumentDbTemplate(documentDbFactory, mappingDocumentDbConverter, properties.getDatabase()); }
add|public static INDArray (INDArray x, INDArray y, INDArray z, int... dimensions) { if (dimensions == null || dimensions.length == 0) { validateShapesNoDimCase(x, y, z); return Nd4j.getExecutioner().exec(new OldAddOp(x, y, z)); } return Nd4j.getExecutioner().exec(new BroadcastAddOp(x, y, z, dimensions)); }
update|void (@NonNull List<PageEntry> entries) { if (entries.isEmpty()) { // Nothing to do. return; } // Apply the in-place updates and collect the new entries to be added. val ci = applyUpdates(entries); if (ci.changes.isEmpty()) { // Nothing else to change. We've already updated the keys in-place. return; } val newPage = applyInsertsAndRemovals(ci); // Make sure we swap all the segments with those from the new page. We need to release all pointers to our // existing buffers. this.header = newPage.header; this.data = newPage.data; this.contents = newPage.contents; this.footer = newPage.footer; this.count = newPage.count; }
monitor detailed|@Nonnull protected final Result<T> () throws InterruptedException { Map<Computer, Future<T>> futures = new HashMap<>(); Set<Computer> skipped = new HashSet<>(); for (Computer c : Jenkins.getInstance().getComputers()) { try { VirtualChannel ch = c.getChannel(); // sentinel value futures.put(c, null); if (ch != null) { Callable<T, ?> cc = createCallable(c); if (cc != null) futures.put(c, ch.callAsync(cc)); } } catch (RuntimeException | IOException e) { LOGGER.log(WARNING, "Failed to monitor " + c.getDisplayName() + " for " + getDisplayName(), e); } } final long now = System.currentTimeMillis(); final long end = now + getMonitoringTimeOut(); final Map<Computer, T> data = new HashMap<>(); for (Entry<Computer, Future<T>> e : futures.entrySet()) { Computer c = e.getKey(); Future<T> f = futures.get(c); // sentinel value data.put(c, null); if (f != null) { try { data.put(c, f.get(Math.max(0, end - System.currentTimeMillis()), MILLISECONDS)); } catch (RuntimeException | TimeoutException | ExecutionException x) { LOGGER.log(WARNING, "Failed to monitor " + c.getDisplayName() + " for " + getDisplayName(), x); } } else { skipped.add(c); } } return new Result<>(data, skipped); }
get qualified name|public static QualifiedName (DereferenceExpression expression) { List<String> parts = tryParseParts(expression.base, expression.field.getValue().toLowerCase(Locale.ENGLISH)); return parts == null ? null : QualifiedName.of(parts); }
get failure in range cut off date|protected Date () { val cutoff = ZonedDateTime.now(ZoneOffset.UTC).minusSeconds(configurationContext.getFailureRangeInSeconds()); return DateTimeUtils.timestampOf(cutoff); }
server rejected|static FirestoreException (Status status, String message, Object... params) { return new FirestoreException(String.format(message, params), status); }
get json array|public JSONArray (K key) { final Object object = this.getObj(key); if (null == object) { return null; } if (object instanceof JSONArray) { return (JSONArray) object; } return new JSONArray(object); }
check display name|// expose utility check method to subclasses @SuppressWarnings("unused") protected FormValidation (@Nonnull View view, @CheckForNull String value) { if (StringUtils.isBlank(value)) { // no custom name, no need to check return FormValidation.ok(); } for (View v : view.owner.getViews()) { if (v.getViewName().equals(view.getViewName())) { continue; } if (StringUtils.equals(v.getDisplayName(), value)) { return FormValidation.warning(Messages.View_DisplayNameNotUniqueWarning(value)); } } return FormValidation.ok(); }
create application|public final Application (String parent, Application application) { CreateApplicationRequest request = CreateApplicationRequest.newBuilder().setParent(parent).setApplication(application).build(); return createApplication(request); }
get sheet name|public String () { if (this.boundSheetRecords.size() > this.sheetIndex) { return this.boundSheetRecords.get(this.sheetIndex > -1 ? this.sheetIndex : this.curSheetIndex).getSheetname(); } return null; }
decrypt value|public String (final String value) { try { return decryptValuePropagateExceptions(value); } catch (final Exception e) { LOGGER.error("Could not decrypt value [{}]", value, e); } return null; }
transfer|private int (int state, int[] ids) { for (int c : ids) { if ((getBase(state) + c < getBaseArraySize()) && (getCheck(getBase(state) + c) == state)) { state = getBase(state) + c; } else { return -1; } } return state; }
retrieve saml authentication request from http request|protected AuthnRequest (final HttpServletRequest request) throws Exception { LOGGER.debug("Retrieving authentication request from scope"); val requestValue = request.getParameter(SamlProtocolConstants.PARAMETER_SAML_REQUEST); if (StringUtils.isBlank(requestValue)) { throw new IllegalArgumentException("SAML request could not be determined from the authentication request"); } val encodedRequest = EncodingUtils.decodeBase64(requestValue.getBytes(StandardCharsets.UTF_8)); return (AuthnRequest) XMLObjectSupport.unmarshallFromInputStream(samlProfileHandlerConfigurationContext.getOpenSamlConfigBean().getParserPool(), new ByteArrayInputStream(encodedRequest)); }
set strictly co located with|public void (JobVertex strictlyCoLocatedWith) { if (this.slotSharingGroup == null || this.slotSharingGroup != strictlyCoLocatedWith.slotSharingGroup) { throw new IllegalArgumentException("Strict co-location requires that both vertices are in the same slot sharing group."); } CoLocationGroup thisGroup = this.coLocationGroup; CoLocationGroup otherGroup = strictlyCoLocatedWith.coLocationGroup; if (otherGroup == null) { if (thisGroup == null) { CoLocationGroup group = new CoLocationGroup(this, strictlyCoLocatedWith); this.coLocationGroup = group; strictlyCoLocatedWith.coLocationGroup = group; } else { thisGroup.addVertex(strictlyCoLocatedWith); strictlyCoLocatedWith.coLocationGroup = thisGroup; } } else { if (thisGroup == null) { otherGroup.addVertex(this); this.coLocationGroup = otherGroup; } else { // both had yet distinct groups, we need to merge them thisGroup.mergeInto(otherGroup); } } }
get label atom|@Nullable public LabelAtom (@CheckForNull String name) { if (name == null) return null; while (true) { Label l = labels.get(name); if (l != null) return (LabelAtom) l; // non-existent LabelAtom la = new LabelAtom(name); if (labels.putIfAbsent(name, la) == null) la.load(); } }
resolve template|private String (AnnotationValue<Client> clientAnnotation, String templateString) { String path = clientAnnotation.get("path", String.class).orElse(null); if (StringUtils.isNotEmpty(path)) { return path + templateString; } else { String value = clientAnnotation.getValue(String.class).orElse(null); if (StringUtils.isNotEmpty(value)) { if (value.startsWith("/")) { return value + templateString; } } return templateString; } }
get login context|protected LoginContext (final UsernamePasswordCredential credential) throws GeneralSecurityException { val callbackHandler = new UsernamePasswordCallbackHandler(credential.getUsername(), credential.getPassword()); if (this.loginConfigurationFile != null && StringUtils.isNotBlank(this.loginConfigType) && this.loginConfigurationFile.exists() && this.loginConfigurationFile.canRead()) { final Configuration.Parameters parameters = new URIParameter(loginConfigurationFile.toURI()); val loginConfig = Configuration.getInstance(this.loginConfigType, parameters); return new LoginContext(this.realm, null, callbackHandler, loginConfig); } return new LoginContext(this.realm, callbackHandler); }
reset thread context loader|public void () { if (LoaderUtils.isContextLoaderAvailable() && isContextLoaderSaved) { LoaderUtils.setContextClassLoader(savedContextLoader); savedContextLoader = null; isContextLoaderSaved = false; } }
load|public static StringDictionary (String path, String separator) { StringDictionary dictionary = new StringDictionary(separator); if (dictionary.load(path)) return dictionary; return null; }
get font|public static Font (int style, Size size) { return getFont(getDefaultFont(), size).deriveFont(style); }
populate metadata|public static void (AbstractComputeInstanceMetadata instanceMetadata, Map<?, ?> metadata) { if (metadata != null) { Map<String, String> finalMetadata = new HashMap<>(metadata.size()); for (Map.Entry<?, ?> entry : metadata.entrySet()) { Object key = entry.getKey(); Object value = entry.getValue(); if (value instanceof String) { finalMetadata.put(key.toString(), value.toString()); } } instanceMetadata.setMetadata(finalMetadata); } }
get extensions|public List<Extension> () { List<Extension> list = new ArrayList<Extension>(); for (AddOn addOn : getAddOnCollection().getAddOns()) { list.addAll(getExtensions(addOn)); } return list; }
prepare saml outbound destination handler|protected <T extends SAMLObject> void (final MessageContext<T> outboundContext) throws Exception { val handlerDest = new SAMLOutboundDestinationHandler(); handlerDest.initialize(); handlerDest.invoke(outboundContext); }
combine|public void (SimpleItem other) { for (Map.Entry<String, Integer> entry : other.labelMap.entrySet()) { addLabel(entry.getKey(), entry.getValue()); } }
symmetric generalized eigenvalues|public static INDArray (INDArray A, INDArray B) { Preconditions.checkArgument(A.isMatrix() && A.isSquare(), "Argument A must be a square matrix: has shape %s", A.shape()); Preconditions.checkArgument(B.isMatrix() && B.isSquare(), "Argument B must be a square matrix: has shape %s", B.shape()); INDArray W = Nd4j.create(A.rows()); A = InvertMatrix.invert(B, false).mmuli(A); Nd4j.getBlasWrapper().syev('V', 'L', A, W); return W; }
process|@Override public boolean (Set<? extends TypeElement> annotations, RoundEnvironment roundEnv) { if (errorToShow != null) { if (errorToShow != null) { Set<? extends Element> rootElements = roundEnv.getRootElements(); if (!rootElements.isEmpty()) { processingEnv.getMessager().printMessage(Kind.WARNING, errorToShow, rootElements.iterator().next()); errorToShow = null; } } } return false; }
compute unclosed branch stack|@Override public void () { if (this.openBranches != null) { return; } addClosedBranches(getPredecessorNode().closedBranchingNodes); List<UnclosedBranchDescriptor> fromInput = getPredecessorNode().getBranchesForParent(this.inConn); // handle the data flow branching for the broadcast inputs List<UnclosedBranchDescriptor> result = computeUnclosedBranchStackForBroadcastInputs(fromInput); this.openBranches = (result == null || result.isEmpty()) ? Collections.<UnclosedBranchDescriptor>emptyList() : result; }
collect client|public void (RpcClientLookoutModel rpcClientMetricsModel) { try { Id methodConsumerId = createMethodConsumerId(rpcClientMetricsModel); MixinMetric methodConsumerMetric = Lookout.registry().mixinMetric(methodConsumerId); recordCounterAndTimer(methodConsumerMetric, rpcClientMetricsModel); recordSize(methodConsumerMetric, rpcClientMetricsModel); } catch (Throwable t) { LOGGER.error(LogCodes.getLog(LogCodes.ERROR_METRIC_REPORT_ERROR), t); } }
parse reader|public static JsonElement (JsonReader reader) throws JsonIOException, JsonSyntaxException { boolean lenient = reader.isLenient(); reader.setLenient(true); try { return Streams.parse(reader); } catch (StackOverflowError e) { throw new JsonParseException("Failed parsing JSON source: " + reader + " to Json", e); } catch (OutOfMemoryError e) { throw new JsonParseException("Failed parsing JSON source: " + reader + " to Json", e); } finally { reader.setLenient(lenient); } }
list region autoscalers|@BetaApi public final ListRegionAutoscalersPagedResponse (ProjectRegionName region) { ListRegionAutoscalersHttpRequest request = ListRegionAutoscalersHttpRequest.newBuilder().setRegion(region == null ? null : region.toString()).build(); return listRegionAutoscalers(request); }
set status|public void (String asgName, boolean enabled) { String asgAccountId = getASGAccount(asgName); asgCache.put(new CacheKey(asgAccountId, asgName), enabled); }
binary search|public static <T> int (List<? extends Comparable<? super T>> sortedList, T key) { return Collections.binarySearch(sortedList, key); }
set credentials|public void (final UsernamePasswordCredential credential) { val wss4jSecurityInterceptor = new Wss4jSecurityInterceptor(); wss4jSecurityInterceptor.setSecurementActions("Timestamp UsernameToken"); wss4jSecurityInterceptor.setSecurementUsername(credential.getUsername()); wss4jSecurityInterceptor.setSecurementPassword(credential.getPassword()); setInterceptors(new ClientInterceptor[] { wss4jSecurityInterceptor }); }
resolve execution jar name|public static String (String workingDirectory, String userSpecifiedJarName, Logger log) { if (log.isDebugEnabled()) { String debugMsg = String.format("Resolving execution jar name: working directory: %s, user specified name: %s", workingDirectory, userSpecifiedJarName); log.debug(debugMsg); } if (userSpecifiedJarName.endsWith(".jar")) { userSpecifiedJarName = userSpecifiedJarName.replace(".jar", ""); } else if (userSpecifiedJarName.endsWith(".py")) { userSpecifiedJarName = userSpecifiedJarName.replace(".py", ""); } String userSpecifiedJarPath = String.format("%s/%s", workingDirectory, userSpecifiedJarName); int lastIndexOfSlash = userSpecifiedJarPath.lastIndexOf("/"); final String jarPrefix = userSpecifiedJarPath.substring(lastIndexOfSlash + 1); final String dirName = userSpecifiedJarPath.substring(0, lastIndexOfSlash); if (log.isDebugEnabled()) { String debugMsg = String.format("Resolving execution jar name: dirname: %s, jar name: %s", dirName, jarPrefix); log.debug(debugMsg); } File[] potentialExecutionJarList; try { potentialExecutionJarList = getFilesInFolderByRegex(new File(dirName), jarPrefix + ".*(jar|py)"); } catch (FileNotFoundException e) { throw new IllegalStateException("execution jar is suppose to be in this folder, but the folder doesn't exist: " + dirName); } if (potentialExecutionJarList.length == 0) { throw new IllegalStateException("unable to find execution jar for Spark at path: " + userSpecifiedJarPath + "*.(jar|py)"); } else if (potentialExecutionJarList.length > 1) { throw new IllegalStateException("I find more than one matching instance of the execution jar at the path, don't know which one to use: " + userSpecifiedJarPath + "*.(jar|py)"); } String resolvedJarName = potentialExecutionJarList[0].toString(); log.info("Resolving execution jar/py name: resolvedJarName: " + resolvedJarName); return resolvedJarName; }
undelete table|public void (String datasetId) throws InterruptedException { generateTableWithDdl(datasetId, "oops_undelete_me"); // [START bigquery_undelete_table] // String datasetId = "my_dataset"; String tableId = "oops_undelete_me"; // Record the current time. We'll use this as the snapshot time // for recovering the table. long snapTime = Instant.now().toEpochMilli(); // "Accidentally" delete the table. bigquery.delete(TableId.of(datasetId, tableId)); // Construct the restore-from tableID using a snapshot decorator. String snapshotTableId = String.format("%s@%d", tableId, snapTime); // Choose a new table ID for the recovered table data. String recoverTableId = String.format("%s_recovered", tableId); // Construct and run a copy job. CopyJobConfiguration configuration = CopyJobConfiguration.newBuilder(TableId.of(datasetId, recoverTableId), TableId.of(datasetId, snapshotTableId)).build(); Job job = bigquery.create(JobInfo.of(configuration)); job = job.waitFor(); // Check the table StandardTableDefinition table = bigquery.getTable(TableId.of(datasetId, recoverTableId)).getDefinition(); System.out.println("State: " + job.getStatus().getState()); System.out.printf("Recovered %d rows.\n", table.getNumRows()); // [END bigquery_undelete_table] }
fit sequences|public void (JavaRDD<Sequence<T>> corpus) { /** * Basically all we want for base implementation here is 3 things: * a) build vocabulary * b) build huffman tree * c) do training * * in this case all classes extending SeqVec, like deepwalk or word2vec will be just building their RDD<Sequence<T>>, * and calling this method for training, instead implementing own routines */ validateConfiguration(); if (ela == null) { try { ela = (SparkElementsLearningAlgorithm) Class.forName(configuration.getElementsLearningAlgorithm()).newInstance(); } catch (Exception e) { throw new RuntimeException(e); } } if (workers > 1) { log.info("Repartitioning corpus to {} parts...", workers); corpus.repartition(workers); } if (storageLevel != null) corpus.persist(storageLevel); final JavaSparkContext sc = new JavaSparkContext(corpus.context()); // this will have any effect only if wasn't called before, in extension classes broadcastEnvironment(sc); Counter<Long> finalCounter; long numberOfSequences = 0; /** * Here we s */ if (paramServerConfiguration == null) { paramServerConfiguration = VoidConfiguration.builder().numberOfShards(2).unicastPort(40123).multicastPort(40124).build(); paramServerConfiguration.setFaultToleranceStrategy(FaultToleranceStrategy.NONE); } isAutoDiscoveryMode = paramServerConfiguration.getShardAddresses() != null && !paramServerConfiguration.getShardAddresses().isEmpty() ? false : true; Broadcast<VoidConfiguration> paramServerConfigurationBroadcast = null; if (isAutoDiscoveryMode) { log.info("Trying auto discovery mode..."); elementsFreqAccumExtra = corpus.context().accumulator(new ExtraCounter<Long>(), new ExtraElementsFrequenciesAccumulator()); ExtraCountFunction<T> elementsCounter = new ExtraCountFunction<>(elementsFreqAccumExtra, configuration.isTrainSequenceVectors()); JavaRDD<Pair<Sequence<T>, Long>> countedCorpus = corpus.map(elementsCounter); // just to trigger map function, since we need huffman tree before proceeding numberOfSequences = countedCorpus.count(); finalCounter = elementsFreqAccumExtra.value(); ExtraCounter<Long> spareReference = (ExtraCounter<Long>) finalCounter; // getting list of available hosts Set<NetworkInformation> availableHosts = spareReference.getNetworkInformation(); log.info("availableHosts: {}", availableHosts); if (availableHosts.size() > 1) { // now we have to pick N shards and optionally N backup nodes, and pass them within configuration bean NetworkOrganizer organizer = new NetworkOrganizer(availableHosts, paramServerConfiguration.getNetworkMask()); paramServerConfiguration.setShardAddresses(organizer.getSubset(paramServerConfiguration.getNumberOfShards())); // backup shards are optional if (paramServerConfiguration.getFaultToleranceStrategy() != FaultToleranceStrategy.NONE) { paramServerConfiguration.setBackupAddresses(organizer.getSubset(paramServerConfiguration.getNumberOfShards(), paramServerConfiguration.getShardAddresses())); } } else { // for single host (aka driver-only, aka spark-local) just run on loopback interface paramServerConfiguration.setShardAddresses(Arrays.asList("127.0.0.1:" + paramServerConfiguration.getPortSupplier().getPort())); paramServerConfiguration.setFaultToleranceStrategy(FaultToleranceStrategy.NONE); } log.info("Got Shards so far: {}", paramServerConfiguration.getShardAddresses()); // update ps configuration with real values where required paramServerConfiguration.setNumberOfShards(paramServerConfiguration.getShardAddresses().size()); paramServerConfiguration.setUseHS(configuration.isUseHierarchicSoftmax()); paramServerConfiguration.setUseNS(configuration.getNegative() > 0); paramServerConfigurationBroadcast = sc.broadcast(paramServerConfiguration); } else { // update ps configuration with real values where required paramServerConfiguration.setNumberOfShards(paramServerConfiguration.getShardAddresses().size()); paramServerConfiguration.setUseHS(configuration.isUseHierarchicSoftmax()); paramServerConfiguration.setUseNS(configuration.getNegative() > 0); paramServerConfigurationBroadcast = sc.broadcast(paramServerConfiguration); // set up freqs accumulator elementsFreqAccum = corpus.context().accumulator(new Counter<Long>(), new ElementsFrequenciesAccumulator()); CountFunction<T> elementsCounter = new CountFunction<>(configurationBroadcast, paramServerConfigurationBroadcast, elementsFreqAccum, configuration.isTrainSequenceVectors()); // count all sequence elements and their sum JavaRDD<Pair<Sequence<T>, Long>> countedCorpus = corpus.map(elementsCounter); // just to trigger map function, since we need huffman tree before proceeding numberOfSequences = countedCorpus.count(); // now we grab counter, which contains frequencies for all SequenceElements in corpus finalCounter = elementsFreqAccum.value(); } long numberOfElements = (long) finalCounter.totalCount(); long numberOfUniqueElements = finalCounter.size(); log.info("Total number of sequences: {}; Total number of elements entries: {}; Total number of unique elements: {}", numberOfSequences, numberOfElements, numberOfUniqueElements); /* build RDD of reduced SequenceElements, just get rid of labels temporary, stick to some numerical values, like index or hashcode. So we could reduce driver memory footprint */ // build huffman tree, and update original RDD with huffman encoding info shallowVocabCache = buildShallowVocabCache(finalCounter); shallowVocabCacheBroadcast = sc.broadcast(shallowVocabCache); // FIXME: probably we need to reconsider this approach JavaRDD<T> vocabRDD = corpus.flatMap(new VocabRddFunctionFlat<T>(configurationBroadcast, paramServerConfigurationBroadcast)).distinct(); vocabRDD.count(); /** * now we initialize Shards with values. That call should be started from driver which is either Client or Shard in standalone mode. */ VoidParameterServer.getInstance().init(paramServerConfiguration, new RoutedTransport(), ela.getTrainingDriver()); VoidParameterServer.getInstance().initializeSeqVec(configuration.getLayersSize(), (int) numberOfUniqueElements, 119, configuration.getLayersSize() / paramServerConfiguration.getNumberOfShards(), paramServerConfiguration.isUseHS(), paramServerConfiguration.isUseNS()); // proceed to training // also, training function is the place where we invoke ParameterServer TrainingFunction<T> trainer = new TrainingFunction<>(shallowVocabCacheBroadcast, configurationBroadcast, paramServerConfigurationBroadcast); PartitionTrainingFunction<T> partitionTrainer = new PartitionTrainingFunction<>(shallowVocabCacheBroadcast, configurationBroadcast, paramServerConfigurationBroadcast); if (configuration != null) for (int e = 0; e < configuration.getEpochs(); e++) corpus.foreachPartition(partitionTrainer); //corpus.foreach(trainer); // we're transferring vectors to ExportContainer JavaRDD<ExportContainer<T>> exportRdd = vocabRDD.map(new DistributedFunction<T>(paramServerConfigurationBroadcast, configurationBroadcast, shallowVocabCacheBroadcast)); // at this particular moment training should be pretty much done, and we're good to go for export if (exporter != null) exporter.export(exportRdd); // unpersist, if we've persisten corpus after all if (storageLevel != null) corpus.unpersist(); log.info("Training finish, starting cleanup..."); VoidParameterServer.getInstance().shutdown(); }
get order|@Override public int () { return Ordered.HIGHEST_PRECEDENCE; }
set codec configuration|@Inject public void (@Nullable Configuration.CodecConfiguration codecConfiguration) { if (codecConfiguration != null) { configuration.withCodec(codecConfiguration); } }
handle request internal|@Override protected ModelAndView (final HttpServletRequest request, final HttpServletResponse response) throws Exception { for (val delegate : this.delegates) { if (delegate.canHandle(request, response)) { return delegate.handleRequestInternal(request, response); } } return generateErrorView(CasProtocolConstants.ERROR_CODE_INVALID_REQUEST, null); }
create rpc service|public static RpcService (final Configuration configuration, final HighAvailabilityServices haServices) throws Exception { checkNotNull(configuration); checkNotNull(haServices); final String taskManagerAddress = determineTaskManagerBindAddress(configuration, haServices); final String portRangeDefinition = configuration.getString(TaskManagerOptions.RPC_PORT); return AkkaRpcServiceUtils.createRpcService(taskManagerAddress, portRangeDefinition, configuration); }
with unsafe|public ConfigBuilder<T> (Property<?> property, Object value) { String key = String.format("%s.%s", this.namespace, property.getName()); this.properties.setProperty(key, value.toString()); return this; }
list products|public final ListProductsPagedResponse (String parent) { ListProductsRequest request = ListProductsRequest.newBuilder().setParent(parent).build(); return listProducts(request); }
intercept|@Override public Object (MethodInvocationContext<Object, Object> context) { AnnotationValue<Client> clientAnnotation = context.findAnnotation(Client.class).orElseThrow(() -> new IllegalStateException("Client advice called from type that is not annotated with @Client: " + context)); HttpClient httpClient = getClient(context, clientAnnotation); Class<?> declaringType = context.getDeclaringType(); if (Closeable.class == declaringType || AutoCloseable.class == declaringType) { String clientId = clientAnnotation.getValue(String.class).orElse(null); String path = clientAnnotation.get("path", String.class).orElse(null); String clientKey = computeClientKey(clientId, path); clients.remove(clientKey); httpClient.close(); return null; } Optional<Class<? extends Annotation>> httpMethodMapping = context.getAnnotationTypeByStereotype(HttpMethodMapping.class); if (context.hasStereotype(HttpMethodMapping.class) && httpClient != null) { AnnotationValue<HttpMethodMapping> mapping = context.getAnnotation(HttpMethodMapping.class); String uri = mapping.getRequiredValue(String.class); if (StringUtils.isEmpty(uri)) { uri = "/" + context.getMethodName(); } Class<? extends Annotation> annotationType = httpMethodMapping.get(); HttpMethod httpMethod = HttpMethod.valueOf(annotationType.getSimpleName().toUpperCase()); ReturnType returnType = context.getReturnType(); Class<?> javaReturnType = returnType.getType(); UriMatchTemplate uriTemplate = UriMatchTemplate.of(""); if (!(uri.length() == 1 && uri.charAt(0) == '/')) { uriTemplate = uriTemplate.nest(uri); } Map<String, Object> paramMap = context.getParameterValueMap(); Map<String, String> queryParams = new LinkedHashMap<>(); List<String> uriVariables = uriTemplate.getVariableNames(); boolean variableSatisfied = uriVariables.isEmpty() || uriVariables.containsAll(paramMap.keySet()); MutableHttpRequest<Object> request; Object body = null; Map<String, MutableArgumentValue<?>> parameters = context.getParameters(); Argument[] arguments = context.getArguments(); Map<String, String> headers = new LinkedHashMap<>(HEADERS_INITIAL_CAPACITY); List<AnnotationValue<Header>> headerAnnotations = context.getAnnotationValuesByType(Header.class); for (AnnotationValue<Header> headerAnnotation : headerAnnotations) { String headerName = headerAnnotation.get("name", String.class).orElse(null); String headerValue = headerAnnotation.getValue(String.class).orElse(null); if (StringUtils.isNotEmpty(headerName) && StringUtils.isNotEmpty(headerValue)) { headers.put(headerName, headerValue); } } context.findAnnotation(Version.class).flatMap( versionAnnotation -> versionAnnotation.getValue(String.class)).filter(StringUtils::isNotEmpty).ifPresent( version -> { ClientVersioningConfiguration configuration = getVersioningConfiguration(clientAnnotation); configuration.getHeaders().forEach( header -> headers.put(header, version)); configuration.getParameters().forEach( parameter -> queryParams.put(parameter, version)); }); Map<String, Object> attributes = new LinkedHashMap<>(ATTRIBUTES_INITIAL_CAPACITY); List<AnnotationValue<RequestAttribute>> attributeAnnotations = context.getAnnotationValuesByType(RequestAttribute.class); for (AnnotationValue<RequestAttribute> attributeAnnotation : attributeAnnotations) { String attributeName = attributeAnnotation.get("name", String.class).orElse(null); Object attributeValue = attributeAnnotation.getValue(Object.class).orElse(null); if (StringUtils.isNotEmpty(attributeName) && attributeValue != null) { attributes.put(attributeName, attributeValue); } } List<NettyCookie> cookies = new ArrayList<>(); List<Argument> bodyArguments = new ArrayList<>(); ConversionService<?> conversionService = ConversionService.SHARED; for (Argument argument : arguments) { String argumentName = argument.getName(); AnnotationMetadata annotationMetadata = argument.getAnnotationMetadata(); MutableArgumentValue<?> value = parameters.get(argumentName); Object definedValue = value.getValue(); if (paramMap.containsKey(argumentName)) { if (annotationMetadata.hasStereotype(Format.class)) { final Object v = paramMap.get(argumentName); if (v != null) { paramMap.put(argumentName, conversionService.convert(v, ConversionContext.of(String.class).with(argument.getAnnotationMetadata()))); } } } if (definedValue == null) { definedValue = argument.getAnnotationMetadata().getValue(Bindable.class, "defaultValue", String.class).orElse(null); } if (definedValue == null && !argument.isAnnotationPresent(Nullable.class)) { throw new IllegalArgumentException(String.format("Null values are not allowed to be passed to client methods (%s). Add @javax.validation.Nullable if that is the desired behavior", context.getExecutableMethod().toString())); } if (argument.isAnnotationPresent(Body.class)) { body = definedValue; } else if (annotationMetadata.isAnnotationPresent(Header.class)) { String headerName = annotationMetadata.getValue(Header.class, String.class).orElse(null); if (StringUtils.isEmpty(headerName)) { headerName = NameUtils.hyphenate(argumentName); } String finalHeaderName = headerName; conversionService.convert(definedValue, String.class).ifPresent( o -> headers.put(finalHeaderName, o)); } else if (annotationMetadata.isAnnotationPresent(CookieValue.class)) { String cookieName = annotationMetadata.getValue(CookieValue.class, String.class).orElse(null); if (StringUtils.isEmpty(cookieName)) { cookieName = argumentName; } String finalCookieName = cookieName; conversionService.convert(definedValue, String.class).ifPresent( o -> cookies.add(new NettyCookie(finalCookieName, o))); } else if (annotationMetadata.isAnnotationPresent(QueryValue.class)) { String parameterName = annotationMetadata.getValue(QueryValue.class, String.class).orElse(null); conversionService.convert(definedValue, ConversionContext.of(String.class).with(annotationMetadata)).ifPresent( o -> { if (!StringUtils.isEmpty(parameterName)) { paramMap.put(parameterName, o); queryParams.put(parameterName, o); } else { queryParams.put(argumentName, o); } }); } else if (annotationMetadata.isAnnotationPresent(RequestAttribute.class)) { String attributeName = annotationMetadata.getValue(Annotation.class, String.class).orElse(null); if (StringUtils.isEmpty(attributeName)) { attributeName = NameUtils.hyphenate(argumentName); } String finalAttributeName = attributeName; conversionService.convert(definedValue, Object.class).ifPresent( o -> attributes.put(finalAttributeName, o)); } else if (annotationMetadata.isAnnotationPresent(PathVariable.class)) { String parameterName = annotationMetadata.getValue(PathVariable.class, String.class).orElse(null); conversionService.convert(definedValue, ConversionContext.of(String.class).with(annotationMetadata)).ifPresent( o -> { if (!StringUtils.isEmpty(o)) { paramMap.put(parameterName, o); } }); } else if (!uriVariables.contains(argumentName)) { bodyArguments.add(argument); } } if (HttpMethod.permitsRequestBody(httpMethod)) { if (body == null && !bodyArguments.isEmpty()) { Map<String, Object> bodyMap = new LinkedHashMap<>(); for (Argument bodyArgument : bodyArguments) { String argumentName = bodyArgument.getName(); MutableArgumentValue<?> value = parameters.get(argumentName); bodyMap.put(argumentName, value.getValue()); } body = bodyMap; } if (body != null) { if (!variableSatisfied) { if (body instanceof Map) { paramMap.putAll((Map) body); } else { BeanMap<Object> beanMap = BeanMap.of(body); for (Map.Entry<String, Object> entry : beanMap.entrySet()) { String k = entry.getKey(); Object v = entry.getValue(); if (v != null) { paramMap.put(k, v); } } } } } } uri = uriTemplate.expand(paramMap); uriVariables.forEach(queryParams::remove); request = HttpRequest.create(httpMethod, appendQuery(uri, queryParams)); if (body != null) { request.body(body); MediaType[] contentTypes = context.getValue(Produces.class, MediaType[].class).orElse(DEFAULT_ACCEPT_TYPES); if (ArrayUtils.isNotEmpty(contentTypes)) { request.contentType(contentTypes[0]); } } request.setAttribute(HttpAttributes.URI_TEMPLATE, resolveTemplate(clientAnnotation, uriTemplate.toString())); String serviceId = clientAnnotation.getValue(String.class).orElse(null); Argument<?> errorType = clientAnnotation.get("errorType", Class.class).map((Function<Class, Argument>) Argument::of).orElse(HttpClient.DEFAULT_ERROR_TYPE); request.setAttribute(HttpAttributes.SERVICE_ID, serviceId); if (!headers.isEmpty()) { for (Map.Entry<String, String> entry : headers.entrySet()) { request.header(entry.getKey(), entry.getValue()); } } cookies.forEach(request::cookie); if (!attributes.isEmpty()) { for (Map.Entry<String, Object> entry : attributes.entrySet()) { request.setAttribute(entry.getKey(), entry.getValue()); } } MediaType[] acceptTypes = context.getValue(Consumes.class, MediaType[].class).orElse(DEFAULT_ACCEPT_TYPES); boolean isFuture = CompletableFuture.class.isAssignableFrom(javaReturnType); final Class<?> methodDeclaringType = declaringType; if (Publishers.isConvertibleToPublisher(javaReturnType) || isFuture) { boolean isSingle = Publishers.isSingle(javaReturnType) || isFuture || context.getValue(Consumes.class, "single", Boolean.class).orElse(false); Argument<?> publisherArgument = returnType.asArgument().getFirstTypeVariable().orElse(Argument.OBJECT_ARGUMENT); Class<?> argumentType = publisherArgument.getType(); if (HttpResponse.class.isAssignableFrom(argumentType) || HttpStatus.class.isAssignableFrom(argumentType)) { isSingle = true; } Publisher<?> publisher; if (!isSingle && httpClient instanceof StreamingHttpClient) { StreamingHttpClient streamingHttpClient = (StreamingHttpClient) httpClient; if (!Void.class.isAssignableFrom(argumentType)) { request.accept(acceptTypes); } if (HttpResponse.class.isAssignableFrom(argumentType) || Void.class.isAssignableFrom(argumentType)) { publisher = streamingHttpClient.exchangeStream(request); } else { boolean isEventStream = Arrays.asList(acceptTypes).contains(MediaType.TEXT_EVENT_STREAM_TYPE); if (isEventStream && streamingHttpClient instanceof SseClient) { SseClient sseClient = (SseClient) streamingHttpClient; if (publisherArgument.getType() == Event.class) { publisher = sseClient.eventStream(request, publisherArgument.getFirstTypeVariable().orElse(Argument.OBJECT_ARGUMENT)); } else { publisher = Flowable.fromPublisher(sseClient.eventStream(request, publisherArgument)).map(Event::getData); } } else { boolean isJson = isJsonParsedMediaType(acceptTypes); if (isJson) { publisher = streamingHttpClient.jsonStream(request, publisherArgument); } else { Publisher<ByteBuffer<?>> byteBufferPublisher = streamingHttpClient.dataStream(request); if (argumentType == ByteBuffer.class) { publisher = byteBufferPublisher; } else { if (conversionService.canConvert(ByteBuffer.class, argumentType)) { // It would be nice if we could capture the TypeConverter here publisher = Flowable.fromPublisher(byteBufferPublisher).map( value -> conversionService.convert(value, argumentType).get()); } else { throw new ConfigurationException("Cannot create the generated HTTP client's " + "required return type, since no TypeConverter from ByteBuffer to " + argumentType + " is registered"); } } } } } } else { if (Void.class.isAssignableFrom(argumentType)) { publisher = httpClient.exchange(request, null, errorType); } else { request.accept(acceptTypes); if (HttpResponse.class.isAssignableFrom(argumentType)) { publisher = httpClient.exchange(request, publisherArgument, errorType); } else { publisher = httpClient.retrieve(request, publisherArgument, errorType); } } } if (isFuture) { CompletableFuture<Object> future = new CompletableFuture<>(); publisher.subscribe(new CompletionAwareSubscriber<Object>() { AtomicReference<Object> reference = new AtomicReference<>(); @Override protected void doOnSubscribe(Subscription subscription) { subscription.request(1); } @Override protected void doOnNext(Object message) { if (!Void.class.isAssignableFrom(argumentType)) { reference.set(message); } } @Override protected void doOnError(Throwable t) { if (t instanceof HttpClientResponseException) { HttpClientResponseException e = (HttpClientResponseException) t; if (e.getStatus() == HttpStatus.NOT_FOUND) { future.complete(null); return; } } if (LOG.isErrorEnabled()) { LOG.error("Client [" + methodDeclaringType.getName() + "] received HTTP error response: " + t.getMessage(), t); } future.completeExceptionally(t); } @Override protected void doOnComplete() { future.complete(reference.get()); } }); return future; } else { Object finalPublisher = conversionService.convert(publisher, javaReturnType).orElseThrow(() -> new HttpClientException("Cannot convert response publisher to Reactive type (Unsupported Reactive type): " + javaReturnType)); for (ReactiveClientResultTransformer transformer : transformers) { finalPublisher = transformer.transform(finalPublisher); } return finalPublisher; } } else { BlockingHttpClient blockingHttpClient = httpClient.toBlocking(); if (void.class != javaReturnType) { request.accept(acceptTypes); } if (HttpResponse.class.isAssignableFrom(javaReturnType)) { return blockingHttpClient.exchange(request, returnType.asArgument().getFirstTypeVariable().orElse(Argument.OBJECT_ARGUMENT), errorType); } else if (void.class == javaReturnType) { blockingHttpClient.exchange(request, null, errorType); return null; } else { try { return blockingHttpClient.retrieve(request, returnType.asArgument(), errorType); } catch (RuntimeException t) { if (t instanceof HttpClientResponseException && ((HttpClientResponseException) t).getStatus() == HttpStatus.NOT_FOUND) { if (javaReturnType == Optional.class) { return Optional.empty(); } return null; } else { throw t; } } } } } // try other introduction advice return context.proceed(); }
get grouper group attribute|public static String (final GrouperGroupField groupField, final WsGroup group) { switch(groupField) { case DISPLAY_EXTENSION: return group.getDisplayExtension(); case DISPLAY_NAME: return group.getDisplayName(); case EXTENSION: return group.getExtension(); case NAME: default: return group.getName(); } }
future with timeout|public static <T> CompletableFuture<T> (Duration timeout, ScheduledExecutorService executorService) { return futureWithTimeout(timeout, null, executorService); }
next|@Override public MultiDataSet () { val features = new ArrayList<INDArray>(); val labels = new ArrayList<INDArray>(); val featuresMask = new ArrayList<INDArray>(); val labelsMask = new ArrayList<INDArray>(); boolean hasFM = false; boolean hasLM = false; int cnt = 0; for (val i : iterators) { val ds = i.next(); features.add(ds.getFeatures()); featuresMask.add(ds.getFeaturesMaskArray()); if (outcome < 0 || cnt == outcome) { labels.add(ds.getLabels()); labelsMask.add(ds.getLabelsMaskArray()); } if (ds.getFeaturesMaskArray() != null) hasFM = true; if (ds.getLabelsMaskArray() != null) hasLM = true; cnt++; } INDArray[] fm = hasFM ? featuresMask.toArray(new INDArray[0]) : null; INDArray[] lm = hasLM ? labelsMask.toArray(new INDArray[0]) : null; val mds = new org.nd4j.linalg.dataset.MultiDataSet(features.toArray(new INDArray[0]), labels.toArray(new INDArray[0]), fm, lm); if (preProcessor != null) preProcessor.preProcess(mds); return mds; }
gte|public static INDArray (INDArray x, INDArray y, INDArray z, int... dimensions) { if (dimensions == null || dimensions.length == 0) { validateShapesNoDimCase(x, y, z); return Nd4j.getExecutioner().exec(new OldGreaterThanOrEqual(x, y, z)); } return Nd4j.getExecutioner().exec(new BroadcastGreaterThanOrEqual(x, y, z, dimensions)); }
validate by secure key|public static boolean (Map<String, String> resData, String secureKey, String encoding) { LogUtil.writeLog("验签处理开始"); if (isEmpty(encoding)) { encoding = "UTF-8"; } String signMethod = resData.get(SDKConstants.param_signMethod); if (SIGNMETHOD_SHA256.equals(signMethod)) { // 1.进行SHA256验证 String stringSign = resData.get(SDKConstants.param_signature); LogUtil.writeLog("签名原文：[" + stringSign + "]"); // 将Map信息转换成key1=value1&key2=value2的形式 String stringData = coverMap2String(resData); LogUtil.writeLog("待验签返回报文串：[" + stringData + "]"); String strBeforeSha256 = stringData + SDKConstants.AMPERSAND + SecureUtil.sha256X16Str(secureKey, encoding); String strAfterSha256 = SecureUtil.sha256X16Str(strBeforeSha256, encoding); return stringSign.equals(strAfterSha256); } else if (SIGNMETHOD_SM3.equals(signMethod)) { // 1.进行SM3验证 String stringSign = resData.get(SDKConstants.param_signature); LogUtil.writeLog("签名原文：[" + stringSign + "]"); // 将Map信息转换成key1=value1&key2=value2的形式 String stringData = coverMap2String(resData); LogUtil.writeLog("待验签返回报文串：[" + stringData + "]"); String strBeforeSM3 = stringData + SDKConstants.AMPERSAND + SecureUtil.sm3X16Str(secureKey, encoding); String strAfterSM3 = SecureUtil.sm3X16Str(strBeforeSM3, encoding); return stringSign.equals(strAfterSM3); } return false; }
is applicable|public boolean (Class<? extends Job> jobType) { Type parameterization = Types.getBaseClass(clazz, JobProperty.class); if (parameterization instanceof ParameterizedType) { ParameterizedType pt = (ParameterizedType) parameterization; Class applicable = Types.erasure(Types.getTypeArgument(pt, 0)); return applicable.isAssignableFrom(jobType); } else { throw new AssertionError(clazz + " doesn't properly parameterize JobProperty. The isApplicable() method must be overridden."); } }
merge initializers|protected final ServletContextInitializer[] (ServletContextInitializer... initializers) { List<ServletContextInitializer> mergedInitializers = new ArrayList<>(); mergedInitializers.add(( servletContext) -> this.initParameters.forEach(servletContext::setInitParameter)); mergedInitializers.add(new SessionConfiguringInitializer(this.session)); mergedInitializers.addAll(Arrays.asList(initializers)); mergedInitializers.addAll(this.initializers); return mergedInitializers.toArray(new ServletContextInitializer[0]); }
compare to|@Override public int (ValueArray<CharValue> o) { CharValueArray other = (CharValueArray) o; int min = Math.min(position, other.position); for (int i = 0; i < min; i++) { int cmp = Character.compare(data[i], other.data[i]); if (cmp != 0) { return cmp; } } return Integer.compare(position, other.position); }
split by length|public static String[] (String str, int len) { int partCount = str.length() / len; int lastPartCount = str.length() % len; int fixPart = 0; if (lastPartCount != 0) { fixPart = 1; } final String[] strs = new String[partCount + fixPart]; for (int i = 0; i < partCount + fixPart; i++) { if (i == partCount + fixPart - 1 && lastPartCount != 0) { strs[i] = str.substring(i * len, i * len + lastPartCount); } else { strs[i] = str.substring(i * len, i * len + len); } } return strs; }
add column|void (String family, String qualifier, Class<?> clazz) { Preconditions.checkNotNull(family, "family name"); Preconditions.checkNotNull(qualifier, "qualifier name"); Preconditions.checkNotNull(clazz, "class type"); Map<String, TypeInformation<?>> qualifierMap = this.familyMap.get(family); if (!HBaseRowInputFormat.isSupportedType(clazz)) { // throw exception throw new IllegalArgumentException("Unsupported class type found " + clazz + ". " + "Better to use byte[].class and deserialize using user defined scalar functions"); } if (qualifierMap == null) { qualifierMap = new LinkedHashMap<>(); } qualifierMap.put(qualifier, TypeExtractor.getForClass(clazz)); familyMap.put(family, qualifierMap); }
register|@Override public void (Metric metric, String metricName, AbstractMetricGroup group) { synchronized (lock) { if (isShutdown()) { LOG.warn("Cannot register metric, because the MetricRegistry has already been shut down."); } else { if (reporters != null) { for (int i = 0; i < reporters.size(); i++) { MetricReporter reporter = reporters.get(i); try { if (reporter != null) { FrontMetricGroup front = new FrontMetricGroup<AbstractMetricGroup<?>>(i, group); reporter.notifyOfAddedMetric(metric, metricName, front); } } catch (Exception e) { LOG.warn("Error while registering metric.", e); } } } try { if (queryService != null) { queryService.addMetric(metricName, metric, group); } } catch (Exception e) { LOG.warn("Error while registering metric.", e); } try { if (metric instanceof View) { if (viewUpdater == null) { viewUpdater = new ViewUpdater(executor); } viewUpdater.notifyOfAddedView((View) metric); } } catch (Exception e) { LOG.warn("Error while registering metric.", e); } } } }
read|public static DoubleArrayTrie (InputStream input) throws IOException { DoubleArrayTrie trie = new DoubleArrayTrie(); DataInputStream dis = new DataInputStream(new BufferedInputStream(input)); trie.compact = dis.readBoolean(); // Read size of baseArr and checkArr int baseCheckSize = dis.readInt(); // Read size of tailArr int tailSize = dis.readInt(); ReadableByteChannel channel = Channels.newChannel(dis); ByteBuffer tmpBaseBuffer = ByteBuffer.allocate(baseCheckSize * 4); channel.read(tmpBaseBuffer); tmpBaseBuffer.rewind(); trie.baseBuffer = tmpBaseBuffer.asIntBuffer(); ByteBuffer tmpCheckBuffer = ByteBuffer.allocate(baseCheckSize * 4); channel.read(tmpCheckBuffer); tmpCheckBuffer.rewind(); trie.checkBuffer = tmpCheckBuffer.asIntBuffer(); ByteBuffer tmpTailBuffer = ByteBuffer.allocate(tailSize * 2); channel.read(tmpTailBuffer); tmpTailBuffer.rewind(); trie.tailBuffer = tmpTailBuffer.asCharBuffer(); input.close(); return trie; }
to map|public static <K, V> HashMap<K, V> (Iterable<Entry<K, V>> entryIter) { final HashMap<K, V> map = new HashMap<K, V>(); if (isNotEmpty(entryIter)) { for (Entry<K, V> entry : entryIter) { map.put(entry.getKey(), entry.getValue()); } } return map; }
output|public <T> T (@NonNull ModelAdapter<T> adapter, INDArray... inputs) { return output(adapter, inputs, null); }
get caches|@Read public Single<Map<String, Object>> () { return Flowable.fromIterable(cacheManager.getCacheNames()).flatMapMaybe( n -> Flowable.fromPublisher(cacheManager.getCache(n).getCacheInfo()).firstElement()).reduce(new HashMap<>(), ( seed, info) -> { seed.put(info.getName(), info.get()); return seed; }).map( objectObjectHashMap -> Collections.singletonMap(NAME, objectObjectHashMap)); }
put if absent|public static synchronized Forest (String key, String path, Forest forest) { KV<String, Forest> kv = DIC.get(key); if (kv != null && kv.getV() != null) { return kv.getV(); } put(key, path, forest); return forest; }
renegotiate|public Future<Channel> () { ChannelHandlerContext ctx = this.ctx; if (ctx == null) { throw new IllegalStateException(); } return renegotiate(ctx.executor().<Channel>newPromise()); }
read message|private void () throws IOException { while (true) { if (closed) throw new IOException("closed"); if (frameLength > 0) { source.readFully(messageFrameBuffer, frameLength); if (!isClient) { messageFrameBuffer.readAndWriteUnsafe(maskCursor); maskCursor.seek(messageFrameBuffer.size() - frameLength); toggleMask(maskCursor, maskKey); maskCursor.close(); } } // We are exhausted and have no continuations. if (isFinalFrame) break; readUntilNonControlFrame(); if (opcode != OPCODE_CONTINUATION) { throw new ProtocolException("Expected continuation opcode. Got: " + toHexString(opcode)); } } }
destroy sso sessions|@DeleteOperation public Map<String, Object> (final String type) { val sessionsMap = new HashMap<String, Object>(); val failedTickets = new HashMap<String, String>(); val option = SsoSessionReportOptions.valueOf(type); val collection = getActiveSsoSessions(option); collection.stream().map( sso -> sso.get(SsoSessionAttributeKeys.TICKET_GRANTING_TICKET.toString()).toString()).forEach( ticketGrantingTicket -> { try { this.centralAuthenticationService.destroyTicketGrantingTicket(ticketGrantingTicket); } catch (final Exception e) { LOGGER.error(e.getMessage(), e); failedTickets.put(ticketGrantingTicket, e.getMessage()); } }); if (failedTickets.isEmpty()) { sessionsMap.put(STATUS, HttpServletResponse.SC_OK); } else { sessionsMap.put(STATUS, HttpServletResponse.SC_INTERNAL_SERVER_ERROR); sessionsMap.put("failedTicketGrantingTickets", failedTickets); } return sessionsMap; }
get sorted applications|@Override public List<Application> () { List<Application> apps = new ArrayList<Application>(getApplications().getRegisteredApplications()); Collections.sort(apps, APP_COMPARATOR); return apps; }
hash|public int () { hash ^= 4 * count; hash ^= hash >>> 16; hash *= 0x85ebca6b; hash ^= hash >>> 13; hash *= 0xc2b2ae35; hash ^= hash >>> 16; return hash; }
schedule|public Scheduler (String id, String pattern, Runnable task) { return schedule(id, new CronPattern(pattern), new RunnableTask(task)); }
new setter|protected final MethodSpec (TypeName varType, String varName, Visibility visibility) { String methodName = "set" + Character.toUpperCase(varName.charAt(0)) + varName.substring(1); String type; if (varType.isPrimitive()) { type = varType.equals(TypeName.INT) ? "Int" : "Long"; } else { type = "Object"; } MethodSpec.Builder setter = MethodSpec.methodBuilder(methodName).addModifiers(context.publicFinalModifiers()).addParameter(varType, varName); if (visibility.isRelaxed) { setter.addStatement("$T.UNSAFE.put$L(this, $N, $N)", UNSAFE_ACCESS, type, offsetName(varName), varName); } else { setter.addStatement("this.$N = $N", varName, varName); } return setter.build(); }
create row from proto|@InternalApi public Row (com.google.bigtable.v2.Row row) { RowBuilder<Row> builder = createRowBuilder(); builder.startRow(row.getKey()); for (Family family : row.getFamiliesList()) { for (Column column : family.getColumnsList()) { for (Cell cell : column.getCellsList()) { builder.startCell(family.getName(), column.getQualifier(), cell.getTimestampMicros(), cell.getLabelsList(), cell.getValue().size()); builder.cellValue(cell.getValue()); builder.finishCell(); } } } return builder.finishRow(); }
explan special cond with both sides are property|private boolean (SQLBinaryOpExpr bExpr, Where where) throws SqlParseException { //join is not support if ((bExpr.getLeft() instanceof SQLPropertyExpr || bExpr.getLeft() instanceof SQLIdentifierExpr) && (bExpr.getRight() instanceof SQLPropertyExpr || bExpr.getRight() instanceof SQLIdentifierExpr) && Sets.newHashSet("=", "<", ">", ">=", "<=").contains(bExpr.getOperator().getName()) && !Util.isFromJoinOrUnionTable(bExpr)) { SQLMethodInvokeExpr sqlMethodInvokeExpr = new SQLMethodInvokeExpr("script", null); String operator = bExpr.getOperator().getName(); if (operator.equals("=")) { operator = "=="; } String leftProperty = Util.expr2Object(bExpr.getLeft()).toString(); String rightProperty = Util.expr2Object(bExpr.getRight()).toString(); if (leftProperty.split("\\.").length > 1) { leftProperty = leftProperty.substring(leftProperty.split("\\.")[0].length() + 1); } if (rightProperty.split("\\.").length > 1) { rightProperty = rightProperty.substring(rightProperty.split("\\.")[0].length() + 1); } sqlMethodInvokeExpr.addParameter(new SQLCharExpr("doc['" + leftProperty + "'].value " + operator + " doc['" + rightProperty + "'].value")); explanCond("AND", sqlMethodInvokeExpr, where); return true; } return false; }
do on subscribe|@Override protected void (Subscription subscription) { subscription.request(1); }
rules|@Override public List<AlternateTypeRule> () { return singletonList(newRule(resolver.resolve(Pageable.class), resolver.resolve(pageableMixin(restConfiguration)))); }
put|private Object (Class<?> type, boolean isSingleton) { return put(type.getName(), type, isSingleton); }
vary matches|public static boolean (Response cachedResponse, Headers cachedRequest, Request newRequest) { for (String field : varyFields(cachedResponse)) { if (!Objects.equals(cachedRequest.values(field), newRequest.headers(field))) return false; } return true; }
get param names|public String[] () { Vector<String> v = new Vector<>(); SortedSet<String> pns = this.getParamNameSet(HtmlParameter.Type.url); Iterator<String> iterator = pns.iterator(); while (iterator.hasNext()) { String name = iterator.next(); if (name != null) { v.add(name); } } if (getRequestHeader().getMethod().equalsIgnoreCase(HttpRequestHeader.POST)) { pns = this.getParamNameSet(HtmlParameter.Type.form); iterator = pns.iterator(); while (iterator.hasNext()) { String name = iterator.next(); if (name != null) { v.add(name); } } } String[] a = new String[v.size()]; v.toArray(a); return a; }
get bean provider|@Nonnull protected <T> Provider<T> (@Nullable BeanResolutionContext resolutionContext, @Nonnull Class<T> beanType) { return getBeanProvider(resolutionContext, beanType, null); }
apply overrides|protected void (Alert alert) { if (this.alertOverrides.isEmpty()) { // Nothing to do return; } String changedName = this.alertOverrides.getProperty(alert.getPluginId() + ".name"); if (changedName != null) { alert.setName(applyOverride(alert.getName(), changedName)); } String changedDesc = this.alertOverrides.getProperty(alert.getPluginId() + ".description"); if (changedDesc != null) { alert.setDescription(applyOverride(alert.getDescription(), changedDesc)); } String changedSolution = this.alertOverrides.getProperty(alert.getPluginId() + ".solution"); if (changedSolution != null) { alert.setSolution(applyOverride(alert.getSolution(), changedSolution)); } String changedOther = this.alertOverrides.getProperty(alert.getPluginId() + ".otherInfo"); if (changedOther != null) { alert.setOtherInfo(applyOverride(alert.getOtherInfo(), changedOther)); } String changedReference = this.alertOverrides.getProperty(alert.getPluginId() + ".reference"); if (changedReference != null) { alert.setReference(applyOverride(alert.getReference(), changedReference)); } }
model to do|private CanalDO (Canal canal) { CanalDO canalDo = new CanalDO(); try { canalDo.setId(canal.getId()); canalDo.setName(canal.getName()); canalDo.setStatus(canal.getStatus()); canalDo.setDescription(canal.getDesc()); canalDo.setParameters(canal.getCanalParameter()); canalDo.setGmtCreate(canal.getGmtCreate()); canalDo.setGmtModified(canal.getGmtModified()); } catch (Exception e) { logger.error("ERROR ## change the canal Model to Do has an exception"); throw new ManagerException(e); } return canalDo; }
get term frequency|public int (String term) { TermFrequency termFrequency = trieSingle.get(term); if (termFrequency == null) return 0; return termFrequency.getValue(); }
read by sax|public static void (File file, int sheetIndex, RowHandler rowHandler) { BufferedInputStream in = null; try { in = FileUtil.getInputStream(file); readBySax(in, sheetIndex, rowHandler); } finally { IoUtil.close(in); } }
disabled|public static QueryableStateConfiguration () { final Iterator<Integer> proxyPorts = NetUtils.getPortRangeFromString(QueryableStateOptions.PROXY_PORT_RANGE.defaultValue()); final Iterator<Integer> serverPorts = NetUtils.getPortRangeFromString(QueryableStateOptions.SERVER_PORT_RANGE.defaultValue()); return new QueryableStateConfiguration(proxyPorts, serverPorts, 0, 0, 0, 0); }
add root file seed|private void (URI baseUri, String fileName) { String seed = buildUri(baseUri.getScheme(), baseUri.getRawHost(), baseUri.getPort(), "/" + fileName); try { this.seedList.add(new URI(seed, true)); } catch (Exception e) { log.warn("Error while creating [" + fileName + "] seed: " + seed, e); } }
submit|public Observable<T> (final ServerOperation<T> operation) { final ExecutionInfoContext context = new ExecutionInfoContext(); if (listenerInvoker != null) { try { listenerInvoker.onExecutionStart(); } catch (AbortExecutionException e) { return Observable.error(e); } } final int maxRetrysSame = retryHandler.getMaxRetriesOnSameServer(); final int maxRetrysNext = retryHandler.getMaxRetriesOnNextServer(); // Use the load balancer Observable<T> o = (server == null ? selectServer() : Observable.just(server)).concatMap(new Func1<Server, Observable<T>>() { @Override public // Called for each server being selected Observable<T> call(Server server) { context.setServer(server); final ServerStats stats = loadBalancerContext.getServerStats(server); // Called for each attempt and retry Observable<T> o = Observable.just(server).concatMap(new Func1<Server, Observable<T>>() { @Override public Observable<T> call(final Server server) { context.incAttemptCount(); loadBalancerContext.noteOpenConnection(stats); if (listenerInvoker != null) { try { listenerInvoker.onStartWithServer(context.toExecutionInfo()); } catch (AbortExecutionException e) { return Observable.error(e); } } final Stopwatch tracer = loadBalancerContext.getExecuteTracer().start(); return operation.call(server).doOnEach(new Observer<T>() { private T entity; @Override public void onCompleted() { recordStats(tracer, stats, entity, null); // TODO: What to do if onNext or onError are never called? } @Override public void onError(Throwable e) { recordStats(tracer, stats, null, e); logger.debug("Got error {} when executed on server {}", e, server); if (listenerInvoker != null) { listenerInvoker.onExceptionWithServer(e, context.toExecutionInfo()); } } @Override public void onNext(T entity) { this.entity = entity; if (listenerInvoker != null) { listenerInvoker.onExecutionSuccess(entity, context.toExecutionInfo()); } } private void recordStats(Stopwatch tracer, ServerStats stats, Object entity, Throwable exception) { tracer.stop(); loadBalancerContext.noteRequestCompletion(stats, entity, exception, tracer.getDuration(TimeUnit.MILLISECONDS), retryHandler); } }); } }); if (maxRetrysSame > 0) o = o.retry(retryPolicy(maxRetrysSame, true)); return o; } }); if (maxRetrysNext > 0 && server == null) o = o.retry(retryPolicy(maxRetrysNext, false)); return o.onErrorResumeNext(new Func1<Throwable, Observable<T>>() { @Override public Observable<T> call(Throwable e) { if (context.getAttemptCount() > 0) { if (maxRetrysNext > 0 && context.getServerAttemptCount() == (maxRetrysNext + 1)) { e = new ClientException(ClientException.ErrorType.NUMBEROF_RETRIES_NEXTSERVER_EXCEEDED, "Number of retries on next server exceeded max " + maxRetrysNext + " retries, while making a call for: " + context.getServer(), e); } else if (maxRetrysSame > 0 && context.getAttemptCount() == (maxRetrysSame + 1)) { e = new ClientException(ClientException.ErrorType.NUMBEROF_RETRIES_EXEEDED, "Number of retries exceeded max " + maxRetrysSame + " retries, while making a call for: " + context.getServer(), e); } } if (listenerInvoker != null) { listenerInvoker.onExecutionFailed(e, context.toFinalExecutionInfo()); } return Observable.error(e); } }); }
load remote config|@Override public void () { try { // 加载远程adapter配置 ConfigItem configItem = getRemoteAdapterConfig(); if (configItem != null) { if (configItem.getModifiedTime() != currentConfigTimestamp) { currentConfigTimestamp = configItem.getModifiedTime(); overrideLocalCanalConfig(configItem.getContent()); logger.info("## Loaded remote adapter config: application.yml"); } } } catch (Exception e) { logger.error(e.getMessage(), e); } }
to str|public static String (Number number) { if (null == number) { throw new NullPointerException("Number is null !"); } if (false == ObjectUtil.isValidIfNumber(number)) { throw new IllegalArgumentException("Number is non-finite!"); } // 去掉小数点儿后多余的0 String string = number.toString(); if (string.indexOf('.') > 0 && string.indexOf('e') < 0 && string.indexOf('E') < 0) { while (string.endsWith("0")) { string = string.substring(0, string.length() - 1); } if (string.endsWith(".")) { string = string.substring(0, string.length() - 1); } } return string; }
supply async|public static <T> CompletableFuture<T> (SupplierWithException<T, ?> supplier, Executor executor) { return CompletableFuture.supplyAsync(() -> { try { return supplier.get(); } catch (Throwable e) { throw new CompletionException(e); } }, executor); }
set sort triangle vertices|public TriangleListingBase<K, VV, EV, R> (boolean sortTriangleVertices) { this.sortTriangleVertices.set(sortTriangleVertices); return this; }
method|@Deprecated public RequestTemplate (String method) { checkNotNull(method, "method"); try { this.method = HttpMethod.valueOf(method); } catch (IllegalArgumentException iae) { throw new IllegalArgumentException("Invalid HTTP Method: " + method); } return this; }
get name|public UnixPath (int index) { if (path.isEmpty()) { return this; } try { return new UnixPath(permitEmptyComponents, getParts().get(index)); } catch (IndexOutOfBoundsException e) { throw new IllegalArgumentException(); } }
start instance|@BetaApi public final Operation (String instance) { StartInstanceHttpRequest request = StartInstanceHttpRequest.newBuilder().setInstance(instance).build(); return startInstance(request); }
get|@Deprecated public Blade (@NonNull String path, @NonNull RouteHandler0 handler) { this.routeMatcher.addRoute(path, handler, HttpMethod.GET); return this; }
size at|public SDVariable (String name, SDVariable in, int dimension) { SDVariable ret = f().sizeAt(in, dimension); return updateVariableNameAndReference(ret, name); }
all for|public static INDArrayIndex[] (INDArray arr) { INDArrayIndex[] ret = new INDArrayIndex[arr.rank()]; for (int i = 0; i < ret.length; i++) ret[i] = NDArrayIndex.all(); return ret; }
remove health check target pool|@BetaApi public final Operation (ProjectRegionTargetPoolName targetPool, TargetPoolsRemoveHealthCheckRequest targetPoolsRemoveHealthCheckRequestResource) { RemoveHealthCheckTargetPoolHttpRequest request = RemoveHealthCheckTargetPoolHttpRequest.newBuilder().setTargetPool(targetPool == null ? null : targetPool.toString()).setTargetPoolsRemoveHealthCheckRequestResource(targetPoolsRemoveHealthCheckRequestResource).build(); return removeHealthCheckTargetPool(request); }
get pin yin|public static String (String chinese) { final StrBuilder result = StrUtil.strBuilder(); String strTemp = null; int len = chinese.length(); for (int j = 0; j < len; j++) { strTemp = chinese.substring(j, j + 1); int ascii = getChsAscii(strTemp); if (ascii > 0) { //非汉字 result.append((char) ascii); } else { for (int i = pinyinValue.length - 1; i >= 0; i--) { if (pinyinValue[i] <= ascii) { result.append(pinyinStr[i]); break; } } } } return result.toString(); }
size|public int () { int count = 0; for (Node<E> p = first(); p != null; p = succ(p)) if (p.item != null) // Collection.size() spec says to max out if (++count == Integer.MAX_VALUE) break; return count; }
trim right|public BinaryString (BinaryString trimStr) { ensureMaterialized(); if (trimStr == null) { return null; } trimStr.ensureMaterialized(); if (trimStr.isSpaceString()) { return trimRight(); } if (inFirstSegment()) { int charIdx = 0; int byteIdx = 0; // each element in charLens is length of character in the source string int[] charLens = new int[sizeInBytes]; // each element in charStartPos is start position of first byte in the source string int[] charStartPos = new int[sizeInBytes]; while (byteIdx < sizeInBytes) { charStartPos[charIdx] = byteIdx; charLens[charIdx] = numBytesForFirstByte(getByteOneSegment(byteIdx)); byteIdx += charLens[charIdx]; charIdx++; } // searchIdx points to the first character which is not in trim string from the right // end. int searchIdx = sizeInBytes - 1; charIdx -= 1; while (charIdx >= 0) { BinaryString currentChar = copyBinaryStringInOneSeg(charStartPos[charIdx], charStartPos[charIdx] + charLens[charIdx] - 1); if (trimStr.contains(currentChar)) { searchIdx -= charLens[charIdx]; } else { break; } charIdx--; } if (searchIdx < 0) { // empty string return EMPTY_UTF8; } else { return copyBinaryStringInOneSeg(0, searchIdx); } } else { return trimRightSlow(trimStr); } }
evaluate|public BinaryClassificationFMeasure (Instance[] instanceList) { int TP = 0, FP = 0, FN = 0; for (Instance instance : instanceList) { int y = model.decode(instance.x); if (y == 1) { if (instance.y == 1) ++TP; else ++FP; } else if (instance.y == 1) ++FN; } float p = TP / (float) (TP + FP) * 100; float r = TP / (float) (TP + FN) * 100; return new BinaryClassificationFMeasure(p, r, 2 * p * r / (p + r)); }
get key generator|public static KeyGenerator (String algorithm) { final Provider provider = GlobalBouncyCastleProvider.INSTANCE.getProvider(); KeyGenerator generator; try { generator = // (null == provider) ? // KeyGenerator.getInstance(getMainAlgorithm(algorithm)) : KeyGenerator.getInstance(getMainAlgorithm(algorithm), provider); } catch (NoSuchAlgorithmException e) { throw new CryptoException(e); } return generator; }
set resources|private DeltaIteration<ST, WT> (ResourceSpec minResources, ResourceSpec preferredResources) { Preconditions.checkNotNull(minResources, "The min resources must be not null."); Preconditions.checkNotNull(preferredResources, "The preferred resources must be not null."); Preconditions.checkArgument(minResources.isValid() && preferredResources.isValid() && minResources.lessThanOrEqual(preferredResources), "The values in resources must be not less than 0 and the preferred resources must be greater than the min resources."); this.minResources = minResources; this.preferredResources = preferredResources; return this; }
load class|private Class<?> (String className) { Class<?> clazz = null; try { clazz = Class.forName(className, this.initialize, ClassUtil.getClassLoader()); } catch (NoClassDefFoundError e) { } catch (UnsupportedClassVersionError e) { } catch (Exception e) { throw new RuntimeException(e); } return clazz; }
fix entity|private Entity (Entity entity) { if (null == entity) { entity = Entity.create(tableName); } else if (StrUtil.isBlank(entity.getTableName())) { entity.setTableName(tableName); } return entity; }
enable xpn resource project|@BetaApi public final Operation (ProjectName project, ProjectsEnableXpnResourceRequest projectsEnableXpnResourceRequestResource) { EnableXpnResourceProjectHttpRequest request = EnableXpnResourceProjectHttpRequest.newBuilder().setProject(project == null ? null : project.toString()).setProjectsEnableXpnResourceRequestResource(projectsEnableXpnResourceRequestResource).build(); return enableXpnResourceProject(request); }
generate output variable for op|public SDVariable[] (DifferentialFunction function, String baseName, boolean isImport) { if (baseName == null || baseName.isEmpty() && getBaseNameForFunction(function) != null) baseName = getBaseNameForFunction(function); if (baseName == null) baseName = function.opName(); List<org.nd4j.linalg.api.buffer.DataType> outputDataTypes = null; if (!isImport) { List<org.nd4j.linalg.api.buffer.DataType> inputDataTypes = new ArrayList<>(); List<String> fnInputs = ops.get(function.getOwnName()).getInputsToOp(); if (fnInputs != null) { for (String var : fnInputs) { inputDataTypes.add(variables.get(var).getVariable().dataType()); } } outputDataTypes = function.calculateOutputDataTypes(inputDataTypes); } val outputShape = function.calculateOutputShape(); if (outputShape == null || outputShape.isEmpty()) { if (function instanceof CustomOp) { CustomOp customOp = (CustomOp) function; int num_outputs = function.getNumOutputs(); if (num_outputs <= 0) { val descriptor = customOp.getDescriptor(); if (descriptor != null) { num_outputs = descriptor.getNumOutputs(); } if (num_outputs <= 0) { throw new ND4UnresolvedOutputVariables("Could not determine number of output variables for op " + function.getOwnName() + " - " + function.getClass().getSimpleName() + ". Ops can override" + " getNumOutputs() to specify number of outputs if required"); } } char ordering = 'c'; SDVariable[] args = function.args(); if (args != null && args.length > 0 && args[0].getArr() != null) { ordering = function.args()[0].getArr().ordering(); } SDVariable[] ret = new SDVariable[num_outputs]; Preconditions.checkState(isImport || num_outputs == 0 || (outputDataTypes != null && outputDataTypes.size() == num_outputs), "Incorrect number of output datatypes: got %s but expected datatypes for %s outputs - %s (op: %s)", (outputDataTypes == null ? null : outputDataTypes.size()), num_outputs, outputDataTypes, function.getClass().getSimpleName()); for (int i = 0; i < ret.length; i++) { SDVariable var = (i == 0 ? getVariable(baseName) : getVariable(baseName + ":" + i)); if (var == null) { org.nd4j.linalg.api.buffer.DataType dataType = isImport ? null : outputDataTypes.get(i); var = var(generateNewVarName(baseName, i), VariableType.ARRAY, null, dataType, (long[]) null); } var.setOutputIndex(i); var.setCreator(function); ret[i] = var; } if (getOutputsForFunction(function) == null) addOutgoingFor(ret, function); return ret; } else if (function instanceof BaseOp && outputShape.isEmpty()) { SDVariable[] ret = new SDVariable[1]; SDVariable checkGet = getVariable(baseName); char ordering = 'c'; SDVariable[] args = function.args(); if (args != null && args.length > 0 && function.args()[0].getArr() != null) { ordering = function.args()[0].getArr().ordering(); } if (checkGet == null) { org.nd4j.linalg.api.buffer.DataType dataType = outputDataTypes.get(0); checkGet = var(baseName, VariableType.ARRAY, null, dataType, (long[]) null); } if (checkGet == null) { org.nd4j.linalg.api.buffer.DataType dataType = outputDataTypes.get(0); checkGet = var(baseName, VariableType.ARRAY, null, dataType, (long[]) null); } checkGet.setOutputIndex(0); checkGet.setCreator(function); ret[0] = checkGet; if (getOutputsForFunction(function) == null) addOutgoingFor(ret, function); return ret; } } if (!isImport) { for (int i = 0; i < outputShape.size(); i++) { org.nd4j.linalg.api.buffer.DataType shapeDataType = outputShape.get(i).dataType(); org.nd4j.linalg.api.buffer.DataType calcType = outputDataTypes.get(i); Preconditions.checkState(calcType == shapeDataType, "Calculated output data types do not match for shape calculation vs. datatype calculation:" + " %s vs %s for op %s output %s", shapeDataType, calcType, function.getClass().getName(), i); } } char ordering = 'c'; if (function.args() != null && function.args().length > 0 && function.args()[0].getArr() != null) { ordering = function.args()[0].getArr().ordering(); } SDVariable[] ret = new SDVariable[outputShape.size()]; // ownName/baseName will be used to get variables names val ownName = function.getOwnName(); val rootName = baseName; for (int i = 0; i < ret.length; i++) { LongShapeDescriptor shape = outputShape.get(i); // it should be: rootName:index. i.e.: split:1, split:2, split:3, split:4 etc baseName = rootName + (i > 0 ? ":" + i : ""); SDVariable checkGet = getVariable(baseName); if (checkGet == null) { // obviously - there's no such var, just add it //Note: output of an op is ARRAY type - activations, not a trainable parameter. Thus has no weight init scheme checkGet = var(baseName, VariableType.ARRAY, null, shape.dataType(), shape.getShape()); } else if (shape != null && !shapeAlreadyExistsForVarName(checkGet.getVarName())) { // var exists, let's update its shape putShapeForVarName(checkGet.getVarName(), shape); } else if (shape != null && shapeAlreadyExistsForVarName(checkGet.getVarName())) { // no-op. // TODO: maybe we should check shapes equality here? // it's either var that already exist, or something bad happening } if (checkGet == null) { //TODO FIX THIS org.nd4j.linalg.api.buffer.DataType dataType = org.nd4j.linalg.api.buffer.DataType.FLOAT; checkGet = var(baseName + (i > 0 ? ":" + i : ""), new ZeroInitScheme(ordering), dataType, shape.getShape()); } checkGet.setOutputIndex(i); checkGet.setCreator(function); ret[i] = checkGet; } return ret; }
new ldaptive search request|public static SearchRequest (final String baseDn, final SearchFilter filter, final String[] binaryAttributes, final String[] returnAttributes) { val sr = new SearchRequest(baseDn, filter); sr.setBinaryAttributes(binaryAttributes); sr.setReturnAttributes(returnAttributes); sr.setSearchScope(SearchScope.SUBTREE); return sr; }
evaluate|public <T extends Evaluation> T (DataSetIterator iterator) { return (T) evaluate(iterator, (List<String>) null); }
valid message|public static MessageValidity (NDArrayMessage message) { if (message.getDimensions() == null || message.getArr() == null) return MessageValidity.NULL_VALUE; if (message.getIndex() != -1 && message.getDimensions().length == 1 && message.getDimensions()[0] != -1) return MessageValidity.INCONSISTENT_DIMENSIONS; return MessageValidity.VALID; }
classify text|public final ClassifyTextResponse (Document document) { ClassifyTextRequest request = ClassifyTextRequest.newBuilder().setDocument(document).build(); return classifyText(request); }
from|public static RequestTemplate (RequestTemplate requestTemplate) { RequestTemplate template = new RequestTemplate(requestTemplate.target, requestTemplate.fragment, requestTemplate.uriTemplate, requestTemplate.method, requestTemplate.charset, requestTemplate.body, requestTemplate.decodeSlash, requestTemplate.collectionFormat); if (!requestTemplate.queries().isEmpty()) { template.queries.putAll(requestTemplate.queries); } if (!requestTemplate.headers().isEmpty()) { template.headers.putAll(requestTemplate.headers); } return template; }
call|@Override public HystrixEventType.Collapser () { return HystrixEventType.Collapser.BATCH_EXECUTED; }
form|public HttpRequest (Map<String, Object> formMap) { if (MapUtil.isNotEmpty(formMap)) { for (Map.Entry<String, Object> entry : formMap.entrySet()) { form(entry.getKey(), entry.getValue()); } } return this; }
cancel pending take|public void () { CompletableFuture<Queue<T>> pending; synchronized (this.contents) { pending = this.pendingTake; this.pendingTake = null; } // Cancel any pending poll request. if (pending != null) { pending.cancel(true); } }
find all in content|public List<String> (String content) { List<String> results = new LinkedList<String>(); // First check for all simple exact occurrences for (BoyerMooreMatcher matcher : strings) { if (matcher.findInContent(content) >= 0) results.add(matcher.getPattern()); } // Then check for all regex occurrences Matcher matcher; for (Pattern pattern : patterns) { matcher = pattern.matcher(content); if (matcher.find()) { results.add(content); } } return results; }
get proxied user|public static UserGroupInformation (final Properties prop, final Logger log, final Configuration conf) throws IOException { final String toProxy = verifySecureProperty(prop, JobProperties.USER_TO_PROXY, log); final UserGroupInformation user = getProxiedUser(toProxy, prop, log, conf); if (user == null) { throw new IOException("Proxy as any user in unsecured grid is not supported!" + prop.toString()); } log.info("created proxy user for " + user.getUserName() + user.toString()); return user; }
get process piece|private static int () { // 进程码 // 因为静态变量类加载可能相同,所以要获取进程ID + 加载对象的ID值 final int processPiece; // 进程ID初始化 int processId; try { // 获取进程ID final String processName = ManagementFactory.getRuntimeMXBean().getName(); final int atIndex = processName.indexOf('@'); if (atIndex > 0) { processId = Integer.parseInt(processName.substring(0, atIndex)); } else { processId = processName.hashCode(); } } catch (Throwable t) { processId = RandomUtil.randomInt(); } final ClassLoader loader = ClassLoaderUtil.getClassLoader(); // 返回对象哈希码,无论是否重写hashCode方法 int loaderId = (loader != null) ? System.identityHashCode(loader) : 0; // 进程ID + 对象加载ID StringBuilder processSb = new StringBuilder(); processSb.append(Integer.toHexString(processId)); processSb.append(Integer.toHexString(loaderId)); // 保留前2位 processPiece = processSb.toString().hashCode() & 0xFFFF; return processPiece; }
delete child executions|protected void (ExecutionEntity parentExecution, boolean deleteExecution, CommandContext commandContext) { // Delete all child executions ExecutionEntityManager executionEntityManager = commandContext.getExecutionEntityManager(); Collection<ExecutionEntity> childExecutions = executionEntityManager.findChildExecutionsByParentExecutionId(parentExecution.getId()); if (CollectionUtil.isNotEmpty(childExecutions)) { for (ExecutionEntity childExecution : childExecutions) { deleteChildExecutions(childExecution, true, commandContext); } } if (deleteExecution) { executionEntityManager.deleteExecutionAndRelatedData(parentExecution, null, false); } }
compare to|public int (UUID val) { // can simply be numerically compared as two numbers return (// this.mostSigBits < val.mostSigBits ? // -1 : (// this.mostSigBits > val.mostSigBits ? // 1 : (// this.leastSigBits < val.leastSigBits ? // -1 : (// this.leastSigBits > val.leastSigBits ? // 1 : 0)))); }
get and filter expired entries|protected Map<K, Expirable<V>> (Set<? extends K> keys, boolean updateAccessTime) { Map<K, Expirable<V>> result = new HashMap<>(cache.getAllPresent(keys)); int[] expired = { 0 }; long[] millis = { 0L }; result.entrySet().removeIf( entry -> { if (!entry.getValue().isEternal() && (millis[0] == 0L)) { millis[0] = currentTimeMillis(); } if (entry.getValue().hasExpired(millis[0])) { cache.asMap().computeIfPresent(entry.getKey(), ( k, expirable) -> { if (expirable == entry.getValue()) { dispatcher.publishExpired(this, entry.getKey(), entry.getValue().get()); expired[0]++; return null; } return expirable; }); return true; } if (updateAccessTime) { setAccessExpirationTime(entry.getValue(), millis[0]); } return false; }); statistics.recordHits(result.size()); statistics.recordMisses(keys.size() - result.size()); statistics.recordEvictions(expired[0]); return result; }
new resource leak detector|public final <T> ResourceLeakDetector<T> (Class<T> resource) { return newResourceLeakDetector(resource, ResourceLeakDetector.SAMPLING_INTERVAL); }
transfer back to vocab cache|public void (VocabCache cache, boolean emptyHolder) { if (!(cache instanceof InMemoryLookupCache)) throw new IllegalStateException("Sorry, only InMemoryLookupCache use implemented."); // make sure that huffman codes are updated before transfer //updateHuffmanCodes(); List<VocabularyWord> words = words(); for (VocabularyWord word : words) { if (word.getWord().isEmpty()) continue; VocabWord vocabWord = new VocabWord(1, word.getWord()); // if we're transferring full model, it CAN contain HistoricalGradient for AdaptiveGradient feature if (word.getHistoricalGradient() != null) { INDArray gradient = Nd4j.create(word.getHistoricalGradient()); vocabWord.setHistoricalGradient(gradient); } // put VocabWord into both Tokens and Vocabs maps ((InMemoryLookupCache) cache).getVocabs().put(word.getWord(), vocabWord); ((InMemoryLookupCache) cache).getTokens().put(word.getWord(), vocabWord); // update Huffman tree information if (word.getHuffmanNode() != null) { vocabWord.setIndex(word.getHuffmanNode().getIdx()); vocabWord.setCodeLength(word.getHuffmanNode().getLength()); vocabWord.setPoints(arrayToList(word.getHuffmanNode().getPoint(), word.getHuffmanNode().getLength())); vocabWord.setCodes(arrayToList(word.getHuffmanNode().getCode(), word.getHuffmanNode().getLength())); // put word into index cache.addWordToIndex(word.getHuffmanNode().getIdx(), word.getWord()); } // >1 hack is required since VocabCache impl imples 1 as base word count, not 0 if (word.getCount() > 1) cache.incrementWordCount(word.getWord(), word.getCount() - 1); } // at this moment its pretty safe to nullify all vocabs. if (emptyHolder) { idxMap.clear(); vocabulary.clear(); } }
start|@Deprecated public Blade (Class<?> bootClass, @NonNull String address, int port, String... args) { return this; }
press text|public static BufferedImage (Image srcImage, String pressText, Color color, Font font, int x, int y, float alpha) { return Img.from(srcImage).pressText(pressText, color, font, x, y, alpha).getImg(); }
call|@Override public HystrixEventType.Collapser () { return HystrixEventType.Collapser.RESPONSE_FROM_CACHE; }
get value|@Override public Number () { return properties.metricsRollingStatisticalWindowInMilliseconds().get(); }
grpc headers server handler|@Benchmark @BenchmarkMode(Mode.AverageTime) @OutputTimeUnit(TimeUnit.NANOSECONDS) public void (Blackhole bh) { serverHandler(bh, new GrpcHttp2RequestHeaders(4)); }
add loss variable|public void (@NonNull String variableName) { Preconditions.checkState(hasVariable(variableName), "No variable with name \"%s\" exists", variableName); SDVariable v = getVariable(variableName); Preconditions.checkState(v.dataType().isFPType(), "Only floating point type variables can be marked as losses to be minimized." + " SDVariable \"%s\" has datatype %s", variableName, v.dataType()); Preconditions.checkState(v.getVariableType() == VariableType.ARRAY, "Only ARRAY type SDVariables can be marked as losses to be minimized." + " SDVariable \"%s\" has variable type %s", variableName, v.getVariableType()); if (!lossVariables.contains(variableName)) { lossVariables.add(variableName); } }
retention|public CompletableFuture<Void> (final String scope, final String stream, final RetentionPolicy policy, final long recordingTime, final OperationContext contextOpt, final String delegationToken) { Preconditions.checkNotNull(policy); final OperationContext context = contextOpt == null ? streamMetadataStore.createContext(scope, stream) : contextOpt; final long requestId = requestTracker.getRequestIdFor("truncateStream", scope, stream); return streamMetadataStore.getRetentionSet(scope, stream, context, executor).thenCompose( retentionSet -> { StreamCutReferenceRecord latestCut = retentionSet.getLatest(); return generateStreamCutIfRequired(scope, stream, latestCut, recordingTime, context, delegationToken).thenCompose( newRecord -> truncate(scope, stream, policy, context, retentionSet, newRecord, recordingTime, requestId)); }).thenAccept( x -> StreamMetrics.reportRetentionEvent(scope, stream)); }
to big integer|public static BigInteger (Object value, BigInteger defaultValue) { return convert(BigInteger.class, value, defaultValue); }
to frame|static ByteBuffer (ByteBuffer input, int dataSize) throws GeneralSecurityException { Preconditions.checkNotNull(input); if (dataSize > input.remaining()) { dataSize = input.remaining(); } Producer producer = new Producer(); ByteBuffer inputAlias = input.duplicate(); inputAlias.limit(input.position() + dataSize); producer.readBytes(inputAlias); producer.flush(); input.position(inputAlias.position()); ByteBuffer output = producer.getRawFrame(); return output; }
link|static String (Key key, String column, long row, String match) { return "/2/Find?key=" + key + (column == null ? "" : "&column=" + column) + "&row=" + row + (match == null ? "" : "&match=" + match); }
copy|public static ByteBuffer (ByteBuffer src, ByteBuffer dest) { return copy(src, dest, Math.min(src.limit(), dest.remaining())); }
page|public PageResult<Entity> (Connection conn, Collection<String> fields, Entity where, int page, int numPerPage) throws SQLException { checkConn(conn); final int count = count(conn, where); PageResultHandler pageResultHandler = PageResultHandler.create(new PageResult<Entity>(page, numPerPage, count)); return this.page(conn, fields, where, page, numPerPage, pageResultHandler); }
submit job|@Override public CompletableFuture<JobSubmissionResult> (@Nonnull JobGraph jobGraph) { // we have to enable queued scheduling because slot will be allocated lazily jobGraph.setAllowQueuedScheduling(true); CompletableFuture<java.nio.file.Path> jobGraphFileFuture = CompletableFuture.supplyAsync(() -> { try { final java.nio.file.Path jobGraphFile = Files.createTempFile("flink-jobgraph", ".bin"); try (ObjectOutputStream objectOut = new ObjectOutputStream(Files.newOutputStream(jobGraphFile))) { objectOut.writeObject(jobGraph); } return jobGraphFile; } catch (IOException e) { throw new CompletionException(new FlinkException("Failed to serialize JobGraph.", e)); } }, executorService); CompletableFuture<Tuple2<JobSubmitRequestBody, Collection<FileUpload>>> requestFuture = jobGraphFileFuture.thenApply( jobGraphFile -> { List<String> jarFileNames = new ArrayList<>(8); List<JobSubmitRequestBody.DistributedCacheFile> artifactFileNames = new ArrayList<>(8); Collection<FileUpload> filesToUpload = new ArrayList<>(8); filesToUpload.add(new FileUpload(jobGraphFile, RestConstants.CONTENT_TYPE_BINARY)); for (Path jar : jobGraph.getUserJars()) { jarFileNames.add(jar.getName()); filesToUpload.add(new FileUpload(Paths.get(jar.toUri()), RestConstants.CONTENT_TYPE_JAR)); } for (Map.Entry<String, DistributedCache.DistributedCacheEntry> artifacts : jobGraph.getUserArtifacts().entrySet()) { artifactFileNames.add(new JobSubmitRequestBody.DistributedCacheFile(artifacts.getKey(), new Path(artifacts.getValue().filePath).getName())); filesToUpload.add(new FileUpload(Paths.get(artifacts.getValue().filePath), RestConstants.CONTENT_TYPE_BINARY)); } final JobSubmitRequestBody requestBody = new JobSubmitRequestBody(jobGraphFile.getFileName().toString(), jarFileNames, artifactFileNames); return Tuple2.of(requestBody, Collections.unmodifiableCollection(filesToUpload)); }); final CompletableFuture<JobSubmitResponseBody> submissionFuture = requestFuture.thenCompose( requestAndFileUploads -> sendRetriableRequest(JobSubmitHeaders.getInstance(), EmptyMessageParameters.getInstance(), requestAndFileUploads.f0, requestAndFileUploads.f1, isConnectionProblemOrServiceUnavailable())); submissionFuture.thenCombine(jobGraphFileFuture, ( ignored, jobGraphFile) -> jobGraphFile).thenAccept( jobGraphFile -> { try { Files.delete(jobGraphFile); } catch (IOException e) { log.warn("Could not delete temporary file {}.", jobGraphFile, e); } }); return submissionFuture.thenApply((JobSubmitResponseBody jobSubmitResponseBody) -> new JobSubmissionResult(jobGraph.getJobID())).exceptionally((Throwable throwable) -> { throw new CompletionException(new JobSubmissionException(jobGraph.getJobID(), "Failed to submit JobGraph.", ExceptionUtils.stripCompletionException(throwable))); }); }
delete instance async|@SuppressWarnings("WeakerAccess") public ApiFuture<Void> (String instanceId) { String instanceName = NameUtil.formatInstanceName(projectId, instanceId); com.google.bigtable.admin.v2.DeleteInstanceRequest request = com.google.bigtable.admin.v2.DeleteInstanceRequest.newBuilder().setName(instanceName).build(); return ApiFutures.transform(stub.deleteInstanceCallable().futureCall(request), new ApiFunction<Empty, Void>() { @Override public Void apply(Empty input) { return null; } }, MoreExecutors.directExecutor()); }
get strings with default|public String[] (String key, String[] defaultValue) { String[] value = getStrings(key, null); if (null == value) { value = defaultValue; } return value; }
handle annotation|public void (JCCompilationUnit unit, JavacNode node, JCAnnotation annotation, long priority) { TypeResolver resolver = new TypeResolver(node.getImportList()); String rawType = annotation.annotationType.toString(); String fqn = resolver.typeRefToFullyQualifiedName(node, typeLibrary, rawType); if (fqn == null) return; List<AnnotationHandlerContainer<?>> containers = annotationHandlers.get(fqn); if (containers == null) return; for (AnnotationHandlerContainer<?> container : containers) { try { if (container.getPriority() == priority) { if (checkAndSetHandled(annotation)) { container.handle(node); } else { if (container.isEvenIfAlreadyHandled()) container.handle(node); } } } catch (AnnotationValueDecodeFail fail) { fail.owner.setError(fail.getMessage(), fail.idx); } catch (Throwable t) { String sourceName = "(unknown).java"; if (unit != null && unit.sourcefile != null) sourceName = unit.sourcefile.getName(); javacError(String.format("Lombok annotation handler %s failed on " + sourceName, container.handler.getClass()), t); } } }
retry|public static <T> CompletableFuture<T> (final Supplier<CompletableFuture<T>> operation, final int retries, final Executor executor) { final CompletableFuture<T> resultFuture = new CompletableFuture<>(); retryOperation(resultFuture, operation, retries, executor); return resultFuture; }
set finding state|static Finding (FindingName findingName) { try (SecurityCenterClient client = SecurityCenterClient.create()) { // FindingName findingName = FindingName.of(/*organization=*/"123234324", // /*source=*/"423432321", /*findingId=*/"samplefindingid2"); // Use the current time as the finding "event time". Instant eventTime = Instant.now(); Finding response = client.setFindingState(findingName, State.INACTIVE, Timestamp.newBuilder().setSeconds(eventTime.getEpochSecond()).setNanos(eventTime.getNano()).build()); System.out.println("Updated Finding: " + response); return response; } catch (IOException e) { throw new RuntimeException("Couldn't create client.", e); } }
copy|public static Object (Object src, Object dest, int length) { System.arraycopy(src, 0, dest, 0, length); return dest; }
compose user and config data|private ComposeUserData (UserData userData, ConfigData configData) { ComposeUserData result = new ComposeUserData(); Map<String, List<ProviderInfo>> zoneData = new HashMap<String, List<ProviderInfo>>(); if (userData == null) { return result; } else { result.setLocalZone(userData.getLocalZone()); final Map<String, List<String>> listZoneData = userData.getZoneData(); final String[] configDatas = StringUtils.split(configData == null ? StringUtils.EMPTY : configData.getData(), CONFIG_SEPARATOR); final List<String> attrData = Arrays.asList(configDatas); for (String key : listZoneData.keySet()) { final List<ProviderInfo> providerInfos = mergeProviderInfo(listZoneData.get(key), attrData); zoneData.put(key, providerInfos); } result.setZoneData(zoneData); } return result; }
apply|@Override public Void (Empty input) { return null; }
to druid expressions|@Nullable public static List<DruidExpression> (final PlannerContext plannerContext, final RowSignature rowSignature, final List<RexNode> rexNodes) { final List<DruidExpression> retVal = new ArrayList<>(rexNodes.size()); for (RexNode rexNode : rexNodes) { final DruidExpression druidExpression = toDruidExpression(plannerContext, rowSignature, rexNode); if (druidExpression == null) { return null; } retVal.add(druidExpression); } return retVal; }
get allocation statistics|@Override public Table<AllocationStatus, Integer, Long> () { Table<AllocationStatus, Integer, Long> table = HashBasedTable.create(); table.put(AllocationStatus.HOST, 0, zeroUseCounter.get()); for (Integer deviceId : configuration.getAvailableDevices()) { table.put(AllocationStatus.DEVICE, deviceId, getAllocatedDeviceMemory(deviceId)); } return table; }
format header and footer|private void (int itemCount, int id) { // Header. this.header.set(VERSION_OFFSET, CURRENT_VERSION); this.header.set(FLAGS_OFFSET, getFlags(this.config.isIndexPage ? FLAG_INDEX_PAGE : FLAG_NONE)); setHeaderId(id); setCount(itemCount); // Matching footer. setFooterId(id); }
get parameter types|public TypeInformation<?>[] (Class<?>[] signature) { final TypeInformation<?>[] types = new TypeInformation<?>[signature.length]; for (int i = 0; i < signature.length; i++) { try { types[i] = TypeExtractor.getForClass(signature[i]); } catch (InvalidTypesException e) { throw new ValidationException("Parameter types of scalar function " + this.getClass().getCanonicalName() + " cannot be automatically determined. Please provide type information manually."); } } return types; }
delete log metric|public final void (MetricName metricName) { DeleteLogMetricRequest request = DeleteLogMetricRequest.newBuilder().setMetricName(metricName == null ? null : metricName.toString()).build(); deleteLogMetric(request); }
apply|public static OperationInvoker (OperationInvoker invoker, long timeToLive) { if (timeToLive > 0) { return new CachingOperationInvoker(invoker, timeToLive); } return invoker; }
between month|public long (boolean isReset) { final Calendar beginCal = DateUtil.calendar(begin); final Calendar endCal = DateUtil.calendar(end); final int betweenYear = endCal.get(Calendar.YEAR) - beginCal.get(Calendar.YEAR); final int betweenMonthOfYear = endCal.get(Calendar.MONTH) - beginCal.get(Calendar.MONTH); int result = betweenYear * 12 + betweenMonthOfYear; if (false == isReset) { endCal.set(Calendar.YEAR, beginCal.get(Calendar.YEAR)); endCal.set(Calendar.MONTH, beginCal.get(Calendar.MONTH)); long between = endCal.getTimeInMillis() - beginCal.getTimeInMillis(); if (between < 0) { return result - 1; } } return result; }
do on next|@Override protected void (Object message) { if (!Void.class.isAssignableFrom(argumentType)) { reference.set(message); } }
chunk for chunk idx|@Override public Chunk (int cidx) { Chunk crows = rows().chunkForChunkIdx(cidx); return new SubsetChunk(crows, this, masterVec()); }
do on error|@Override protected void (Throwable t) { if (t instanceof HttpClientResponseException) { HttpClientResponseException e = (HttpClientResponseException) t; if (e.getStatus() == HttpStatus.NOT_FOUND) { future.complete(null); return; } } if (LOG.isErrorEnabled()) { LOG.error("Client [" + methodDeclaringType.getName() + "] received HTTP error response: " + t.getMessage(), t); } future.completeExceptionally(t); }
close|@Override public void () { IOUtils.closeQuietly(defaultColumnFamilyHandle); IOUtils.closeQuietly(nativeMetricMonitor); IOUtils.closeQuietly(db); // Making sure the already created column family options will be closed columnFamilyDescriptors.forEach(( cfd) -> IOUtils.closeQuietly(cfd.getOptions())); }
get user authorization redirect url|protected String (ProtectedResourceDetails details, OAuthConsumerToken requestToken, String callbackURL) { try { String baseURL = details.getUserAuthorizationURL(); StringBuilder builder = new StringBuilder(baseURL); char appendChar = baseURL.indexOf('?') < 0 ? '?' : '&'; builder.append(appendChar).append("oauth_token="); builder.append(URLEncoder.encode(requestToken.getValue(), "UTF-8")); if (!details.isUse10a()) { builder.append('&').append("oauth_callback="); builder.append(URLEncoder.encode(callbackURL, "UTF-8")); } return builder.toString(); } catch (UnsupportedEncodingException e) { throw new IllegalStateException(e); } }
list region backend services|@BetaApi public final ListRegionBackendServicesPagedResponse (ProjectRegionName region) { ListRegionBackendServicesHttpRequest request = ListRegionBackendServicesHttpRequest.newBuilder().setRegion(region == null ? null : region.toString()).build(); return listRegionBackendServices(request); }
preprocess|public static String (String text) { return text.replaceAll("\\p{P}", " ").replaceAll("\\s+", " ").toLowerCase(Locale.getDefault()); }
call|@Override public // Called for each server being selected Observable<T> (Server server) { context.setServer(server); final ServerStats stats = loadBalancerContext.getServerStats(server); // Called for each attempt and retry Observable<T> o = Observable.just(server).concatMap(new Func1<Server, Observable<T>>() { @Override public Observable<T> call(final Server server) { context.incAttemptCount(); loadBalancerContext.noteOpenConnection(stats); if (listenerInvoker != null) { try { listenerInvoker.onStartWithServer(context.toExecutionInfo()); } catch (AbortExecutionException e) { return Observable.error(e); } } final Stopwatch tracer = loadBalancerContext.getExecuteTracer().start(); return operation.call(server).doOnEach(new Observer<T>() { private T entity; @Override public void onCompleted() { recordStats(tracer, stats, entity, null); // TODO: What to do if onNext or onError are never called? } @Override public void onError(Throwable e) { recordStats(tracer, stats, null, e); logger.debug("Got error {} when executed on server {}", e, server); if (listenerInvoker != null) { listenerInvoker.onExceptionWithServer(e, context.toExecutionInfo()); } } @Override public void onNext(T entity) { this.entity = entity; if (listenerInvoker != null) { listenerInvoker.onExecutionSuccess(entity, context.toExecutionInfo()); } } private void recordStats(Stopwatch tracer, ServerStats stats, Object entity, Throwable exception) { tracer.stop(); loadBalancerContext.noteRequestCompletion(stats, entity, exception, tracer.getDuration(TimeUnit.MILLISECONDS), retryHandler); } }); } }); if (maxRetrysSame > 0) o = o.retry(retryPolicy(maxRetrysSame, true)); return o; }
call|@Override public Observable<T> (final Server server) { context.incAttemptCount(); loadBalancerContext.noteOpenConnection(stats); if (listenerInvoker != null) { try { listenerInvoker.onStartWithServer(context.toExecutionInfo()); } catch (AbortExecutionException e) { return Observable.error(e); } } final Stopwatch tracer = loadBalancerContext.getExecuteTracer().start(); return operation.call(server).doOnEach(new Observer<T>() { private T entity; @Override public void onCompleted() { recordStats(tracer, stats, entity, null); // TODO: What to do if onNext or onError are never called? } @Override public void onError(Throwable e) { recordStats(tracer, stats, null, e); logger.debug("Got error {} when executed on server {}", e, server); if (listenerInvoker != null) { listenerInvoker.onExceptionWithServer(e, context.toExecutionInfo()); } } @Override public void onNext(T entity) { this.entity = entity; if (listenerInvoker != null) { listenerInvoker.onExecutionSuccess(entity, context.toExecutionInfo()); } } private void recordStats(Stopwatch tracer, ServerStats stats, Object entity, Throwable exception) { tracer.stop(); loadBalancerContext.noteRequestCompletion(stats, entity, exception, tracer.getDuration(TimeUnit.MILLISECONDS), retryHandler); } }); }
do on complete|@Override protected void () { future.complete(reference.get()); }
get value|@Override public Boolean () { return properties.requestCacheEnabled().get(); }
get value|@Override public Number () { return properties.maxRequestsInBatch().get(); }
get value|@Override public Number () { return properties.timerDelayInMilliseconds().get(); }
on completed|@Override public void () { recordStats(tracer, stats, entity, null); // TODO: What to do if onNext or onError are never called? }
on error|@Override public void (Throwable e) { recordStats(tracer, stats, null, e); logger.debug("Got error {} when executed on server {}", e, server); if (listenerInvoker != null) { listenerInvoker.onExceptionWithServer(e, context.toExecutionInfo()); } }
on next|@Override public void (T entity) { this.entity = entity; if (listenerInvoker != null) { listenerInvoker.onExecutionSuccess(entity, context.toExecutionInfo()); } }
record stats|private void (Stopwatch tracer, ServerStats stats, Object entity, Throwable exception) { tracer.stop(); loadBalancerContext.noteRequestCompletion(stats, entity, exception, tracer.getDuration(TimeUnit.MILLISECONDS), retryHandler); }
call|@Override public Observable<T> (Throwable e) { if (context.getAttemptCount() > 0) { if (maxRetrysNext > 0 && context.getServerAttemptCount() == (maxRetrysNext + 1)) { e = new ClientException(ClientException.ErrorType.NUMBEROF_RETRIES_NEXTSERVER_EXCEEDED, "Number of retries on next server exceeded max " + maxRetrysNext + " retries, while making a call for: " + context.getServer(), e); } else if (maxRetrysSame > 0 && context.getAttemptCount() == (maxRetrysSame + 1)) { e = new ClientException(ClientException.ErrorType.NUMBEROF_RETRIES_EXEEDED, "Number of retries exceeded max " + maxRetrysSame + " retries, while making a call for: " + context.getServer(), e); } } if (listenerInvoker != null) { listenerInvoker.onExecutionFailed(e, context.toFinalExecutionInfo()); } return Observable.error(e); }
